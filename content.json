{"meta":{"title":"宋涛的博客","subtitle":"漫步在大陆上的海龟","description":"有些东西写下来是想告诉自己，别太着急","author":"宋涛","url":"http://yoursite.com","root":"/"},"pages":[{"title":"","date":"2020-05-24T06:30:08.162Z","updated":"2017-09-03T09:20:09.000Z","comments":true,"path":"404.html","permalink":"http://yoursite.com/404.html","excerpt":"","text":""},{"title":"categories","date":"2017-09-03T02:27:56.000Z","updated":"2017-09-03T07:31:26.000Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2017-09-03T02:17:50.000Z","updated":"2017-09-03T07:31:23.000Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"","date":"2019-04-14T01:55:16.728Z","updated":"2019-04-14T01:55:16.728Z","comments":true,"path":"images/fluent_python/README.html","permalink":"http://yoursite.com/images/fluent_python/README.html","excerpt":"","text":"2. 函数 本文为《流畅的Python》的读书笔记，强烈推荐阅读原书Fluent Python by Luciano Ramalho (O’Reilly). Copyright2015 Luciano Ramalho, 978-1-491-94600-8. 内容概要 函数作为一等对象 闭包 函数装饰器 Python 基本概念 可调用（callable） 函数属性(function attribute) 内省(introspection) 参数注解(parameter annotation) nonlocal 声明"}],"posts":[{"title":"29 MYSQL 分区表","slug":"mysql/MySQL实战45讲/61_分区表","date":"2020-03-28T16:00:00.000Z","updated":"2020-05-24T15:57:24.485Z","comments":true,"path":"2020/03/29/mysql/MySQL实战45讲/61_分区表/","link":"","permalink":"http://yoursite.com/2020/03/29/mysql/MySQL实战45讲/61_分区表/","excerpt":"要不要使用分区表","text":"要不要使用分区表 1. 分区表为了说明分区表的组织形式，先创建一个表 t： 1234567891011CREATE TABLE `t` ( `ftime` datetime NOT NULL, `c` int(11) DEFAULT NULL, KEY (`ftime`)) ENGINE=InnoDB DEFAULT CHARSET=latin1PARTITION BY RANGE (YEAR(ftime))(PARTITION p_2017 VALUES LESS THAN (2017) ENGINE = InnoDB, PARTITION p_2018 VALUES LESS THAN (2018) ENGINE = InnoDB, PARTITION p_2019 VALUES LESS THAN (2019) ENGINE = InnoDB,PARTITION p_others VALUES LESS THAN MAXVALUE ENGINE = InnoDB);insert into t values('2017-4-1',1),('2018-4-1',1); 表 t 将包含了一个.frm 文件和 4 个.ibd 文件，每个分区对应一个.ibd 文件。也就是说： 对于引擎层来说，这是 4 个表； 对于 Server 层来说，这是 1 个表。 1.1 分区表的引擎层行为innodb 分区表 从上面的实验效果可以看出，session B 的第一个 insert 语句是可以执行成功的。这是因为，对于引擎来说，p_2018 和 p_2019 是两个不同的表，也就是说 2017-4-1 的下一个记录并不是 2018-4-1，而是 p_2018 分区的 supremum。所以 T1 时刻，在表 t 的 ftime 索引上，间隙和加锁的状态其实是图 4 这样的： myisam 分区表首先用 alter table t engine=myisam，把表 t 改成 MyISAM 表，然后执行下面这个操作 由于 MyISAM 引擎只支持表锁，所以这条 update 语句会锁住整个表 t 上的读。正是因为 MyISAM 的表锁是在引擎层实现的，session A 加的表锁，其实是锁在分区 p_2018 上。因此，只会堵住在这个分区上执行的查询，落到其他分区的查询是不受影响的。 分区策略每当第一次访问一个分区表的时候，MySQL 需要把所有的分区都访问一遍。 一个典型的报错情况是这样的：如果一个分区表的分区很多，比如超过了 1000 个，而 MySQL 启动的时候，open_files_limit 参数使用的是默认值 1024，那么就会在访问这个表的时候，由于需要打开所有的文件，导致打开表文件的个数超过了上限而报错。 MyISAM 分区表使用的分区策略，我们称为通用分区策略（generic partitioning），每次访问分区都由 server 层控制。通用分区策略，是 MySQL 一开始支持分区表的时候就存在的代码，在文件管理、表管理的实现上很粗糙，因此有比较严重的性能问题。 从 MySQL 5.7.9 开始，InnoDB 引擎引入了本地分区策略（native partitioning）。这个策略是在 InnoDB 内部自己管理打开分区的行为。 从 MySQL 8.0 版本开始，就不允许创建 MyISAM 分区表了，只允许创建已经实现了本地分区策略的引擎。目前来看，只有 InnoDB 和 NDB 这两个引擎支持了本地分区策略。 1.2 分区表的 server 层行为如果从 server 层看的话，一个分区表就只是一个表。 分区表，在做 DDL 的时候，影响会更大。虽然 session B 只需要操作 p_2107 这个分区，但是由于 session A 持有整个表 t 的 MDL 锁，就导致了 session B 的 alter 语句被堵住。 到这里我们小结一下： MySQL 在第一次打开分区表的时候，需要访问所有的分区； 在 server 层，认为这是同一张表，因此所有分区共用同一个 MDL 锁； 在引擎层，认为这是不同的表，因此 MDL 锁之后的执行过程，会根据分区表规则，只访问必要的分区。 而关于“必要的分区”的判断，就是根据 SQL 语句中的 where 条件，结合分区规则来实现的。 2. 分区表的适用场景2.1 分区表的优点分区表的一个显而易见的优势是对业务透明，相对于用户分表来说，使用分区表的业务代码更简洁。 还有，分区表可以很方便的清理历史数据。如果一项业务跑的时间足够长，往往就会有根据时间删除历史数据的需求。这时候，按照时间分区的分区表，就可以直接通过 alter table t drop partition …这个语法删掉分区，从而删掉过期的历史数据。 这个 alter table t drop partition …操作是直接删除分区文件，效果跟 drop 普通表类似。与使用 delete 语句删除数据相比，优势是速度快、对系统影响小。 2.2 分区表的劣势实际使用时，分区表跟用户分表比起来，有两个绕不开的问题： 一个是第一次访问的时候需要访问所有分区 另一个是共用 MDL 锁。 因此，如果要使用分区表，就不要创建太多的分区。这里有两个问题需要注意： 分区并不是越细越好。实际上，单表或者单分区的数据一千万行，只要没有特别大的索引，对于现在的硬件能力来说都已经是小表了 分区也不要提前预留太多，在使用之前预先创建即可。对于没有数据的历史分区，要及时的 drop 掉。 至于分区表的其他问题，比如查询需要跨多个分区取数据，查询性能就会比较慢，基本上就不是分区表本身的问题，而是数据量的问题或者说是使用方式的问题了。当然，如果你的团队已经维护了成熟的分库分表中间件，用业务分表，对业务开发同学没有额外的复杂性，对 DBA 也更直观，自然是更好的。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"28 MySQL 连接管理","slug":"mysql/MySQL实战45讲/52_连接管理","date":"2020-03-27T16:00:00.000Z","updated":"2020-05-24T15:57:17.267Z","comments":true,"path":"2020/03/28/mysql/MySQL实战45讲/52_连接管理/","link":"","permalink":"http://yoursite.com/2020/03/28/mysql/MySQL实战45讲/52_连接管理/","excerpt":"MySQL 的连接管理","text":"MySQL 的连接管理 2. mysql 连接管理在 MySQL 中有两个 kill 命令： kill query + 线程 id: 表示终止这个线程中正在执行的语句； kill connection + 线程 id: connection 可缺省，表示断开这个线程的连接，如果这个线程有语句正在执行，也是要先停止正在执行的语句。 2.1 收到 kill 以后，线程做什么当用户执行 kill query thread_id_B 时，MySQL 里处理 kill 命令的线程做了两件事： 把 session B 的运行状态改成 THD::KILL_QUERY(将变量 killed 赋值为 THD::KILL_QUERY)； 给 session B 的执行线程发一个信号，让 session B 退出等待或终止执行，来处理这个 THD::KILL_QUERY 状态 上面的分析中，隐含了这么三层意思： 一个语句执行过程中有多处“埋点”，在这些“埋点”的地方判断线程状态，如果发现线程状态是 THD::KILL_QUERY，才开始进入语句终止逻辑； 如果处于等待状态，必须是一个可以被唤醒的等待，否则根本不会执行到“埋点”处； 语句从开始进入终止逻辑，到终止逻辑完全完成，是有一个过程的。 因此不是“说停就停的”。我们来看另种kill 无效的情况: 线程没有执行到判断线程状态的逻辑 终止逻辑耗时较长 2.2 线程没有执行到判断线程状态的逻辑执行 set global innodb_thread_concurrency=2，将 InnoDB 的并发线程上限数设置为 2；然后，执行下面的序列： session D 执行的 kill query C 命令却没什么效果，直到 session E 执行了 kill connection 命令，才断开了 session C 的连接，提示“Lost connection to MySQL server during query”，此时执行 show processlist id=12 这个线程的 Commnad 列显示的是 Killed。也就是说，客户端虽然断开了连接，但实际上服务端上这条语句还在执行过程中。 killed 原因分析在这个例子里，12 号线程的等待逻辑是这样的：每 10 毫秒判断一下是否可以进入 InnoDB 执行，如果不行，就调用 nanosleep 函数进入 sleep 状态。 虽然 12 号线程的状态已经被设置成了 KILL_QUERY，但是在这个等待进入 InnoDB 的循环过程中，并没有去判断线程的状态，因此根本不会进入终止逻辑阶段。 当 session E 执行 kill connection 命令时，是这么做的: 把 12 号线程状态设置为 KILL_CONNECTION； 关掉 12 号线程的网络连接。因为有这个操作，所以你会看到，这时候 session C 收到了断开连接的提示。 如果一个线程的状态是KILL_CONNECTION，show processlist 就把Command列显示成Killed。 所以其实，即使是客户端退出了，这个线程的状态仍然是在等待中。那这个线程什么时候会退出呢？答案是，只有等到满足进入 InnoDB 的条件后，session C 的查询语句继续执行，然后才有可能判断到线程状态已经变成了 KILL_QUERY 或者 KILL_CONNECTION，再进入终止逻辑阶段。 2.3 终止逻辑耗时较长终止逻辑耗时较长。从 show processlist 结果上看也是 Command=Killed，需要等到终止逻辑完成，语句才算真正完成。这类情况，比较常见的场景有以下几种： 超大事务执行期间被 kill。这时候，回滚操作需要对事务执行期间生成的所有新数据版本做回收操作，耗时很长。大查询回滚。 如果查询过程中生成了比较大的临时文件，加上此时文件系统压力大，删除临时文件可能需要等待 IO 资源，导致耗时较长。 DDL 命令执行到最后阶段，如果被 kill，需要删除中间过程的临时文件，也可能受 IO 资源影响耗时较久。 2.4 killed 线程的处理如果你发现一个线程处于 Killed 状态，你可以做的事情就是，通过影响系统环境，让这个 Killed 状态尽快结束。 InnoDB 并发度的问题，你就可以临时调大 innodb_thread_concurrency 的值，或者停掉别的线程 如果是回滚逻辑由于受到 IO 资源限制执行得比较慢，就通过减少系统压力让它加速 对于回滚大事务导致的 killed 线程，重启服务是没用的，为重启之后该做的回滚动作还是不能少的。最好还是等待它自己执行完成。如果这个语句可能会占用别的锁，或者由于占用 IO 资源过多，从而影响到了别的语句执行的话，就需要先做主备切换，切到新主库提供服务。切换之后别的线程都断开了连接，自动停止执行。接下来还是等它自己执行完成。这个操作属于我们在文章中说到的，减少系统压力，加速终止逻辑。 3. mysql 客户端行为关于 mysql 的客户端，有以下几个常见的误解: 3.1 客户端终止连接客户端通过 Ctrl+C 命令，是不是就可以直接终止线程 Ctrl+C 操作的是客户端进程，与服务器端没有直接关系 MySQL 是停等协议，所以这个线程执行的语句还没有返回的时候，再往这个连接里面继续发命令也是没有用的。实际上，执行 Ctrl+C 的时候，是 MySQL 客户端另外启动一个连接，然后发送一个 kill query 命令 3.2 如果库里面的表特别多，连接就会很慢 有些线上的库，会包含很多表（我见过最多的一个库里有 6 万个表）。这时候，你就会发现，每次用客户端连接都会卡在下面这个界面上。 当使用默认参数连接的时候，MySQL 客户端会提供一个本地库名和表名补全的功能。为了实现这个功能，客户端在连接成功后，需要多做一些操作： 执行 show databases； 切到 db1 库，执行 show tables； 把这两个命令的结果用于构建一个本地的哈希表 最花时间的就是第三步在本地构建哈希表的操作,我们感知到的连接过程慢，其实并不是连接慢，也不是服务端慢，而是客户端慢。提示也说了，如果在连接命令中加上 -A，就可以关掉这个自动补全的功能。 除了加 -A 以外，加–quick(或者简写为 -q) 参数，也可以跳过这个阶段。但是，这个–quick 是一个更容易引起误会的参数，也是关于客户端常见的一个误解。 3.3 客户端的 -quick 参数设置了这个参数可能会降低服务端的性能，而不是加速连接。 MySQL 客户端发送请求后，接收服务端返回结果的方式有两种： 一种是本地缓存，也就是在本地开一片内存，先把结果存起来。如果你用 API 开发，对应的就是 mysql_store_result 方法。 另一种是不缓存，读一个处理一个。如果你用 API 开发，对应的就是 mysql_use_result 方法。 MySQL 客户端默认采用第一种方式，而如果加上–quick 参数，就会使用第二种不缓存的方式。采用不缓存的方式时，如果本地处理得慢，就会导致服务端发送结果被阻塞，因此会让服务端变慢。 为什么要给这个参数取名叫作 quick 呢？这是因为使用这个参数可以达到以下三点效果： 第一点，就是前面提到的，跳过表名自动补全功能。 第二点，mysql_store_result 需要申请本地内存来缓存查询结果，如果查询结果太大，会耗费较多的本地内存，可能会影响客户端本地机器的性能； 第三点，是不会把执行命令记录到本地的命令历史文件 –quick 参数的意思，是让客户端变得更快","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"27 MYSQL flush privileges","slug":"mysql/MySQL实战45讲/51_grant_privileges","date":"2020-03-26T16:00:00.000Z","updated":"2020-05-24T15:57:09.521Z","comments":true,"path":"2020/03/27/mysql/MySQL实战45讲/51_grant_privileges/","link":"","permalink":"http://yoursite.com/2020/03/27/mysql/MySQL实战45讲/51_grant_privileges/","excerpt":"grant 之后要不要跟 flush privileges","text":"grant 之后要不要跟 flush privileges 1. 用户权限MySQL 使用 create user 来创建用户。 create user &#39;ua&#39;@&#39;%&#39; identified by &#39;pa&#39;; 的执行过程如下: 磁盘上，往 mysql.user 表里插入一行，由于没有指定权限，所以这行数据上所有表示权限的字段的值都是 N； 内存里，往数组 acl_users 里插入一个 acl_user 对象，这个对象的 access 字段值为 0。 在 MySQL 中，用户权限是有不同的范围的。从大到小的顺序依次是: 全局权限 db 权限 表权限和列权限 1.1 全局权限全局权限，作用于整个 MySQL 实例，这些权限信息保存在 mysql 库的 user 表里。 12# 1. 赋予 ua 全局权限grant all privileges on *.* to 'ua'@'%' with grant option; 这个 grant 命令做了两个动作： 磁盘上，将 mysql.user 表里，用户’ua’@’%’这一行的所有表示权限的字段的值都修改为‘Y’； 内存里，从数组 acl_users 中找到这个用户对应的对象，将 access 值（权限位）修改为二进制的“全 1” 在这个 grant 命令执行完成后，如果有新的客户端使用用户名 ua 登录成功，MySQL 会为新连接维护一个线程对象，然后从 acl_users 数组里查到这个用户的权限，并将权限值拷贝到这个线程对象中。之后在这个连接中执行的语句，所有关于全局权限的判断，都直接使用线程对象内部保存的权限位。 基于上面的分析我们可以知道： grant 命令对于全局权限，同时更新了磁盘和内存。命令完成后即时生效，接下来新创建的连接会使用新的权限。 对于一个已经存在的连接，它的全局权限不受 grant 命令的影响。 收回全局权限使用下面的 revoke 命令:1revoke all privileges on *.* from 'ua'@'%'; 这条 revoke 命令的用法与 grant 类似，做了如下两个动作： 磁盘上，将 mysql.user 表里，用户’ua’@’%’这一行的所有表示权限的字段的值都修改为“N”； 内存里，从数组 acl_users 中找到这个用户对应的对象，将 access 的值修改为 0。 另外 grant 语句赋权时，可能还会看到这样的写法：1grant super on *.* to 'ua'@'%' identified by 'pa'; 这条命令加了 identified by ‘密码’， 语句的逻辑里面除了赋权外，还包含了： 如果用户’ua’@’%’不存在，就创建这个用户，密码是 pa； 如果用户 ua 已经存在，就将密码修改成 pa。 这是一种不建议的写法，因为这种写法很容易就会不慎把密码给改了。 1.2 db 权限12# 赋予 ua 数据库 db1 的操作权限grant all privileges on db1.* to 'ua'@'%' with grant option; 基于库的权限记录保存在 mysql.db 表中，在内存里则保存在数组 acl_dbs 中。这条 grant 命令做了如下两个动作： 磁盘上，往 mysql.db 表中插入了一行记录，所有权限位字段设置为“Y”； 内存里，增加一个对象到数组 acl_dbs 中，这个对象的权限位为“全 1”。 每次需要判断一个用户对一个数据库读写权限的时候，都需要遍历一次 acl_dbs 数组，根据 user、host 和 db 找到匹配的对象，然后根据对象的权限位来判断。 也就是说，grant 修改 db 权限的时候，是同时对磁盘和内存生效的。 已存在连接的权限grant 操作对于已经存在的连接的影响，在全局权限和基于 db 的权限效果是不同的。 从上面这个操作序列我们可以看到: super 是全局权限，这个权限信息在线程对象中，而 revoke 操作影响不到这个线程对象。 acl_dbs 是一个全局数组，所有线程判断 db 权限都用这个数组，这样 revoke 操作马上就会影响到 session B。 代码实现上有一个特别的逻辑，如果当前会话已经处于某一个 db 里面，之前 use 这个库的时候拿到的库权限会保存在会话变量中。session C 在 T2 时刻执行的 use db1，拿到了这个库的权限，在切换出 db1 库之前，session C 对这个库就一直有权限。 1.3 表权限和列权限表权限定义存放在表 mysql.tables_priv 中，列权限定义存放在表 mysql.columns_priv 中。这两类权限，组合起来存放在内存的 hash 结构 column_priv_hash 中。 1234create table db1.t1(id int, a int);grant all privileges on db1.t1 to 'ua'@'%' with grant option;GRANT SELECT(id), INSERT (id,a) ON mydb.mytbl TO 'ua'@'%' with grant option; 跟 db 权限类似，这两个权限每次 grant 的时候都会修改数据表，也会同步修改内存中的 hash 结构。因此，对这两类权限的操作，也会马上影响到已经存在的连接。 2. flush privileges从上面的分析来看，grant 语句都是即时生效的，不需要执行 flush privileges 语句。 flush privileges 命令会清空 acl_users 数组，然后从 mysql.user 表中读取数据重新加载，重新构造一个 acl_users 数组。对于 db 权限、表权限和列权限，MySQL 也做了这样的处理。 也就是说，如果内存的权限数据和磁盘数据表相同的话，不需要执行 flush privileges。而如果我们都是用 grant/revoke 语句来执行的话，内存和数据表本来就是保持同步更新的。 因此，grant 语句会同时修改数据表和内存，判断权限的时候使用的是内存数据。因此，规范地使用 grant 和 revoke 语句，是不需要随后加上 flush privileges 语句的。 2.1 需要执行 flush privilege 的场景显然，当数据表中的权限数据跟内存中的权限数据不一致的时候，flush privileges 语句可以用来重建内存数据，达到一致状态。这种不一致往往是由不规范的操作导致的，比如直接用 DML 语句操作系统权限表。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"26 MYSQL 自增主键","slug":"mysql/MySQL实战45讲/45_自增主键","date":"2020-03-25T16:00:00.000Z","updated":"2020-05-24T15:57:04.211Z","comments":true,"path":"2020/03/26/mysql/MySQL实战45讲/45_自增主键/","link":"","permalink":"http://yoursite.com/2020/03/26/mysql/MySQL实战45讲/45_自增主键/","excerpt":"自增主键为什么不连续","text":"自增主键为什么不连续 1. 自增值1.1 自增值的保存表的结构定义存放在后缀名为.frm 的文件中，但是并不会保存自增值。不同的引擎对于自增值的保存策略不同。 MyISAM 引擎的自增值保存在数据文件中。 InnoDB 引擎的自增值，其实是保存在了内存里，并且到了 MySQL 8.0 版本后，才有了“自增值持久化”的能力，也就是才实现了“如果发生重启，表的自增值可以恢复为 MySQL 重启前的值”，具体情况是： 在 MySQL 5.7 及之前的版本，自增值保存在内存里，并没有持久化。每次重启后，第一次打开表的时候，都会去找自增值的最大值 max(id)，然后将 max(id)+1 作为这个表当前的自增值。 在 MySQL 8.0 版本，将自增值的变更记录在了 redo log 中，重启的时候依靠 redo log 恢复重启之前的值。 1.2 自增值修改机制果字段被定义为 AUTO_INCREMENT，在插入一行数据的时候，自增值的行为如下： 如果插入数据时 id 字段指定为 0、null 或未指定值，那么就把这个表当前的 AUTO_INCREMENT 值填到自增字段； 如果插入数据时 id 字段指定了具体的值，就直接使用语句里指定的值。 假设，某次要插入的值是 X，当前的自增值是 Y。 如果 X&lt;Y，那么这个表的自增值不变； 如果 X≥Y，就需要把当前自增值修改为新的自增值。 新的自增值生成算法是：从 auto_increment_offset 开始，以 auto_increment_increment 为步长，持续叠加，直到找到第一个大于 X 的值，作为新的自增值。 1.3 自增值的修改时机自增值的修改，是在真正执行插入数据的操作之前，如果插入操作并没有真正完成，自增值也不会改回来。 唯一键冲突是导致自增主键 id 不连续的第一种原因。 回滚也会产生类似的现象，这就是第二种原因。 1.4 自增值为什么不能回退假设有两个并行执行的事务，在申请自增值的时候，为了避免两个事务申请到相同的自增 id，肯定要加锁，然后顺序申请。 假设事务 A 申请到了 id=2， 事务 B 申请到 id=3，那么这时候表 t 的自增值是 4，之后继续执行。 事务 B 正确提交了，但事务 A 出现了唯一键冲突。 如果允许事务 A 把自增 id 回退，也就是把表 t 的当前自增值改回 2，那么就会出现这样的情况：表里面已经有 id=3 的行，而当前的自增 id 值是 2。 接下来，继续执行的其他事务就会申请到 id=2，然后再申请到 id=3。这时，就会出现插入语句报错“主键冲突”。 而为了解决这个主键冲突，有两种方法： 每次申请 id 之前，先判断表里面是否已经存在这个 id。如果存在，就跳过这个 id。但是，这个方法的成本很高。因为，本来申请 id 是一个很快的操作，现在还要再去主键索引树上判断 id 是否存在。 把自增 id 的锁范围扩大，必须等到一个事务执行完成并提交，下一个事务才能再申请自增 id。这个方法的问题，就是锁的粒度太大，系统并发能力大大下降。 可见，这两个方法都会导致性能问题。造成这些麻烦的罪魁祸首，就是我们假设的这个“允许自增 id 回退”的前提导致的。因此，InnoDB 放弃了这个设计，语句执行失败也不回退自增 id。也正是因为这样，所以才只保证了自增 id 是递增的，但不保证是连续的。 1.5 自增锁的优化史自增 id 锁并不是一个事务锁，而是每次申请完就马上释放，以便允许别的事务再申请。接下来，我们先看一下自增锁设计的历史: 在 MySQL 5.0 版本的时候，自增锁的范围是语句级别。也就是说，如果一个语句申请了一个表自增锁，这个锁会等语句执行结束以后才释放。显然，这样设计会影响并发度。 MySQL 5.1.22 版本引入了一个新策略，新增参数 innodb_autoinc_lock_mode，默认值是 1。 =0 时，表示采用之前 MySQL 5.0 版本的策略 =1 时: 普通 insert 语句，自增锁在申请之后就马上释放； 类似 insert … select 这样的批量插入数据的语句，自增锁还是要等语句结束后才被释放； =2 时，所有的申请自增主键的动作都是申请后就释放锁。 那么，为什么默认设置下，insert … select 要使用语句级的锁？为什么这个参数的默认值不是 2？答案是，这么设计还是为了数据的一致性。 设想一下，如果 session B 是申请了自增值以后马上就释放自增锁，那么就可能出现这样的情况： session B 先插入了两个记录，(1,1,1)、(2,2,2)； 然后，session A 来申请自增 id 得到 id=3，插入了（3,5,5)； 之后，session B 继续执行，插入两条记录 (4,3,3)、 (5,4,4) 如果我们现在的 binlog_format=statement，由于两个 session 是同时执行插入数据命令的，所以 binlog 里面对表 t2 的更新日志只有两种情况：要么先记 session A 的，要么先记 session B 的。 但不论是哪一种，这个 binlog 拿去从库执行，或者用来恢复临时实例，备库和临时实例里面，session B 这个语句执行出来，生成的结果里面，id 都是连续的。这时，这个库就发生了数据不一致。 其实，这是因为原库 session B 的 insert 语句，生成的 id 不连续。这个不连续的 id，用 statement 格式的 binlog 来串行执行，是执行不出来的。 而要解决这个问题，有两种思路： 一种思路是，让原库的批量插入数据语句，固定生成连续的 id 值。所以，自增锁直到语句执行结束才释放，就是为了达到这个目的。 另一种思路是，在 binlog 里面把插入数据的操作都如实记录进来，到备库执行的时候，不再依赖于自增主键去生成。这种情况，其实就是 innodb_autoinc_lock_mode 设置为 2，同时 binlog_format 设置为 row。 因此，在生产上，尤其是有 insert … select 这种批量插入数据的场景时，从并发插入数据性能的角度考虑，我建议你这样设置：innodb_autoinc_lock_mode=2 ，并且 binlog_format=row. 这样做，既能提升并发性，又不会出现数据一致性问题。需要注意的是，我这里说的批量插入数据，包含的语句类型是 insert … select、replace … select 和 load data 语句。 在普通的 insert 语句里面包含多个 value 值的情况下，即使 innodb_autoinc_lock_mode 设置为 1，也不会等语句执行完成才释放锁。因为这类语句在申请自增 id 的时候，是可以精确计算出需要多少个 id 的，然后一次性申请，申请完成后锁就可以释放了。 也就是说，批量插入数据的语句，之所以需要这么设置，是因为“不知道要预先申请多少个 id”。 1.6 自增 id 的优化对于批量插入，为了避免自增值用一个申请一个带来的低效。对于批量插入数据的语句，MySQL 有一个批量申请自增 id 的策略： 语句执行过程中，第一次申请自增 id，会分配 1 个； 同一个语句去申请自增 id，每次申请到的自增 id 个数都是上一次的两倍。注意是同一语句 这是主键 id 出现自增 id 不连续的第三种原因。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"25 MYSQL Memory 存储引擎","slug":"mysql/MySQL实战45讲/44_Memory存储引擎","date":"2020-03-24T16:00:00.000Z","updated":"2020-05-24T15:56:59.509Z","comments":true,"path":"2020/03/25/mysql/MySQL实战45讲/44_Memory存储引擎/","link":"","permalink":"http://yoursite.com/2020/03/25/mysql/MySQL实战45讲/44_Memory存储引擎/","excerpt":"什么时候使用 Memory 存储引擎","text":"什么时候使用 Memory 存储引擎 1. 索引组织对比假设有以下的两张表 t1 和 t2，其中表 t1 使用 Memory 引擎， 表 t2 使用 InnoDB 引擎。1234create table t1(id int primary key, c int) engine=Memory;create table t2(id int primary key, c int) engine=innodb;insert into t1 values(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9),(0,0);insert into t2 values(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9),(0,0); 1.1 innodb 组织形式InnoDB 表的数据放在主键索引树上，主键索引是 B+ 树。所以表 t2 的数据组织方式如下图所示：主键索引上的值是有序存储的。 主键索引上的值是有序存储的。在执行 select * 的时候，就会按照叶子节点从左到右扫描，所以得到的结果里，0 就出现在第一行。 1.2 Memory 组织形式与 InnoDB 引擎不同，Memory 引擎的数据和索引是分开的。我们来看一下表 t1 中的数据内容。 内存表的数据部分以数组的方式单独存放，而主键 id 索引里，存的是每个数据的位置。主键 id 是 hash 索引，可以看到索引上的 key 并不是有序的。 在内存表 t1 中，当我执行 select * 的时候，走的是全表扫描，也就是顺序扫描这个数组。因此，0 就是最后一个被读到，并放入结果集的数据。 需要指出的是，表 t1 的这个主键索引是哈希索引，因此如果执行范围查询是用不上主键索引的，需要走全表扫描，但是等值查找的速度比 B-Tree 索引快。 1.3 对比InnoDB 和 Memory 引擎的数据组织方式是不同的： InnoDB 引擎把数据放在主键索引上，其他索引上保存的是主键 id。这种方式，我们称之为索引组织表（Index Organizied Table）。 Memory 引擎采用的是把数据单独存放，索引上保存数据位置的数据组织形式，我们称之为堆组织表（Heap Organizied Table）。 这两个引擎存在一些典型不同： InnoDB 表的数据总是有序存放的，而内存表的数据就是按照写入顺序存放的； 当数据文件有空洞的时候，InnoDB 表在插入新数据的时候，为了保证数据有序性，只能在固定的位置写入新值，而内存表找到空位就可以插入新值； 数据位置发生变化的时候，InnoDB 表只需要修改主键索引，而内存表需要修改所有索引； InnoDB 表用主键索引查询时需要走一次索引查找，用普通索引查询的时候，需要走两次索引查找。而内存表没有这个区别，所有索引的“地位”都是相同的。 InnoDB 支持变长数据类型，不同记录的长度可能不同；内存表不支持 Blob 和 Text 字段，并且即使定义了 varchar(N)，实际也当作 char(N)，也就是固定长度字符串来存储，因此内存表的每行数据长度相同。 由于内存表的这些特性，每个数据行被删除以后，空出的这个位置都可以被接下来要插入的数据复用。 2. Memory 存储引擎内存表也是支 B-Tree 索引的。在 id 列上创建一个 B-Tree 索引，SQL 语句可以这么写：1alter table t1 add index a_btree_index using btree (id); 这时，表 t1 的数据组织形式就变成了这样： 2.1 Memory 特性一般在我们的印象中，内存表的优势是速度快，其中的一个原因就是 Memory 引擎支持 hash 索引。当然，更重要的原因是，内存表的所有数据都保存在内存，而内存的读写速度总是比磁盘快。 但不建议在生产环境上使用内存表。这里的原因主要包括两个方面： 锁粒度问题； 数据持久化问题。 内存表不支持行锁，只支持表锁。因此，一张表只要有更新，就会堵住其他所有在这个表上的读写操作。 2.2 数据持久化数据放在内存中，数据库重启的时候，所有的内存表都会被清空。在高可用架构下，内存表的这个特点简直可以当做 bug 来看待了。 对于一个双 M 集群，我们来看一下下面这个时序： 业务正常访问主库； 备库硬件升级，备库重启，内存表 t1 内容被清空； 备库重启后，客户端发送一条 update 语句，修改表 t1 的数据行，这时备库应用线程就会报错“找不到要更新的行”。 这样就会导致主备同步停止。当然，如果这时候发生主备切换的话，客户端会看到，表 t1 的数据“丢失”了。 由于 MySQL 知道重启之后，内存表的数据会丢失。所以，担心主库重启之后，出现主备不一致，MySQL 在实现上做了这样一件事儿：在数据库重启之后，往 binlog 里面写入一行 DELETE FROM t1。 在备库重启的时候，备库 binlog 里的 delete 语句就会传到主库，然后把主库内存表的内容删除。这样你在使用的时候就会发现，主库的内存表数据突然被清空了。 2.3 适用场景因为: 如果你的表更新量大，那么并发度是一个很重要的参考指标，InnoDB 支持行锁，并发度比内存表好； 能放到内存表的数据量都不大。如果你考虑的是读的性能，一个读 QPS 很高并且数据量不大的表，即使是使用 InnoDB，数据也是都会缓存在 InnoDB Buffer Pool 里的。因此，使用 InnoDB 表的读性能也不会差。 由于重启会丢数据，如果一个备库重启，会导致主备同步线程停止；如果主库跟这个备库是双 M 架构，还可能导致主库的内存表数据被删掉。 因此建议把普通内存表都用 InnoDB 表来代替。但是，有一个场景却是例外的 – 内存临时表。 内存临时表刚好可以无视内存表的两个不足，主要是下面的三个原因： 临时表不会被其他线程访问，没有并发性的问题； 临时表重启后也是需要删除的，清空数据这个问题不存在； 备库的临时表也不会影响主库的用户线程。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"24 临时表","slug":"mysql/MySQL实战45讲/43_临时表","date":"2020-03-23T16:00:00.000Z","updated":"2020-05-24T15:56:54.918Z","comments":true,"path":"2020/03/24/mysql/MySQL实战45讲/43_临时表/","link":"","permalink":"http://yoursite.com/2020/03/24/mysql/MySQL实战45讲/43_临时表/","excerpt":"什么时候会使用临时表","text":"什么时候会使用临时表 1. 临时表1.1 临时表跟内存表 内存表，指的是使用 Memory 引擎的表，建表语法是 create table … engine=memory。这种表的数据都保存在内存里，系统重启的时候会被清空，但是表结构还在。除了这两个特性看上去比较“奇怪”外，从其他的特征上看，它就是一个正常的表。 而临时表，可以使用各种引擎类型 。如果是使用 InnoDB 引擎或者 MyISAM 引擎的临时表，写数据的时候是写到磁盘上的。当然，临时表也可以使用 Memory 引擎 1.2 临时表的特征 从行面可以看到，临时表在使用上有以下几个特点： 一个临时表只能被创建它的 session 访问，对其他线程不可见所以，图中 session A 创建的临时表 t，对于 session B 就是不可见的。 临时表可以与普通表同名。 session A 内有同名的临时表和普通表的时候，show create 语句，以及增删改查语句访问的是临时表。 show tables 命令不显示临时表。 由于临时表只能被创建它的 session 访问，所以在这个 session 结束的时候，会自动删除临时表。 不同 session 的临时表是可以重名的，如果有多个 session 同时执行 join 优化，不需要担心表名重复导致建表失败的问题。 不需要担心数据删除问题。如 1.3 临时表的应用场景由于不用担心线程之间的重名冲突，临时表经常会被用在复杂查询的优化过程中。其中，分库分表系统的跨库查询就是一个典型的使用场景。 一般分库分表的场景，就是要把一个逻辑上的大表分散到不同的数据库实例上。分区 key 的选择是以“减少跨库和跨表查询”为依据的。 但是如果查询条件里面没有用到分区字段，那么该如何实现查询呢，有以下两种思路: 第一种思路是，在 proxy 层的进程代码中实现: 优势是处理速度快，拿到分库的数据以后，直接在内存中参与计算 需要的开发工作量比较大，特别是是对于 group by，join 的操作 对 proxy 端的压力比较大，尤其是很容易出现内存不够用和 CPU 瓶颈的问题 把各个分库拿到的数据，汇总到一个 MySQL 实例的一个表中，然后在这个汇总实例上做逻辑操作。 我们看下面这个示例: 123456789# ht 是一个大的分库分表，分区 key 是 fselect v from ht where k &gt;= M order by t_modified desc limit 100;# 思路二: 使用临时表实现的分库查询，汇总到汇总库# 每个分库的查询select v,k,t_modified from ht_x where k &gt;= M order by t_modified desc limit 100;# 汇总库的查询select v from temp_ht order by t_modified desc limit 100; 按照第二种思路，我们可以这样执行查询: 在汇总库上创建一个临时表 temp_ht，表里包含三个字段 v、k、t_modified； 在各个分库上执行 select v,k,t_modified from ht_x where k &gt;= M order by t_modified desc limit 100; 把分库执行的结果插入到 temp_ht 表中； 执行select v from temp_ht order by t_modified desc limit 100; 我们往往会发现每个分库的计算量都不饱和，所以会直接把临时表 temp_ht 放到 32 个分库中的某一个上。 1.4 为什么临时表可以重命名不同线程可以创建同名的临时表，这是怎么做到的呢？我们来看看MySQL是如何保存临时表的表结构与数据的 执行 create temporary table temp_t(id int primary key)engine=innodb; 临时表的 frm 文件放在临时文件目录下，文件名的后缀是.frm，前缀是#sql{进程 id}_{线程 id}_ 序列号。可以使用 select @@tmpdir 命令，来显示实例的临时文件目录。 关于表中数据的存放方式，在不同的 MySQL 版本中有着不同的处理方式： 在 5.6 以及之前的版本里，MySQL 会在临时文件目录下创建一个相同前缀、以.ibd 为后缀的文件，用来存放数据文件； 从 5.7 版本开始，MySQL 引入了一个临时文件表空间，专门用来存放临时文件的数据。因此，我们就不需要再创建 ibd 文件了。 MySQL 维护数据表，除了物理上要有文件外，内存里面也有一套机制区别不同的表，每个表都对应一个 table_def_key。 个普通表的 table_def_key 的值是由“库名 + 表名”得到的 而对于临时表，table_def_key 在“库名 + 表名”基础上，又加入了“server_id+thread_id”。 在实现上，每个线程都维护了自己的临时表链表。这样每次 session 内操作表的时候，先遍历链表，检查是否有这个名字的临时表，如果有就优先操作临时表，如果没有再操作普通表；在 session 结束的时候，对链表里的每个临时表，执行 “DROP TEMPORARY TABLE + 表名”操作。binlog 中也记录了 DROP TEMPORARY TABLE 这条命令。 临时表的 rename 操作执行 rename table 语句的时候，要求按照“库名 / 表名.frm”的规则去磁盘找文件，但是临时表在磁盘上的 frm 文件是放在 tmpdir 目录下的，并且文件名的规则是“#sql{进程 id}{线程 id} 序列号.frm”，因此会报“找不到文件名”的错误。 1.5 临时表与主从复制临时表只在线程内自己可以访问，为什么需要写到 binlog 里面？ 1234create table t_normal(id int primary key, c int)engine=innodb;/*Q1*/create temporary table temp_t like t_normal;/*Q2*/insert into temp_t values(1,1);/*Q3*/insert into t_normal select * from temp_t;/*Q4*/ 如果关于临时表的操作都不记录，备库在执行到 insert into t_normal 的时候，就会报错“表 temp_t 不存在”。 如果当前的 binlog_format=row，那么跟临时表有关的语句，就不会记录到 binlog 里。也就是说，只在 binlog_format=statment/mixed 的时候，binlog 中才会记录临时表的操作。这种情况下，创建临时表的语句会传到备库执行，因此备库的同步线程就会创建这个临时表。主库在线程退出的时候，会自动删除临时表，但是备库同步线程是持续在运行的。所以，这时候我们就需要在主库上再写一个 DROP TEMPORARY TABLE 传给备库执行。 在 binlog_format=’row’的时候，临时表的操作不记录到 binlog 中，也省去了不少麻烦，这也可以成为你选择 binlog_format 时的一个考虑因素。 MySQL 为什么会重写 drop table 命令MySQL 在记录 binlog 的时候，不论是 create table 还是 alter table 语句，都是原样记录，甚至于连空格都不变。但是如果执行 drop table t_normal，系统记录 binlog 就会写成：DROP TABLE t_normal /* generated by server */ drop table 命令是可以一次删除多个表的。比如，在上面的例子中，设置 binlog_format=row，如果主库上执行 “drop table t_normal, temp_t”这个命令，那么 binlog 中就只能记录：DROP TABLE t_normal /* generated by server */ 因为备库上并没有表 temp_t，将这个命令重写后再传到备库执行，才不会导致备库同步线程停止。所以，drop table 命令记录 binlog 的时候，就必须对语句做改写。/* generated by server */说明了这是一个被服务端改写过的命令。 从服务如何保证临时表不冲突MySQL 在记录 binlog 的时候，会把主库执行这个语句的线程 id 写到 binlog 中。这样，在备库的应用线程就能够知道执行每个语句的主库线程 id，并利用这个线程 id 来构造临时表的 table_def_key： session A 的临时表 t1，在备库的 table_def_key 就是：库名 +t1+“M 的 serverid”+“session A 的 thread_id”; session B 的临时表 t1，在备库的 table_def_key 就是 ：库名 +t1+“M 的 serverid”+“session B 的 thread_id” 2.内存临时表MySQL 什么时候会使用内部临时表？ 如果语句执行过程可以一边读数据，一边直接得到结果，是不需要额外内存的，否则就需要额外的内存，来保存中间结果； join_buffer 是无序数组，sort_buffer 是有序数组，临时表是二维表结构； 如果执行逻辑需要用到二维表特性，就会优先考虑使用临时表。比如我们的例子中，union 需要用到唯一索引约束， group by 还需要用到另外一个字段来存累积计数。 2.1 union union，它的语义是，取这两个子查询结果的并集。并集的意思就是这两个集合加起来，重复的行只保留一行。 1(select 1000 as f) union (select id from t1 order by id desc limit 2); 上面这个 union 语句的执行过程是这样的: 创建一个内存临时表，这个临时表只有一个整型字段 f，并且 f 是主键字段。 执行第一个子查询，得到 1000 这个值，并存入临时表中。 执行第二个子查询： 拿到第一行 id=1000，试图插入临时表中。但由于 1000 这个值已经存在于临时表了，违反了唯一性约束，所以插入失败，然后继续执行； 取到第二行 id=999，插入临时表成功。 从临时表中按行取出数据，返回结果，并删除临时表，结果中包含两行数据分别是 1000 和 999 这里的内存临时表起到了暂存数据的作用，而且计算过程还用上了临时表主键 id 的唯一性约束，实现了 union 的语义。如果把上面这个语句中的 union 改成 union all 的话，就没有了“去重”的语义。这样执行的时候，就依次执行子查询，得到的结果直接作为结果集的一部分，发给客户端。因此也就不需要临时表了。 2.2 group by12345# 会对返回结果排序select id%10 as m, count(*) as c from t1 group by m;# order by null 结果集不排序select id%10 as m, count(*) as c from t1 group by m order by null; 它的 explain 结果如下： Extra 字段： Using index，表示这个语句使用了覆盖索引，选择了索引 a，不需要回表； Using temporary，表示使用了临时表； Using filesort，表示需要排序。 个语句的执行流程是这样的： 创建内存临时表，表里有两个字段 m 和 c，主键是 m； 扫描表 t1 的索引 a，依次取出叶子节点上的 id 值，计算 id%10 的结果，记为 x； 如果临时表中没有主键为 x 的行，就插入一个记录 (x,1); 如果表中有主键为 x 的行，就将 x 这一行的 c 值加 1； 遍历完成后，再根据字段 m 做排序，得到结果集返回给客户端。 临时表大小参数 tmp_table_size 用于控制内存临时表大小的，默认是 16M。如果执行过程中会发现内存临时表大小到达了上限，这时候就会把内存临时表转成磁盘临时表，磁盘临时表默认使用的引擎是 InnoDB。 3. group by 优化3.1 索引优化可以看到，不论是使用内存临时表还是磁盘临时表，group by 逻辑都需要构造一个带唯一索引的表，执行代价很高。 group by 的语义逻辑，是统计不同的值出现的个数。那么，如果扫描过程中可以保证出现的数据是有序的就无须临时表了。在 MySQL 5.7 版本支持了 generated column 机制，用来实现列数据的关联更新。 1alter table t1 add column z int generated always as(id % 100), add index(z); 3.2 直接排序碰上不适合创建索引的场景,如果我们明明知道，一个 group by 语句中需要放到临时表上的数据量特别大，却还是要按照“先放到内存临时表，插入一部分数据后，发现内存临时表不够用了再转成磁盘临时表”，看上去就有点儿傻。 在 group by 语句中加入 SQL_BIG_RESULT 这个提示（hint），就可以告诉优化器：这个语句涉及的数据量很大，请直接用磁盘临时表。 MySQL 的优化器一看，磁盘临时表是 B+ 树存储，存储效率不如数组来得高。所以，既然你告诉我数据量很大，那从磁盘空间考虑，还是直接用数组来存吧。 1select SQL_BIG_RESULT id%100 as m, count(*) as c from t1 group by m; 执行流程就是这样的： 初始化 sort_buffer，确定放入一个整型字段，记为 m； 扫描表 t1 的索引 a，依次取出里面的 id 值, 将 id%100 的值存入 sort_buffer 中； 扫描完成后，对 sort_buffer 的字段 m 做排序（如果 sort_buffer 内存不够用，就会利用磁盘临时文件辅助排序）； 排序完成后，就得到了一个有序数组 据有序数组，得到数组里面的不同值，以及每个值的出现次数。 3.3 group by 使用技巧group by 的几种实现算法，从中可以总结一些使用的指导原则： 如果对 group by 语句的结果没有排序要求，要在语句后面加 order by null； 尽量让 group by 过程用上表的索引，确认方法是 explain 结果里没有 Using temporary 和 Using filesort； 如果 group by 需要统计的数据量不大，尽量只使用内存临时表；也可以通过适当调大 tmp_table_size 参数，来避免用到磁盘临时表； 如果数据量实在太大，使用 SQL_BIG_RESULT 这个提示，来告诉优化器直接使用排序算法得到 group by 的结果 3.4 distinct 和 group by 的性能如果只需要去重，不需要执行聚合函数，distinct 和 group by 哪种效率高一些呢？ 12select a from t group by a order by null;select distinct a from t; 不需要执行聚合函数时，distinct 和 group by 这两条语句的语义和执行流程是相同的，因此执行性能也相同。执行流程是下面这样的。 创建一个临时表，临时表有一个字段 a，并且在这个字段 a 上创建一个唯一索引； 遍历表 t，依次取数据插入临时表中： 如果发现唯一键冲突，就跳过； 否则插入成功； 遍历完成后，将临时表作为结果集返回给客户端。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"23 join","slug":"mysql/MySQL实战45讲/42_join","date":"2020-03-22T16:00:00.000Z","updated":"2020-05-24T15:56:50.988Z","comments":true,"path":"2020/03/23/mysql/MySQL实战45讲/42_join/","link":"","permalink":"http://yoursite.com/2020/03/23/mysql/MySQL实战45讲/42_join/","excerpt":"join 语句是怎么执行的","text":"join 语句是怎么执行的 1. 实验环境12345CREATE TABLE `t2` ( `id` int(11) NOT NULL, `a` int(11) DEFAULT NULL, `b` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `a` (`a`)) ENGINE=InnoDB;drop procedure idata;delimiter ;;create procedure idata()begin declare i int; set i=1; while(i&lt;=1000)do insert into t2 values(i, i, i); set i=i+1; end while;end;;delimiter ;call idata();create table t1 like t2;insert into t1 (select * from t2 where id&lt;=100) 这两个表都有一个主键索引 id 和一个索引 a，字段 b 上无索引。存储过程 idata() 往表 t2 里插入了 1000 行数据，在表 t1 里插入的是 100 行数据。 如果直接使用 join 语句，MySQL 优化器可能会选择表 t1 或 t2 作为驱动表，为了便于分析执行过程中的性能问题，我们改用 straight_join 让 MySQL 使用固定的连接方式执行查询，这样优化器只会按照我们指定的方式去 join。 2. join 的合并算法依据是否能使用被驱动表索引，join 合并算法分为: Index Nested-Loop Join(NLJ): 可以使用被驱动表的索引 Simple Nested-Loop Join: 不能使用被驱动表的索引，执行过程与 Index Nested-Loop Join 类似，MySQL 并没有使用 Block Nested-Loop Join(BNL): 不能使用被驱动表的索引，会将全表扫描转换为内存比较 2.1 Index Nested-Loop Join1select * from t1 straight_join t2 on (t1.a=t2.a); 驱动表走全表扫描，而被驱动表是走树搜索。 假设被驱动表的行数是 M。每次在被驱动表查一行数据，要先搜索索引 a，再搜索主键索引。每次搜索一棵树近似复杂度是以 2 为底的 M 的对数，记为 log2M，所以在被驱动表上查一行的时间复杂度是 2*log2M。假设驱动表的行数是 N，执行过程就要扫描驱动表 N 行，然后对于每一行，到被驱动表上匹配一次。因此整个执行过程，近似复杂度是 N + N*2*log2M。 2.2 Simple Nested-Loop Join驱动表是走全表扫描，被驱动表也是全表扫描，因此近似时间复杂度是N + N*M 2.3 Block Nested-Loop Join1select * from t1 straight_join t2 on (t1.a=t2.b); 这个算法的执行过程是这样的: 把表 t1 的数据读入线程内存 join_buffer 中，由于我们这个语句中写的是 select *，因此是把整个表 t1 放入了内存； 扫描表 t2，把表 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回。 假设小表的行数是 N，大表的行数是 M，那么在这个算法里：两个表都做一次全表扫描，所以总的扫描行数是 M+N；内存中的判断次数是 M*N。因此近似时间复杂度是N + N*M 因此，从时间复杂度上来说，Simple Nested-Loop Join 与 Block Nested-Loop Join 是一样的。但是，Block Nested-Loop Join 算法的M*N次判断是内存操作，速度上会快很多，性能也更好。 join_buffer_sizejoin_buffer 的大小是由参数 join_buffer_size 设定的，默认值是 256k。如果放不下表 t1 的所有数据话，就会分段存在，并执行上面的过程。 假设，驱动表的数据行数是 N，需要分 K 段才能完成算法流程，被驱动表的数据行数是 M， k=size of N /join_buffer_size 所以，在这个算法的执行过程中： 扫描行数是 N+N*M/join_buffer_size； 内存判断 N*M 次。 显然: 使用小表作为驱动表时，扫描行数少 join_buffer_size 越大，对被驱动表的全表扫描次数越少 2.4 join 使用建议 如果可以使用 Index Nested-Loop Join 算法，也就是说可以用上被驱动表上的索引，其实是没问题的； 如果使用 Block Nested-Loop Join 算法，扫描行数就会过多。尤其是在大表上的 join 操作，这样可能要扫描被驱动表很多次，会占用大量的系统资源。所以这种 join 尽量不要用。判断标准就是Extra 字段里面有没有出现“Block Nested Loop”字样。Explain下，没用用index nested-loop 的全要优化。 总是应该使用小表做驱动表。 小表的定义在决定哪个表做驱动表的时候，应该是两个表按照各自的条件过滤，过滤完成之后，计算参与 join 的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表。 1234567# t2 过滤只有 50 行，所以 t2 为小表select * from t1 straight_join t2 on (t1.b=t2.b) where t2.id&lt;=50;select * from t2 straight_join t1 on (t1.b=t2.b) where t2.id&lt;=50;# t1，t2 都是100行，但是 t1 只需要b 字段，t2 需要所有字段，所以 t1 是小表select t1.b,t2.* from t1 straight_join t2 on (t1.b=t2.b) where t2.id&lt;=100;select t1.b,t2.* from t2 straight_join t1 on (t1.b=t2.b) where t2.id&lt;=100; 2.5 BNL算法 对缓存的影响如果一个使用 BNL 算法的 join 语句，多次扫描一个冷表，而且这个语句执行时间超过 1 秒，就会在再次扫描冷表的时候，把冷表的数据页移到 LRU 链表头部。这种情况对应的，是冷表的数据量小于整个 Buffer Pool 的 3/8，能够完全放入 old 区域的情况。如果这个冷表很大，就会出现另外一种情况：业务正常访问的数据页，没有机会进入 young 区域。由于优化机制的存在，一个正常访问的数据页，要进入 young 区域，需要隔 1 秒后再次被访问到。但是，由于我们的 join 语句在循环读磁盘和淘汰内存页，进入 old 区域的数据页，很可能在 1 秒之内就被淘汰了。这样，就会导致这个 MySQL 实例的 Buffer Pool 在这段时间内，young 区域的数据页没有被合理地淘汰。 大表 join 操作虽然对 IO 有影响，但是在语句执行结束后，对 IO 的影响也就结束了。但是，对 Buffer Pool 的影响就是持续性的，需要依靠后续的查询请求慢慢恢复内存命中率。 为了减少这种影响，你可以考虑增大 join_buffer_size 的值，减少对被驱动表的扫描次数。也就是说，BNL 算法对系统的影响主要包括三个方面： 可能会多次扫描被驱动表，占用磁盘 IO 资源； 判断 join 条件需要执行 M*N 次对比（M、N 分别是两张表的行数），如果是大表就会占用非常多的 CPU 资源； 可能会导致 Buffer Pool 的热数据被淘汰，影响内存命中率。 我们执行语句之前，需要通过理论分析和查看 explain 结果的方式，确认是否要使用 BNL 算法。如果确认优化器会使用 BNL 算法，就需要做优化。优化的常见做法是，给被驱动表的 join 字段加上索引，把 BNL 算法转成 BKA 算法。 3. MRRMulti-Range Read 优化 (MRR)主要目的是尽量使用顺序读盘。原理是这样的，因为大多数的数据都是按照主键递增顺序插入得到的，所以我们可以认为，如果按照主键的递增顺序查询的话，对磁盘的读比较接近顺序读，能够提升读性能。 对于语句 select * from t1 where a&gt;=1 and a&lt;=100; a 上有索引，语句的执行过程是这样的: 根据索引 a，定位到满足条件的记录，将 id 值放入 read_rnd_buffer 中 ; 将 read_rnd_buffer 中的 id 进行递增排序； 排序后的 id 数组，依次到主键 id 索引中查记录，并作为结果返回。 这里，read_rnd_buffer 的大小是由 read_rnd_buffer_size 参数控制的。如果步骤 1 中，read_rnd_buffer 放满了，就会先执行完步骤 2 和 3，然后清空 read_rnd_buffer。之后继续找索引 a 的下个记录，并继续循环。 如果你想要稳定地使用 MRR 优化的话，需要设置set optimizer_switch=&quot;mrr_cost_based=off&quot;。（官方文档的说法，是现在的优化器策略，判断消耗的时候，会更倾向于不使用 MRR，把 mrr_cost_based 设置为 off，就是固定使用 MRR 了。） MRR 能够提升性能的核心在于，这条查询语句在索引 a 上做的是一个范围查询（也就是说，这是一个多值查询），可以得到足够多的主键 id 4. Batched Key Access(BKA)MySQL 在 5.6 版本后开始引入的 Batched Key Access(BKA) 算法了。这个 BKA 算法，其实就是对 NLJ 算法的优化。BKA 依赖的就是 MRR。 NLJ 算法执行的逻辑是：从驱动表 t1，一行行地取出 a 的值，再到被驱动表 t2 去做 join。对于表 t2 来说，每次都是匹配一个值。这时，MRR 的优势就用不上了。 我们可以把表 t1 的数据取出来一部分放入 join_buffer 中，然后应用 MRR 要使用BKA，在SQL查询前需要设置:1set optimizer_switch='mrr=on,mrr_cost_based=off,batched_key_access=on'; 使用 BKA 算法的时候，并不是“先计算两个表 join 的结果，再跟第三个表 join”，而是直接嵌套查询的。 5. BNL 算法优化5.1 BNL 转 BKA一些情况下，我们可以直接在被驱动表上建索引，这时就可以直接转成 BKA 算法了。但是，有时候你确实会碰到一些不适合在被驱动表上建索引的情况。比如下面这个语句： 12# t1 有1000行，t2 过滤后有 2000 行因此小表是 t1select * from t1 join t2 on (t1.b=t2.b) where t2.b&gt;=1 and t2.b&lt;=2000; 果使用 BNL 算法来 join 的话，这个语句的执行流程是这样的： 把表 t1 的所有字段取出来，存入 join_buffer 中。这个表只有 1000 行，join_buffer_size 默认值是 256k，可以完全存入。 扫描表 t2，取出每一行数据跟 join_buffer 中的数据进行对比 如果不满足 t1.b=t2.b，则跳过； 如果满足 t1.b=t2.b, 再判断其他条件，也就是是否满足 t2.b 处于[1,2000]的条件，如果是，就作为结果集的一部分返回，否则跳过。 对于表 t2 的每一行，判断 join 是否满足的时候，都需要遍历 join_buffer 中的所有行。因此判断等值条件的次数是 1000*100 万 =10 亿次，这个判断的工作量很大。 这时候，我们可以考虑使用临时表。使用临时表的大致思路是： 把表 t2 中满足条件的数据放在临时表 tmp_t 中； 为了让 join 使用 BKA 算法，给临时表 tmp_t 的字段 b 加上索引； 让表 t1 和 tmp_t 做 join 操作。 123create temporary table temp_t(id int primary key, a int, b int, index(b))engine=innodb;insert into temp_t select * from t2 where b&gt;=1 and b&lt;=2000;select * from t1 join temp_t on (t1.b=temp_t.b); 基于临时表的改进方案，对于能够提前过滤出小数据的 join 语句来说，效果还是很好的； 5.2 扩展 -hash join如果 join_buffer 里面维护的不是一个无序数组，而是一个哈希表的话，那么就不是 10 亿次判断，而是 100 万次 hash 查找。这，也正是 MySQL 的优化器和执行器一直被诟病的一个原因：不支持哈希 join。并且，MySQL 官方的 roadmap，也是迟迟没有把这个优化排上议程。 实际上，这个优化思路，我们可以自己实现在业务端。实现流程大致如下： select * from t1;取得表 t1 的全部 1000 行数据，在业务端存入一个 hash 结构，比如 C++ 里的 set、PHP 的数组这样的数据结构。 select * from t2 where b&gt;=1 and b&lt;=2000; 获取表 t2 中满足条件的 2000 行数据。 把这 2000 行数据，一行一行地取到业务端，到 hash 结构的数据表中寻找匹配的数据。满足匹配的条件的这行数据，就作为结果集的一行 6. 问题6.1 join 的执行顺序问题: 如果用 left join 的话，左边的表一定是驱动表吗？ 如果两个表的 join 包含多个条件的等值匹配，是都要写到 on 里面呢，还是只把一个条件写到 on 里面，其他条件写到 where 部分？ 1234create table a(f1 int, f2 int, index(f1))engine=innodb;create table b(f1 int, f2 int)engine=innodb;insert into a values(1,1),(2,2),(3,3),(4,4),(5,5),(6,6);insert into b values(3,3),(4,4),(5,5),(6,6),(7,7),(8,8); 第二个问题，其实就是下面这两种写法的区别：12select * from a left join b on(a.f1=b.f1) and (a.f2=b.f2); /*Q1*/select * from a left join b on(a.f1=b.f1) where (a.f2=b.f2);/*Q2*/ 在 MySQL 里，NULL 跟任何值执行等值判断和不等值判断的结果，都是 NULL。这里包括， select NULL = NULL 的结果，也是返回 NULL。 因此，语句 Q2 里面 where a.f2=b.f2 就表示“找到这两个表里面，f1、f2 对应同时存在且相同的行。”虽然用的是 left join，但是语义跟 join 是一致的。因此，优化器就把这条语句的 left join 改写成了 join，然后因为表 a 的 f1 上有索引，就把表 b 作为驱动表，这样就可以用上 NLJ 算法。 因此: 这个例子说明，即使我们在 SQL 语句中写成 left join，执行过程还是有可能不是从左到右连接的。也就是说，使用 left join 时，左边的表不一定是驱动表。 如果需要 left join 的语义，就不能把被驱动表的字段放在 where 条件里面做等值判断或不等值判断，必须都写在 on 里面。 join 本身表示的就是同时存在并相等，因此join 将判断条件是否全部放在 on 部分就没有区别了","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"22 常见语句的执行逻辑","slug":"mysql/MySQL实战45讲/41_count","date":"2020-03-21T16:00:00.000Z","updated":"2020-05-24T15:56:46.248Z","comments":true,"path":"2020/03/22/mysql/MySQL实战45讲/41_count/","link":"","permalink":"http://yoursite.com/2020/03/22/mysql/MySQL实战45讲/41_count/","excerpt":"count，order by 都是怎么执行的","text":"count，order by 都是怎么执行的 1. count在不同的 MySQL 引擎中，count(*) 有不同的实现方式: MyISAM: 把一个表的总行数存在了磁盘上，在没有筛选条件时，count(*) 可以直接返回 Innodb: 需要把数据一行一行地从引擎里面读出来，然后累积计数。 由于 Innodb 事务是基于 MVCC 的多版本控制机制实现的，每一行记录都要判断自己是否对这个会话可见，因此对于 count(*) 请求来说，InnoDB 只好把数据一行一行地读出依次判断，可见的行才能够用于计算“基于这个查询”的表的总行数。对于 count(*) 遍历主键索引和二级索引得到的结果逻辑上是一致的。MySQL 优化器会找到最小的那棵树来遍历。在保证逻辑正确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一。 show table status 返回的 TABLE_ROWS 用于显示这个表当前有多少行，但它是通过采样计算得来的，很不准。 那怎么才能快速得到记录总数呢？ 我们只能自己计数。基本思路：你需要自己找一个地方，把操作记录表的行数存起来。 1.1 如何计数我们把这个计数直接放到 MySQL 数据库里单独的一张计数表 C 中，利用事务，我们可以保证计数更新与数据更新之间的一致性。 把计数放在 Redis 里面，不能够保证计数和 MySQL 表里的数据精确一致的原因，是这两个不同的存储构成的系统，不支持分布式事务，无法拿到精确一致的视图。而把计数值也放在 MySQL 中，就解决了一致性视图的问题。 1.2 不同的 count 用法要想弄明白 ount(*)、count(主键 id)、count(字段) 和 count(1) 的差别，首先你要弄清楚 count() 的语义。count() 是一个聚合函数，对于返回的结果集，一行行地判断，如果 count 函数的参数不是 NULL，累计值就加 1，否则不加。最后返回累计值。 所以，count(*)、count(主键 id) 和 count(1) 都表示返回满足条件的结果集的总行数；而 count(字段），则表示返回满足条件的数据行里面，参数“字段”不为 NULL 的总个数。 至于分析性能差别的时候，你可以记住这么几个原则： server 层要什么就给什么； InnoDB 只给必要的值； 现在的优化器只优化了 count(*) 的语义为“取行数”，其他“显而易见”的优化并没有做 性能差别: count(主键 id):InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加 count(1): InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。count(1) 执行得要比 count(主键 id) 快。因为从引擎返回 id 会涉及到解析数据行，以及拷贝字段值的操作。 count(字段): 如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加； 如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加 count(*): 并不会把全部字段取出来，而是专门做了优化，不取值。count(*) 肯定不是 null，按行累加。 按照效率由低到高排序的话，count(字段) -&gt; count(主键 id) -&gt; count(1)≈count(\\*)，所以我建议你，尽量使用 count(*) 2. order byorder by 如何执行取决于如下几个因素: 是否使用外部排序: MySQL 会给每个线程分配一块内存用于排序，称为 sort_buffer。 排序可能在内存中完成，也可能需要使用外部排序，这取决于排序所需的内存和参数 sort_buffer_size。 sort_buffer_size，就是 MySQL 为排序开辟的内存（sort_buffer）的大小 如果要排序的数据量小于 sort_buffer_size，排序就在内存中完成。否则就需要磁盘临时文件辅助排序 单行长度是否太大 默认 MySQL 会使用”全字段排序”，即把所需的所有字段都放到 sort_buffer_size 中然后排序 如果查询要返回的字段很多的话，那么 sort_buffer 里面要放的字段数太多，这样内存里能够同时放下的行数很少，要分成很多个临时文件，排序的性能会很差。此时 MySQL 就会采用另一种排序算法 “rowid 排序” max_length_for_sort_data，是 MySQL 中专门控制用于排序的行数据的长度的一个参数。单行的长度超过这个值，就会使用 rowid 算法 是否有筛选字段与排序字段的联合索引: 可以利用索引的有序性直接排序，下称”索引直接排序” 因此我们将详细下面这几个问题: 如何判断排序语句是否使用了临时文件 全字段排序过程 rowid 排序过程 索引直接排序过程 2.1 是否使用了临时文件假设有个市民表定义如下，我们希望查询城市是“杭州”的所有人名字，并且按照姓名排序返回前 1000 个人的姓名、年龄。123456789101112CREATE TABLE `t` ( `id` int(11) NOT NULL, `city` varchar(16) NOT NULL, `name` varchar(16) NOT NULL, `age` int(11) NOT NULL, `addr` varchar(128) DEFAULT NULL, PRIMARY KEY (`id`), KEY `city` (`city`)) ENGINE=InnoDB;select city,name,age from t where city='杭州' order by name limit 1000 ; 为避免全表扫描，我们需要在 city 字段加上索引。在 city 字段上创建索引之后，我们用 explain 命令来看看这个语句的执行情况。 Extra 这个字段中的“Using filesort”表示的就是需要排序，但是并没有告诉我们MySQL使用了哪种排序是算法，也没有告诉我们是否使用了临时文件排序。用下面介绍的方法，可以确定一个排序语句是否使用了临时文件。 123456789101112131415161718/* 打开optimizer_trace，只对本线程有效 */SET optimizer_trace='enabled=on'; /* @a保存Innodb_rows_read的初始值, mairadb 有所区别 */select VARIABLE_VALUE into @a from performance_schema.session_status where variable_name = 'Innodb_rows_read';/* 执行语句 */select city, name,age from t where city='杭州' order by name limit 1000; /* 查看 OPTIMIZER_TRACE 输出 */SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\\G/* @b保存Innodb_rows_read的当前值 */select VARIABLE_VALUE into @b from performance_schema.session_status where variable_name = 'Innodb_rows_read';/* 计算Innodb_rows_read差值 */select @b-@a; 下面 OPTIMIZER_TRACE 的显示结果(显示的是2.2全字段排序的分析结果) number_of_tmp_files: 表示排序过程中使用的临时文件数。大于 0 表示使用了临时文件排序 examined_rows: 表示参与排序的行数 sort_mode 里面的 packed_additional_fields 的意思是，排序过程对字符串做了“紧凑”处理。即使 name 字段的定义是 varchar(16)，在排序过程中还是要按照实际长度来分配空间的。 最后一个查询语句 select @b-@a 的返回结果是 4000，表示整个执行过程只扫描了 4000 行。 需要注意的是，为了避免对结论造成干扰，我把 internal_tmp_disk_storage_engine 设置成 MyISAM。否则，select @b-@a 的结果会显示为 4001。这是因为查询 OPTIMIZER_TRACE 这个表时，需要用到临时表，而 internal_tmp_disk_storage_engine 的默认值是 InnoDB。如果使用的是 InnoDB 引擎的话，把数据从临时表取出来的时候，会让 Innodb_rows_read 的值加 1。 2.2 全字段排序 如上图，使用全字段排序的过程如下: 初始化 sort_buffer，确定放入 name、city、age 这三个字段； 从索引 city 找到第一个满足 city=’杭州’条件的主键 id； 到主键 id 索引取出整行，取 name、city、age 三个字段的值，存入 sort_buffer 中； 从索引 city 取下一个记录的主键 id； 重复步骤 3、4 直到 city 的值不满足查询条件为止； 对 sort_buffer 中的数据按照字段 name 做快速排序； 按照排序结果取前 1000 行返回给客户端。 2.3 rowid 排序 rowid 算法放入 sort_buffer 的字段，只有要排序的列（即 name 字段）和主键 id。 初始化 sort_buffer，确定放入两个字段，即 name 和 id； 从索引 city 找到第一个满足 city=’杭州’条件的主键 id； 到主键 id 索引取出整行，取 name、id 这两个字段，存入 sort_buffer 中； 从索引 city 取下一个记录的主键 id； 重复步骤 3、4 直到不满足 city=’杭州’条件为止； 对 sort_buffer 中的数据按照字段 name 进行排序； 遍历排序结果，取前 1000 行，并按照 id 的值回到原表中取出 city、name 和 age 三个字段返回给客户端。 rowid 排序多访问了一次表 t 的主键索引，最后的“结果集”是一个逻辑概念，不需要在服务端再耗费内存存储结果，是直接返回给客户端的。 city、name、age 这三个字段的定义总长度是 36，我们把 max_length_for_sort_data 设置为 16，就可以让 MySQL 使用 rowid 进行排序。 12345678910111213141516171819SET max_length_for_sort_data = 16;/* 打开optimizer_trace，只对本线程有效 */SET optimizer_trace='enabled=on'; /* @a保存Innodb_rows_read的初始值, mairadb 有所区别 */select VARIABLE_VALUE into @a from performance_schema.session_status where variable_name = 'Innodb_rows_read';/* 执行语句 */select city, name,age from t where city='杭州' order by name limit 1000; /* 查看 OPTIMIZER_TRACE 输出 */SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\\G/* @b保存Innodb_rows_read的当前值 */select VARIABLE_VALUE into @b from performance_schema.session_status where variable_name = 'Innodb_rows_read';/* 计算Innodb_rows_read差值 */select @b-@a; 重新进行上面的查询分析会看到: sort_mode 变成了 &lt;sort_key, rowid&gt;，表示参与排序的只有 name 和 id 这两个字段。 number_of_tmp_files 变成 10 了，是因为这时候参与排序的行数虽然仍然是 4000 行，但是每一行都变小了，因此需要排序的总数据量就变小了，需要的临时文件也相应地变少了 对于 InnoDB 表来说，rowid 排序会要求回表多造成磁盘读，因此不会被优先选择。这也就体现了 MySQL 的一个设计思想：如果内存够，就要多利用内存，尽量减少磁盘访问。 2.4 索引直接排序我们可以在这个市民表上创建一个 city 和 name 的联合索引，这个索引的示意图如下: 1alter table t add index city_user(city, name); 因为索引是按照city,name 排序的，所以排序过程就变成了: 从索引 (city,name) 找到第一个满足 city=’杭州’条件的主键 id； 到主键 id 索引取出整行，取 name、city、age 三个字段的值，作为结果集的一部分直接返回； 从索引 (city,name) 取下一个记录主键 id； 重复步骤 2、3，直到查到第 1000 条记录，或者是不满足 city=’杭州’条件时循环结束。 如果表上有 (city, name, age)，就可以使用覆盖索引，无须进行上述步骤 2 的回表过程，性能上会快很多。当然，这里并不是说要为了每个查询能用上覆盖索引，就要把语句中涉及的字段都建上联合索引，毕竟索引还是有维护代价的。这是一个需要权衡的决定。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"21 MySQL 表复制","slug":"mysql/MySQL实战45讲/27_表复制","date":"2020-03-20T16:00:00.000Z","updated":"2020-05-24T15:56:42.036Z","comments":true,"path":"2020/03/21/mysql/MySQL实战45讲/27_表复制/","link":"","permalink":"http://yoursite.com/2020/03/21/mysql/MySQL实战45讲/27_表复制/","excerpt":"怎么最快的复制一张表","text":"怎么最快的复制一张表 1. 在两张表中拷贝数据如果可以控制对源表的扫描行数和加锁范围很小的话，我们简单地使用 insert … select 语句即可实现。当然，为了避免对源表加读锁，更稳妥的方案是先将数据写到外部文本文件，然后再写回目标表。这时，有三种常用的方法。 mysqldump 导出 CSV 文件 物理拷贝方法 我们使用下面的试验环境: 假设，我们要把 db1.t 里面 a&gt;900 的数据行导出来，插入到 db2.t 中。1234567891011121314151617181920create database db1;use db1;create table t(id int primary key, a int, b int, index(a))engine=innodb;delimiter ;; create procedure idata() begin declare i int; set i=1; while(i&lt;=1000)do insert into t values(i,i,i); set i=i+1; end while; end;;delimiter ;call idata();create database db2;create table db2.t like db1.t 1.1 mysqldump12345# 数据导出mysqldump -h$host -P$port -u$user --add-locks=0 --no-create-info --single-transaction --set-gtid-purged=OFF db1 t --where=\"a&gt;900\" --result-file=/client_tmp/t.sql# 数据导入mysql -h127.0.0.1 -P13000 -uroot db2 -e \"source /client_tmp/t.sql\" mysqldump 参数: –single-transaction: 在导出数据的时候不需要对表 db1.t 加表锁，而是使用 START TRANSACTION WITH CONSISTENT SNAPSHOT 的方法； –add-locks 设置为 0，表示在输出的文件结果里，不增加” LOCK TABLES t WRITE;” ； –no-create-info 的意思是，不需要导出表结构； –set-gtid-purged=off 表示的是，不输出跟 GTID 相关的信息； –result-file 指定了输出文件的路径，其中 client 表示生成的文件是在客户端机器上的 –skip-extended-insert: 生成的文件中一条 INSERT 语句只插入一行数据的话，默认一条 INSERT 语句里面会包含多个 value 对 –tab 参数，可以同时导出表结构定义文件和 csv 数据文件 1.2 导出 CSV12345# 导出数据select * from db1.t where a&gt;900 into outfile '/server_tmp/t.csv';# 导入数据load data infile '/server_tmp/t.csv' into table db2.t; outfileselect … into outfile，需要注意如下几点。 这条语句会将结果保存在服务端。如果你执行命令的客户端和 MySQL 服务端不在同一个机器上，客户端机器的临时目录下是不会生成 t.csv 文件的。 into outfile 指定了文件的生成位置（/server_tmp/），这个位置必须受参数 secure_file_priv 的限制。参数 secure_file_priv 的可选值和作用分别是： 如果设置为 empty，表示不限制文件生成的位置，这是不安全的设置； 如果设置为一个表示路径的字符串，就要求生成的文件只能放在这个指定的目录，或者它的子目录； 如果设置为 NULL，就表示禁止在这个 MySQL 实例上执行 select … into outfile 操作。 这条命令不会帮你覆盖文件，因此你需要确保 /server_tmp/t.csv 这个文件不存在，否则执行语句时就会因为有同名文件的存在而报错。 这条命令生成的文本文件中，原则上一个数据行对应文本文件的一行。但是，如果字段中包含换行符，在生成的文本中也会有换行符。不过类似换行符、制表符这类符号，前面都会跟上“\\”这个转义符，这样就可以跟字段之间、数据行之间的分隔符区分开。 select …into outfile 方法不会生成表结构文件, 所以我们导数据时还需要单独的命令得到表结构定义。mysqldump 提供了一个–tab 参数，可以同时导出表结构定义文件和 csv 数据文件。 1mysqldump -h$host -P$port -u$user ---single-transaction --set-gtid-purged=OFF db1 t --where=\"a&gt;900\" --tab=$secure_file_priv 这条命令会在 $secure_file_priv 定义的目录下，创建一个 t.sql 文件保存建表语句，同时创建一个 t.txt 文件保存 CSV 数据。 load dataload data infile 的执行流程如下所示。 打开文件 /server_tmp/t.csv，以制表符 (\\t) 作为字段间的分隔符，以换行符（\\n）作为记录之间的分隔符，进行数据读取； 启动事务 判断每一行的字段数与表 db2.t 是否相同： 若不相同，则直接报错，事务回滚； 若相同，则构造成一行，调用 InnoDB 引擎接口，写入到表中。 重复步骤 3，直到 /server_tmp/t.csv 整个文件读入完成，提交事务。 load data 命令有两种用法: 不加“local”，是读取服务端的文件，这个文件必须在 secure_file_priv 指定的目录或子目录下； 加上“local”，读取的是客户端的文件，只要 mysql 客户端有访问这个文件的权限即可。这时候，MySQL 客户端会先把本地文件传给服务端，然后执行上述的 load data 流程。 主从同步如果 binlog_format=statement，这个 load 语句记录到 binlog 里以后，怎么在备库重放呢？主从同步的流程如下: 主库执行完成后，将 /server_tmp/t.csv 文件的内容直接写到 binlog 文件中。 往 binlog 文件中写入语句 load data local infile ‘/tmp/SQL_LOAD_MB-1-0’ INTO TABLE db2.t 把这个 binlog 日志传到备库。 备库的 apply 线程在执行这个事务日志时： a. 先将 binlog 中 t.csv 文件的内容读出来，写入到本地临时目录 /tmp/SQL_LOAD_MB-1-0 中； b. 再执行 load data 语句，往备库的 db2.t 表中插入跟主库相同的数据 为什么要用 load data local,有两点原因: 为了确保备库应用 binlog 正常。因为备库可能配置了 secure_file_priv=null，所以如果不用 local 的话，可能会导入失败，造成主备同步延迟。 另一种应用场景是使用 mysqlbinlog 工具解析 binlog 文件，并应用到目标库的情况 mysqlbinlog $binlog_file | mysql -h$host -P$port -u$user -p$pwd 把日志直接解析出来发给目标库执行。增加 local，就能让这个方法支持非本地的 $host。 1.3 物理拷贝方法前面提到的方法，都是逻辑导数据的方法，也就是将数据从表 db1.t 中读出来，生成文本，然后再写入目标表 db2.t 中。 有物理导数据的方法吗？比如，直接把 db1.t 表的.frm 文件和.ibd 文件拷贝到 db2 目录下。但是这样不行，因为一个 InnoDB 表，除了包含这两个物理文件外，还需要在数据字典中注册。直接拷贝这两个文件的话，因为数据字典中没有 db2.t 这个表，系统是不会识别和接受它们的。 不过，在 MySQL 5.6 版本引入了可传输表空间(transportable tablespace) 的方法，可以通过导出 + 导入表空间的方式，实现物理拷贝表的功能。 假设我们现在的目标是在 db1 库下，复制一个跟表 t 相同的表 r，具体的执行步骤如下 执行 create table r like t，创建一个相同表结构的空表； 执行 alter table r discard tablespace，这时候 r.ibd 文件会被删除； 执行 flush table t for export，这时候 db1 目录下会生成一个 t.cfg 文件； 在 db1 目录下执行 cp t.cfg r.cfg; cp t.ibd r.ibd；这两个命令（这里需要注意的是，拷贝得到的两个文件，MySQL 进程要有读写权限）； 执行 unlock tables，这时候 t.cfg 文件会被删除； 执行 alter table r import tablespace，将这个 r.ibd 文件作为表 r 的新的表空间，由于这个文件的数据内容和 t.ibd 是相同的，所以表 r 中就有了和表 t 相同的数据。 关于拷贝表的这个流程，有以下几个注意点： 在第 3 步执行完 flsuh table 命令之后，db1.t 整个表处于只读状态，直到执行 unlock tables 命令后才释放读锁； 在执行 import tablespace 的时候，为了让文件里的表空间 id 和数据字典中的一致，会修改 r.ibd 的表空间 id。而这个表空间 id 存在于每一个数据页中。因此，如果是一个很大的文件（比如 TB 级别），每个数据页都需要修改，所以你会看到这个 import 语句的执行是需要一些时间的。当然，如果是相比于逻辑导入的方法，import 语句的耗时是非常短的。 1.4 方法对比三种方法的优缺点: 物理拷贝的方式速度最快，尤其对于大表拷贝来说是最快的方法。如果出现误删表的情况，用备份恢复出误删之前的临时库，然后再把临时库中的表拷贝到生产库上，是恢复数据最快的方法。但是，这种方法的使用也有一定的局限性： 必须是全表拷贝，不能只拷贝部分数据； 需要到服务器上拷贝数据，在用户无法登录数据库主机的场景下无法使用； 由于是通过拷贝物理文件实现的，源表和目标表都是使用 InnoDB 引擎时才能使用。 用 mysqldump 生成包含 INSERT 语句文件的方法，可以在 where 参数增加过滤条件，来实现只导出部分数据。这个方式的不足之一是，不能使用 join 这种比较复杂的 where 条件写法。 用 select … into outfile 的方法是最灵活的，支持所有的 SQL 写法。但，这个方法的缺点之一就是，每次只能导出一张表的数据，而且表结构也需要另外的语句单独备份。 后两种方式都是逻辑备份方式，是可以跨引擎使用的。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"20 MySQL 误删恢复","slug":"mysql/MySQL实战45讲/26_误删恢复","date":"2020-03-19T16:00:00.000Z","updated":"2020-05-24T15:56:37.982Z","comments":true,"path":"2020/03/20/mysql/MySQL实战45讲/26_误删恢复/","link":"","permalink":"http://yoursite.com/2020/03/20/mysql/MySQL实战45讲/26_误删恢复/","excerpt":"误删数据恢复","text":"误删数据恢复 1. 误删数据传统的高可用架构是不能预防误删数据的，因为主库的一个 drop table 命令，会通过 binlog 传给所有从库和级联从库，进而导致整个集群的实例都会执行这个命令。 为了找到解决误删数据的更高效的方法，我们需要先对和 MySQL 相关的误删数据，做下分类： 使用 delete 语句误删数据行； 使用 drop table 或者 truncate table 语句误删数据表； 使用 drop database 语句误删数据库； 使用 rm 命令误删整个 MySQL 实例 恢复数据比较安全的做法，是恢复出一个备份，或者找一个从库作为临时库，在这个临时库上执行这些操作，然后再将确认过的临时库的数据，恢复回主库。 这是因为，一个在执行线上逻辑的主库，数据状态的变更往往是有关联的。可能由于发现数据问题的时间晚了一点儿，就导致已经在之前误操作的基础上，业务代码逻辑又继续修改了其他数据。所以，如果这时候单独恢复这几行数据，而又未经确认的话，就可能会出现对数据的二次破坏。 1.1 误删预防误删更重要的是事前预防: 把 sql_safe_updates 参数设置为 on。这样一来，如果我们忘记在 delete 或者 update 语句中写 where 条件，或者 where 条件里面没有包含索引字段的话，这条语句的执行就会报错。 码上线前，必须经过 SQL 审计。 账号分离，避免写错命令。 第二条建议是，制定操作规范。这样做的目的，是避免写错要删除的表名 在删除数据表之前，必须先对表做改名操作。然后，观察一段时间，确保对业务无影响以后再删除这张表。 改表名的时候，要求给表名加固定的后缀（比如加 _to_be_deleted)，然后删除表的动作必须通过管理系统执行。并且，管理系删除表的时候，只能删除固定后缀的表。 脚本分别是：备份脚本、执行脚本、验证脚本和回滚脚本。如果能够坚持做到，即使出现问题，也是可以很快恢复的，一定能降低出现故障的概率。 2. 误删行使用 delete 语句误删了数据行，可以用 Flashback 工具通过闪回把数据恢复回来。而能够使用这个方案的前提是，需要确保 binlog_format=row 和 binlog_row_image=FULL。 Flashback 恢复数据的原理，是修改 binlog 的内容，拿回原库重放。 对于 insert 语句，对应的 binlog event 类型是 Write_rows event，把它改成 Delete_rows event 即可； 对于 delete 语句，也是将 Delete_rows event 改为 Write_rows event； 而如果是 Update_rows 的话，binlog 里面记录了数据行修改前和修改后的值，对调这两行的位置即可。 但是，delete 全表是很慢的，需要生成回滚日志、写 redo、写 binlog。所以，从性能角度考虑，你应该优先考虑使用 truncate table 或者 drop table 命令。 3. 误删库 / 表使用 truncate /drop table 和 drop database 命令删除的数据，就没办法通过 Flashback 来恢复了。因为即使我们配置了 binlog_format=row，执行这三个命令时，记录的 binlog 还是 statement 格式。binlog 里面就只有一个 truncate/drop 语句，这些信息是恢复不出数据的。 这个时候就需要使用全量备份，加增量日志了，这个方案要求线上有定期的全量备份，并且实时备份 binlog。 恢复数据的流程如下: 取最近一次全量备份，假设这个库是一天一备，上次备份是当天 0 点； 用备份恢复出一个临时库； 从日志备份里面，取出凌晨 0 点之后的日志； 把这些日志，除了误删除数据的语句外，全部应用到临时库 关于这个过程，需要说明的是: mysqlbinlog 有一个–database 参数，用来指定误删表所在的库，这样可以跳过其他库，加快恢复速度 需要跳过 1 2 点误操作的语句: 使用了 GTID 模式: 假设误操作命令的 GTID 是 gtid1，那么只需要执行 set gtid_next=gtid1;begin;commit; 就可以跳过误删的操作 未使用 GTID 模式: 手动使用 –start-position –stop-position 跳过误删的操作 这个恢复的操作还是不够快，原因有以下两点: 如果是误删表，最好就是只恢复出这张表，也就是只重放这张表的操作，但是 mysqlbinlog 工具并不能指定只解析一个表的日志； 用 mysqlbinlog 解析出日志应用，应用日志的过程就只能是单线程。 3.1 使用备库同步方式恢复数据更快的方法是在用备份恢复出临时实例之后，将这个临时实例设置成线上备库的从库，这样： 在 start slave 之前，先通过执行 change replication filter replicate_do_table = (tbl_name) 命令，就可以让临时库只同步误操作的表； 这样做可以用上并行复制技术，来加速整个数据恢复过程。 关于这个过程需要说明的是: binlog 备份系统到线上备库有一条虚线，是指如果由于时间太久，备库上已经删除了临时实例需要的 binlog 的话，我们可以从 binlog 备份系统中找到需要的 binlog，再放回备库中。 同步过程，同样需要跳过误删的操作 删掉的 binlog 放回备库的操作步骤 从备份系统下载 master.000005 和 master.000006 这两个文件，放到备库的日志目录下； 打开日志目录下的 master.index 文件，在文件开头加入两行，内容分别是 “./master.000005”和“./master.000006”; 重启备库，目的是要让备库重新识别这两个日志文件； 3.2 延迟复制备库上面利用备库并行复制的方案仍然存在恢复时间不可控问题。如果一个库的备份特别大，或者误操作的时间距离上一个全量备份的时间较长，这个恢复时间可能是要按天来计算的。 我们可以考虑搭建延迟复制的备库。这个功能是 MySQL 5.6 版本引入的。 一般的主备复制结构存在的问题是，如果主库上有个表被误删了，这个命令很快也会被发给所有从库，进而导致所有从库的数据表也都一起被误删了。 延迟复制的备库是一种特殊的备库，通过 CHANGE MASTER TO MASTER_DELAY = N命令，可以指定这个备库持续保持跟主库有 N 秒的延迟。只要在延迟的时间内发现误删，这个命令就还没有在这个延迟复制的备库执行。这时候到这个备库上执行 stop slave，再通过之前介绍的方法，跳过误操作命令，就可以恢复出需要的数据。 4. rm 删除数据对于一个有高可用机制的 MySQL 集群来说，最不怕的就是 rm 删除数据了。只要不是恶意地把整个集群删除，而只是删掉了其中某一个节点的数据的话，HA 系统就会开始工作，选出一个新的主库，从而保证整个集群的正常工作。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"19 MySQL 主库监测","slug":"mysql/MySQL实战45讲/25_主库监测","date":"2020-03-18T16:00:00.000Z","updated":"2020-05-24T15:56:34.266Z","comments":true,"path":"2020/03/19/mysql/MySQL实战45讲/25_主库监测/","link":"","permalink":"http://yoursite.com/2020/03/19/mysql/MySQL实战45讲/25_主库监测/","excerpt":"怎么判断一个主库是否出了问题？","text":"怎么判断一个主库是否出了问题？ 1. 主库监测在一主一备的双 M 架构里，主备切换只需要把客户端流量切到备库；而在一主多从架构里，主备切换除了要把客户端流量切到备库外，还需要把从库接到新主库上。 主备切换有两种场景，一种是主动切换，一种是被动切换。而其中被动切换，往往是因为主库出问题了，由 HA 系统发起的。那怎么判断主库是否出了问题，有以下几种方案: select 1 判断 1.1 select 1 判断select 1 成功返回，只能说明这个库的进程还在，并不能说明主库没问题。 innodb_thread_concurrency 参数的目的是，控制 InnoDB 的并发线程上限。也就是说，一旦并发线程数达到这个值，InnoDB 在接收到新请求的时候，就会进入等待状态，直到有线程退出。此时select 1 是能执行成功的，但是查询表 t 的语句会被堵住，系统已经处于不可用状态。 innodb_thread_concurrencyinnodb_thread_concurrency 这个参数的默认值是 0，表示不限制并发线程数量。但是，不限制并发线程数肯定是不行的。因为，一个机器的 CPU 核数有限，线程全冲进来，上下文切换的成本就会太高。 通常情况下，我们建议把 innodb_thread_concurrency 设置为 64~128 之间的值。 需要注意的是 线程处于空闲状态，不算在并发线程里面。 在线程进入锁等待以后，也就是说等行锁（也包括间隙锁）的线程也不算在并发线程内 对于真正执行的查询，比如 select sleep(100) from t，还是要算进并发线程内 并发连接与并发查询show processlist 的结果指的就是并发连接。而“当前正在执行”的语句，才是我们所说的并发查询。 并发连接数达到几千个影响并不大，就是多占一些内存而已 并发查询太高才是 CPU 杀手。这也是为什么我们需要设置 innodb_thread_concurrency 参数的原因 1.2 更新判断为了能够检测 InnoDB 并发线程数过多导致的系统不可用情况，我们需要找一个访问 InnoDB 的场景。 同时更新事务要写 binlog，而一旦 binlog 所在磁盘的空间占用率达到 100%，那么所有的更新语句和事务提交的 commit 语句就都会被堵住。但是，系统这时候还是可以正常读数据的。因此我们应该使用更新语句，而不是查询语句作为监控语句。更新语句类似于: 1mysql&gt; update mysql.health_check set t_modified=now(); timestamp 字段，用来表示最后一次执行检测的时间。 对于双主模型，如果主库 A 和备库 B 都用相同的更新命令进行监测，就可能出现行冲突。我们可以在 mysql.health_check 表上存入多行数据，并用 A、B 的 server_id 做主键。 更新判断是一个相对比较常用的方案了，不过依然存在一些问题。其中，“判定慢”一直是让 DBA 头疼的问题。这里涉及到的是服务器 IO 资源分配的问题。检测使用的 update 命令，需要的IO资源很少，大概率可以执行成功。但可能此时系统的 IO 利用率已经达到 100%，正常的 SQL 已经执行的很慢了。 根本原因是我们上面说的所有方法，都是基于外部检测的。外部检测天然有一个问题，就是随机性。因为，外部检测都需要定时轮询，所以系统可能已经出问题了，但是却需要等到下一个检测发起执行语句的时候，我们才有可能发现问题。 1.3 内部统计MySQL 5.6 版本以后提供的 performance_schema 库，就在 file_summary_by_event_name 表里统计了每次 IO 请求的时间。 1234567891011121314151617181920212223242526272829MariaDB [performance_schema]&gt; select * from file_summary_by_event_name where event_name like \"%innodb_log%\" \\G*************************** 1. row *************************** EVENT_NAME: wait/io/file/innodb/innodb_log_file # 统计的是 redo log 的写入时间 COUNT_STAR: 94 # 所有 IO 的总次数 SUM_TIMER_WAIT: 247391627463 # 所有 IO 类型的统计，单位是皮秒 MIN_TIMER_WAIT: 0 AVG_TIMER_WAIT: 2631825709 MAX_TIMER_WAIT: 11638807401 COUNT_READ: 6 # 读操作的统计 SUM_TIMER_READ: 30000390 MIN_TIMER_READ: 0 AVG_TIMER_READ: 5000065 MAX_TIMER_READ: 13752883 SUM_NUMBER_OF_BYTES_READ: 68096 # 总共从 redo log 统计了多少字节 COUNT_WRITE: 43 # 写操作统计 SUM_TIMER_WRITE: 1621529472 MIN_TIMER_WRITE: 0 AVG_TIMER_WRITE: 37709927 MAX_TIMER_WRITE: 73538815SUM_NUMBER_OF_BYTES_WRITE: 24576 COUNT_MISC: 45 # 其他类型的统计，对于redolog 就是 fsync SUM_TIMER_MISC: 245740097601 MIN_TIMER_MISC: 0 AVG_TIMER_MISC: 5460890834 MAX_TIMER_MISC: 116388074011 row in set (0.001 sec) 启用 performance_schema打开所有的 performance_schema 项，性能大概会下降 10% 左右。所以，我建议你只打开自己需要的项进行统计。你可以通过下面的方法打开或者关闭某个具体项的统计。 12# 打开 redo log 的统计项mysql&gt; update setup_instruments set ENABLED='YES', Timed='YES' where name like '%wait/io/file/innodb/innodb_log_file%'; 异常检测比如，你可以设定阈值，单次 IO 请求时间超过 200 毫秒属于异常，然后使用类似下面这条语句作为检测逻辑。1234mysql&gt; select event_name,MAX_TIMER_WAIT FROM performance_schema.file_summary_by_event_name where event_name in ('wait/io/file/innodb/innodb_log_file','wait/io/file/sql/binlog') and MAX_TIMER_WAIT&gt;200*1000000000;# 清空统计数据，以免对下次检测产生影响mysql&gt; truncate table performance_schema.file_summary_by_event_name; 1.4 方案总结建议是优先考虑 update 系统表，然后再配合增加检测 performance_schema 的信息。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"18 MySQL 读写分离","slug":"mysql/MySQL实战45讲/24_读写分离","date":"2020-03-17T16:00:00.000Z","updated":"2020-05-24T15:56:30.523Z","comments":true,"path":"2020/03/18/mysql/MySQL实战45讲/24_读写分离/","link":"","permalink":"http://yoursite.com/2020/03/18/mysql/MySQL实战45讲/24_读写分离/","excerpt":"读写分离，以及怎么处理主备延迟导致的读写分离问题。","text":"读写分离，以及怎么处理主备延迟导致的读写分离问题。 1. 读写分离读写分离的主要目标就是分摊主库的压力。一般有两种架构: 客户端（client）主动做负载均衡，即由客户端自己选择连接的 mysql 服务器 在 MySQL 和客户端之间有一个中间代理层 proxy，客户端只连接 proxy， 由 proxy 根据请求类型和上下文决定请求的分发路由。 两种架构的对比: 客户端直连方案，因为少了一层 proxy 转发，所以查询性能稍微好一点儿，并且整体架构简单，排查问题更方便。但是这种方案，由于要了解后端部署细节，所以在出现主备切换、库迁移等操作的时候，客户端都会感知到，并且需要调整数据库连接信息。一般采用这样的架构，一定会伴随一个负责管理后端的组件，比如 Zookeeper，尽量让业务端只专注于业务逻辑开发。 带 proxy 的架构，对客户端比较友好。客户端不需要关注后端细节，连接维护、后端信息维护等工作，都是由 proxy 完成的。但这样的话，对后端维护团队的要求会更高。而且，proxy 也需要有高可用架构。因此，带 proxy 架构的整体就相对比较复杂。 但是无论那种将架构，由于主从可能存在延迟，客户端执行完一个更新事务后马上发起查询，如果查询选择的是从库的话，就有可能读到刚刚的事务更新之前的状态。 主从延迟还是不能 100% 避免的。不论哪种结构，客户端都希望查询从库的数据结果，跟查主库的数据结果是一样的。接下来，我们就来讨论怎么处理过期读问题。通常有下面这些解决方案: 强制走主库方案； sleep 方案； 判断主备无延迟方案； 配合 semi-sync 方案； 等主库位点方案；等 GTID 方案。 1.1 强制走主库方案 对于必须要拿到最新结果的请求，强制将其发到主库上。 对于可以读到旧数据的请求，才将其发到从库上 这个方案最大的问题在于，有时候你会碰到“所有查询都不能是过期读”的需求。 1.2 Sleep 方案这个方案的假设是，大多数情况下主备延迟在 1 秒之内，主库更新后，读从库之前先 sleep 一下。 典型的场景是商品发布后，用 Ajax（Asynchronous JavaScript + XML，异步 JavaScript 和 XML）直接把客户端输入的内容作为“新的商品”显示在页面上，而不是真正地去数据库做查询。等到卖家再刷新页面，去查看商品的时候，其实已经过了一段时间，也就达到了 sleep 的目的，进而也就解决了过期读的问题。 如果这个查询请求本来 0.5 秒就可以在从库上拿到正确结果，也会等 1 秒；如果延迟超过 1 秒，还是会出现过期读。 1.3 判断主备无延迟方案确保备库无延迟，通常有三种做法 查询前，判断 seconds_behind_master 是否为 0 采用对比位点和 GTID 的方法来确保主备无延迟 下图是 show slave status 结果的部分截图 对比位点 Master_Log_File 和 Read_Master_Log_Pos，表示的是读到的主库的最新位点； Relay_Master_Log_File 和 Exec_Master_Log_Pos，表示的是备库执行的最新位点。 这两组值完全相同，就表示接收到的日志已经同步完成。 对比 GTID 集合 Auto_Position=1 ，表示这对主备关系使用了 GTID 协议。 Retrieved_Gtid_Set，是备库收到的所有日志的 GTID 集合； Executed_Gtid_Set，是备库所有已经执行完成的 GTID 集合。 对比位点和对比 GTID 这两种方法，都要比判断 seconds_behind_master 是否为 0 更准确。但还是没有达到“精确”的程度。 一个事务的 binlog 在主备库之间的状态： 主库执行完成，写入 binlog，并反馈给客户端； binlog 被从主库发送给备库，备库收到； 在备库执行 binlog 完成。 们上面判断主备无延迟的逻辑，是“备库收到的日志都执行完成了”。但是，存在客户端已经收到提交确认，而备库还没收到日志的状态。要解决这个问题，需要引入“半同步复制”。semi-sync 配合前面关于位点的判断，就能够确定在从库上执行的查询请求，可以避免过期读。 1.4 配合 semi-syncsemi-sync 做了这样的设计： 事务提交的时候，主库把 binlog 发给从库； 从库收到 binlog 以后，发回给主库一个 ack，表示收到了； 主库收到这个 ack 以后，才能给客户端返回“事务完成”的确认。 也就是说，如果启用了 semi-sync，就表示所有给客户端发送过确认的事务，都确保了备库已经收到了这个日志。 semi-sync+ 位点判断的方案，只对一主一备的场景是成立的。在一主多从场景中，主库只要等到一个从库的 ack，就开始给客户端返回确认。这时对从库的查询请求，落在这个响应了 ack 的从库上，是能够确保读到最新数据；如果不是还是有可能发生过期读。 判断同步位点的方案还有另外一个潜在的问题，即：如果在业务更新的高峰期，主库的位点或者 GTID 集合更新很快，那么上面的两个位点等值判断就会一直不成立，很可能出现从库上迟迟无法响应查询请求的情况。 小结一下，semi-sync 配合判断主备无延迟的方案，存在两个问题： 一主多从的时候，在某些从库执行查询请求会存在过期读的现象； 在持续延迟的情况下，可能出现过度等待的问题。 等主库位点方案，就可以解决这两个问题。 1.5 等主库位点要明白等主库位点，要先明白这条命令:select master_pos_wait(file, pos[, timeout]); 执行逻辑: 它是在从库执行的； 参数 file 和 pos 指的是主库上的文件名和位置； timeout 可选，设置为正整数 N 表示这个函数最多等待 N 秒。 返回结果: 正常返回的结果是一个正整数 M，表示从命令开始执行，到应用完 file 和 pos 表示的 binlog 位置，执行了多少事务 如果执行期间，备库同步线程发生异常，则返回 NULL； 如果等待超过 N 秒，就返回 -1； 如果刚开始执行的时候，就发现已经执行过这个位置了，则返回 0 使用等等主库位点，进行查询的逻辑是这样的: trx1 事务更新完成后，马上执行 show master status 得到当前主库执行到的 File 和 Position； 选定一个从库执行查询语句； 在从库上执行 select master_pos_wait(File, Position, 1)； 如果返回值是 &gt;=0 的正整数，则在这个从库执行查询语句； 否则，到主库执行查询语句 步骤 5 到主库执行查询语句，是这类方案常用的退化机制。因为从库的延迟时间不可控，不能无限等待，所以如果等待超时，就应该放弃，然后到主库去查。如果所有的从库都延迟超过 1 秒了，那查询压力不就都跑到主库上了吗？确实是这样。 按照我们设定不允许过期读的要求，就只有两种选择，一种是超时放弃，一种是转到主库查询。具体怎么选择，就需要业务开发同学做好限流策略了。 1.6 GTID 方案select wait_for_executed_gtid_set(gtid_set, 1); 执行逻辑: 等待，直到这个库执行的事务中包含传入的 gtid_set，返回 0； 超时返回 1。在前面等位点的方案中 在前面等位点的方案中，我们执行完事务后，还要主动去主库执行 show master status。而 MySQL 5.7.6 版本开始，允许在执行完更新类事务后，把这个事务的 GTID 返回给客户端，这样等 GTID 的方案就可以减少一次查询。 等 GTID 的执行流程就变成了： trx1 事务更新完成后，从返回包直接获取这个事务的 GTID，记为 gtid1； 选定一个从库执行查询语句； 在从库上执行 select wait_for_executed_gtid_set(gtid1, 1)； 如果返回值是 0，则在这个从库执行查询语句； 否则，到主库执行查询语句 跟等主库位点的方案一样，等待超时后是否直接到主库查询，需要业务开发同学来做限流考虑。 怎么能够让 MySQL 在执行事务后，返回包中带上 GTID 呢？你只需要将参数 session_track_gtids=OWN_GTID，然后通过 API 接口 mysql_session_track_get_first 从返回包解析出 GTID 的值即可。 1.7 方案总结在实际应用中，这几个方案是可以混合使用的。 比如，先在客户端对请求做分类，区分哪些请求可以接受过期读，而哪些请求完全不能接受过期读；然后，对于不能接受过期读的语句，再使用等 GTID 或等位点的方案。 但话说回来，过期读在本质上是由一写多读导致的。在实际应用中，可能会有别的不需要等待就可以水平扩展的数据库方案，但这往往是用牺牲写性能换来的，也就是需要在读性能和写性能中取权衡。 1.8 大事务的影响对大表做 DDL 的时候会怎么样。假设，这条语句在主库上要执行 10 分钟，提交后传到备库就要 10 分钟（典型的大事务）。那么，在主库 DDL 之后再提交的事务的 GTID，去备库查的时候，就会等 10 分钟才出现。 这样，这个读写分离机制在这 10 分钟之内都会超时，然后走主库。 这种预期内的操作，应该在业务低峰期的时候，确保主库能够支持所有业务查询，然后把读请求都切到主库，再在主库上做 DDL。等备库延迟追上以后，再把读请求切回备库。当然了，使用 gh-ost 方案来解决这个问题也是不错的选择。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"17 MySQL 主备切换","slug":"mysql/MySQL实战45讲/23_主备切换","date":"2020-03-16T16:00:00.000Z","updated":"2020-05-24T15:56:25.770Z","comments":true,"path":"2020/03/17/mysql/MySQL实战45讲/23_主备切换/","link":"","permalink":"http://yoursite.com/2020/03/17/mysql/MySQL实战45讲/23_主备切换/","excerpt":"MySQL 主备切换策略，一主多从","text":"MySQL 主备切换策略，一主多从 1. 双主模型的主备切换策略如图 1 所示就是基本的主备切换流程 正常情况下，只要主库执行更新生成的所有 binlog，都可以传到备库并被正确地执行，备库就能达到跟主库一致的状态，这就是最终一致性。但是，MySQL 要提供高可用能力，只有最终一致性是不够的。由于主备延迟的存在，所以在主备切换的时候，就相应的有不同的策略。 1.1 可靠性优先策略 如上图，可靠性优先的主备切换流程: 判断备库 B 现在的 seconds_behind_master，如果小于某个值（比如 5 秒）继续下一步，否则持续重试这一步； 把主库 A 改成只读状态，即把 readonly 设置为 true； 判断备库 B 的 seconds_behind_master 的值，直到这个值变成 0 为止； 把备库 B 改成可读写状态，也就是把 readonly 设置为 false；把业务请求切到备库 B 以看到，这个切换流程中是有不可用时间的。因为在步骤 2 之后，主库 A 和备库 B 都处于 readonly 状态，也就是说这时系统处于不可写状态，直到步骤 5 完成后才能恢复。 假设，主库 A 和备库 B 间的主备延迟是 30 分钟，这时候主库 A 掉电了。此时我们必须等待备库执行完中转日志，才能切换到备库 B。这段时间，系统处于完全不可用的状态。所以在满足数据可靠性的前提下，MySQL 高可用系统的可用性，是依赖于主备延迟的。延迟的时间越小，在主库故障的时候，服务恢复需要的时间就越短，可用性就越高。 1.2 可用性优先如果强行把步骤 4、5 调整到最开始执行，也就是说不等主备数据同步，那么系统几乎就没有不可用时间了。代价就是可能出现数据不一致的情况。 大多数情况下，应该使用可靠性优先策略。毕竟对数据服务来说的话，数据的可靠性一般还是要优于可用性的。在这个基础上，通过减少主备延迟，提升系统的可用性。 2. 一主多从大多数的互联网应用场景都是读多写少，因此你负责的业务，在发展过程中很可能先会遇到读性能的问题。此时我们就需要 MySQL 一主多从架构。我们将分成两个方面来讲解一主多从: 一主多从的切换正确性 一主多从的查询逻辑正确性的方法: 见下一节”读写分离” 下面，就是一个基本的一主多从结构: 虚线箭头表示的是主备关系，也就是 A 和 A’互为主备 从库 B、C、D 指向的是主库 A。 一主多从的设置，一般用于读写分离，主库负责所有的写入和一部分读，其他的读请求则由从库分担。 3. 一主多从的主备切换如图 2 所示，就是主库发生故障，主备切换后的结果。 相比于一主一备的切换流程，一主多从结构在切换完成后，A’会成为新的主库，从库 B、C、D 也要改接到 A’。 3.1 基于位点的主备切换当我们把节点 B 设置成节点 A’的从库的时候，需要执行一条 change master 命令:1234567CHANGE MASTER TO MASTER_HOST=$host_name MASTER_PORT=$port MASTER_USER=$user_name MASTER_PASSWORD=$password MASTER_LOG_FILE=$master_log_name MASTER_LOG_POS=$master_log_pos 最后两个参数 MASTER_LOG_FILE 和 MASTER_LOG_POS 表示，要从主库的 master_log_name 文件的 master_log_pos 这个位置的日志继续同步。而这个位置就是我们所说的同步位点，也就是主库对应的文件名和日志偏移量。 原来节点 B 是 A 的从库，本地记录的也是 A 的位点。但是相同的日志，A 的位点和 A’的位点是不同的。因此，从库 B 要切换的时候，就需要先经过“找同步位点”这个逻辑。这个位点很难精确取到。 一种取同步位点的方法是这样的： 等待新主库 A’把中转日志（relay log）全部同步完成； 在 A’上执行 show master status 命令，得到当前 A’上最新的 File 和 Position； 取原主库 A 故障的时刻 T； 用 mysqlbinlog 工具解析 A’的 File，得到 T 时刻的位点。 1mysqlbinlog File --stop-datetime=T --start-datetime=T 网络延迟的不确定性，从节点 B 是否已经执行过T时刻的位点是不确定的，因此我们从时刻 T 的位点同步时就有可能出现主键冲突(insert 语句被重复执行)。 通常情况下，我们在切换任务的时候，要先主动跳过这些错误，有两种常用的方法。123456# 方法一: 手动跳过一个事务# 从库 B 刚开始接到新主库 A’时，持续观察，每次碰到这些错误就停下来，执行一次跳过命令set global sql_slave_skip_counter=1;start slave;# 方法二: 通过设置 slave_skip_errors 参数，直接设置跳过指定的错误。 在执行主备切换时，有这么两类错误，是经常会遇到的： 1062 错误是插入数据时唯一键冲突； 1032 错误是删除数据时找不到行。 我们可以把 slave_skip_errors 设置为 “1032,1062”，这样中间碰到这两个错误时就直接跳过。 这个背景是，我们很清楚在主备切换过程中，直接跳过 1032 和 1062 这两类错误是无损的，所以才可以这么设置 slave_skip_errors 参数。等到主备间的同步关系建立完成，并稳定执行一段时间之后，我们还需要把这个参数设置为空，以免之后真的出现了主从数据不一致，也跳过了。 3.2 GTID基于位点的主备切换，复杂也容易出错，所以，MySQL 5.6 版本引入了 GTID，彻底解决了这个困难。 GTID 的全称是 Global Transaction Identifier，也就是全局事务 ID，是一个事务在提交的时候生成的，是这个事务的唯一标识。它由两部分组成，格式是：GTID=server_uuid:gno server_uuid 是一个实例第一次启动时自动生成的，是一个全局唯一的值； gno 是一个整数，初始值是 1，每次提交事务的时候分配给这个事务，并加 1。 注意: 事务 id 是在事务执行过程中分配的，如果这个事务回滚了，事务 id 也会递增，而 gno 是在事务提交的时候才会分配。 GTID 的使用GTID 模式的启动也很简单，只需配置两个参数: gtid_mode=on enforce_gtid_consistency=on 在 GTID 模式下，每个事务都会跟一个 GTID 一一对应。这个 GTID 有两种生成方式，而使用哪种方式取决于 session 变量 gtid_next 的值。 gtid_next=automatic gtid_next 是一个指定的 GTID 的值 gtid_next=automatic 代表使用默认值。这时，MySQL 就会把 server_uuid:gno 分配给这个事务。 记录 binlog 的时候，先记录一行 SET @@SESSION.GTID_NEXT=‘server_uuid:gno’; 把这个 GTID 加入本实例的 GTID 集合 如果 gtid_next 是一个指定的 GTID 的值，比如通过 set gtid_next=’current_gtid’指定为 current_gtid，那么就有两种可能： 如果 current_gtid 已经存在于实例的 GTID 集合中，接下来执行的这个事务会直接被系统忽略； 如果 current_gtid 没有存在于实例的 GTID 集合中，就将这个 current_gtid 分配给接下来要执行的事务，也就是说系统不需要给这个事务生成新的 GTID，因此 gno 也不用加 1 一个 current_gtid 只能给一个事务使用。这个事务提交后，如果要执行下一个事务，就要执行 set 命令，把 gtid_next 设置成另外一个 gtid 或者 automatic。这样，每个 MySQL 实例都维护了一个 GTID 集合，用来对应“这个实例执行过的所有事务”。 通过提交一个特定 GTID 的空事务，我们就可以实现跳过主服务器同步过来的特定时事务: 12345678# 把这个 GTID 加到从库 的 GTID 集合中set gtid_next='aaaaaaaa-cccc-dddd-eeee-ffffffffffff:10';begin;commit;# 恢复 GTID 的默认分配行为set gtid_next=automatic;start slave; 4. 基于 GTID 的主备切换在 GTID 模式下，备库 B 要设置为新主库 A’的从库的语法如下：123456CHANGE MASTER TO MASTER_HOST=$host_name MASTER_PORT=$port MASTER_USER=$user_name MASTER_PASSWORD=$password master_auto_position=1 master_auto_position=1 就表示这个主备关系使用的是 GTID 协议。我们把现在这个时刻，实例 A’的 GTID 集合记为 set_a，实例 B 的 GTID 集合记为 set_b。接下来，我们就看看现在的主备切换逻辑。 我们在实例 B 上执行 start slave 命令，取 binlog 的逻辑是这样的： 实例 B 指定主库 A’，基于主备协议建立连接。 实例 B 把 set_b 发给主库 A’。 实例 A’算出所有存在于 set_a，但是不存在于 set_b 的 GTID 的集合，判断 A’本地是否包含了这个差集需要的所有 binlog 事务。 如果不包含，表示 A’已经把实例 B 需要的 binlog 给删掉了，直接返回错误； 如果确认全部包含，A’从自己的 binlog 文件里面，找出第一个不在 set_b 的事务，发给 B； 之后就从这个事务开始，往后读文件，按顺序取 binlog 发给 B 去执行 在基于 GTID 的主备关系里，系统认为只要建立主备关系，就必须保证主库发给备库的日志是完整的。跟基于位点的主备协议不同。基于位点的协议，是由备库决定的，备库指定哪个位点，主库就发哪个位点，不做日志的完整性判断。 之后这个系统就由新主库 A’写入，主库 A’的自己生成的 binlog 中的 GTID 集合格式是：server_uuid_of_A’:1-M。如果之前从库 B 的 GTID 集合格式是 server_uuid_of_A:1-N， 那么切换之后 GTID 集合的格式就变成了 server_uuid_of_A:1-N, server_uuid_of_A’:1-M。当然，主库 A’之前也是 A 的备库，因此主库 A’和从库 B 的 GTID 集合是一样的。这就达到了我们预期。 4.1 问题在 GTID 模式下，如果一个新的从库接上主库，但是需要的 binlog 已经没了，要怎么做？ 如果业务允许主从不一致的情况，那么可以在主库上先执行 show global variables like ‘gtid_purged’，得到主库已经删除的 GTID 集合，假设是 gtid_purged1；然后先在从库上执行 reset master，再执行 set global gtid_purged =‘gtid_purged1’；最后执行 start slave，就会从主库现存的 binlog 开始同步。binlog 缺失的那一部分，数据在从库上就可能会有丢失，造成主从不一致。 如果需要主从数据一致的话，最好还是通过重新搭建从库来做。 如果有其他的从库保留有全量的 binlog 的话，可以把新的从库先接到这个保留了全量 binlog 的从库，追上日志以后，如果有需要，再接回主库。 如果 binlog 有备份的情况，可以先在从库上应用缺失的 binlog，然后再执行 start slave。 5. GTID 和在线 DDL假设，这两个互为主备关系的库还是实例 X 和实例 Y，且当前主库是 X，并且都打开了 GTID 模式。这时的主备切换流程可以变成下面这样： 在实例 X 上执行 stop slave。在实例 Y 上执行 DDL 语句。 注意，这里并不需要关闭 binlog。 执行完成后，查出这个 DDL 语句对应的 GTID，并记为 server_uuid_of_Y:gno。 到实例 X 上执行以下语句序列：123456set GTID_NEXT=\"server_uuid_of_Y:gno\";begin;commit;set gtid_next=automatic;start slave; 这样做的目的在于，既可以让实例 Y 的更新有 binlog 记录，同时也可以确保不会在实例 X 上执行这条更新。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"16 MySQL 主备延迟","slug":"mysql/MySQL实战45讲/22_mysql主备延迟","date":"2020-03-15T16:00:00.000Z","updated":"2020-05-24T15:56:21.954Z","comments":true,"path":"2020/03/16/mysql/MySQL实战45讲/22_mysql主备延迟/","link":"","permalink":"http://yoursite.com/2020/03/16/mysql/MySQL实战45讲/22_mysql主备延迟/","excerpt":"MySQL 主备延迟与并行复制","text":"MySQL 主备延迟与并行复制 1. 主备延迟与数据同步有关的时间点主要包括以下三个： 主库 A 执行完成一个事务，写入 binlog，我们把这个时刻记为 T1; 之后传给备库 B，我们把备库 B 接收完这个 binlog 的时刻记为 T2; 备库 B 执行完成这个事务，我们把这个时刻记为 T3。 所谓主备延迟，就是同一个事务，在备库执行完成的时间和主库执行完成的时间之间的差值，也就是 T3-T1。在网络正常的时候，日志从主库传给备库所需的时间是很短的，即 T2-T1 的值是非常小的。也就是说，网络正常情况下，主备延迟的主要来源是备库接收完 binlog 和执行完这个事务之间的时间差。所以说，主备延迟最直接的表现是，备库消费中转日志（relay log）的速度，比主库生产 binlog 的速度要慢。 1.1 主备延迟的原因慢的原因有以下几个: 有些部署条件下，备库所在机器的性能要比主库所在的机器性能差。这种部署现在比较少了,因为主备可能发生切换，备库随时可能变成主库，所以主备库选用相同规格的机器，并且做对称部署，是现在比较常见的情况。 备库的压力大,一般的想法是，主库既然提供了写能力，那么备库可以提供一些读能力。或者一些运营后台需要的分析语句，不能影响正常业务，所以只能在备库上跑。结果就是，备库上的查询耗费了大量的 CPU 资源，影响了同步速度，造成主备延迟。 备库的压力大的情况，我们一般可以这么处理： 一主多从。除了备库外，可以多接几个从库，让这些从库来分担读的压力。 通过 binlog 输出到外部系统，比如 Hadoop 这类系统，让外部系统提供统计类查询的能力。 其中，一主多从的方式大都会被采用。因为作为数据库系统，还必须保证有定期全量备份的能力。而从库，就很适合用来做备份。 除了上面这些因素，mysql 的下面这些操作也是造成主备延迟的重要因素: 大事务: 因为主库上必须等事务执行完成才会写入 binlog，再传给备库。所以，如果一个主库上的语句执行 10 分钟，那这个事务很可能就会导致从库延迟 10 分钟。 大表 DDL: 另一种典型的大事务，计划内的 DDL，建议使用 gh-ost 方案 备库的并行复制能力 待会我们会详细讲解备库的并行复制能力是怎么影响主备延迟的。除了上面将的主库导致的主从延迟外，备库也可能导致主备延迟，就是备库起了一个长事务，比如 12begin; select * from t limit 1; # 不动了 这时候主库对表 t 做了一个加字段操作，即使这个表很小，这个 DDL 在备库应用的时候也会被堵住，从而导致主备延迟线性增长。 2. 备库的并行复制在主库上，影响并发度的原因就是各种锁了。由于 InnoDB 引擎支持行锁，除了所有并发事务都在更新同一行（热点行）这种极端场景外，它对业务并发度的支持还是很友好的。 而日志在备库上的执行，就是图中备库上 sql_thread 更新数据 (DATA) 的逻辑。在官方的 5.6 版本之前，MySQL 只支持单线程复制，由此在主库并发高、TPS 高时就会出现严重的主备延迟问题。接下来我们就来看看 mysql 是如何从单线程演化成多线程复制的。 2.1 并行复制的原理 多线程复制机制，就是把只有一个线程的 sql_thread，拆成多个线程: coordinator: 就是原来的 sql_thread, 现在只负责读取中转日志和分发事务 worker 线程: 更新日志的，个数由参数 slave_parallel_workers 配置，对于32 核物理机，建议设置在 8-16 之间，毕竟备库还有可能要提供读查询，不能把 CPU 都吃光了 需要注意的是事务是不能按照轮询的方式分发给各个 worker 的。因为不同的 worker 就独立执行了。但是，由于 CPU 的调度策略，很可能第二个事务最终比第一个事务先执行。而如果这时候刚好这两个事务更新的是同一行，也就意味着，同一行上的两个事务，在主库和备库上的执行顺序相反，会导致主备不一致的问题。同理为了满足事务要求，同一个事务的多个更新语句，也不能分给不同的 worker 来执行 coordinator 在分发的时候，需要满足以下这两个基本要求： 不能造成更新覆盖。这就要求更新同一行的两个事务，必须被分发到同一个 worker 中。 同一个事务不能被拆开，必须放到同一个 worker 中。 2.2 并行复制策略按表分发:按表分发的基本操作是这样的: 每个 worker 线程对应一个 hash 表，用于保存当前正在这个 worker 的“执行队列”里的事务所涉及的表。hash 表的 key 是“库名. 表名”，value 是一个数字，表示队列中有多少个事务修改这个表。 随着每个 worker 的执行，会把已完成事务涉及的表从 hash 表中删除 如果两个事务更新不同的表，它们就可以并行。 如果有跨表的事务，还是要把两张表放在一起考虑的 如果事务 T 跟多于一个 worker 冲突(worker 的hash 表内有与事务 T 操作相同的表)，coordinator 线程就进入等待，直至只与 1 个worker 进程冲突，并发 T 分配给它 这个按表分发的方案，在多个表负载均匀的场景里应用效果很好。如果碰到热点表，就变成了单线程复制。 按行分发按行复制的核心思路是： 如果两个事务没有更新相同的行，它们在备库上可以并行执行。这要求 binlog 格式必须是 row 判断一个事务 T 和 worker 是否冲突，用的就规则就不是“修改同一个表”，而是“修改同一行”。 worker hash 表的 key，就必须是“库名 + 表名 + 唯一键的值” 基于行的策略，事务 hash 表中还需要考虑唯一键(假设唯一索引名为 a)，即 key 应该是“库名 + 表名 + 索引 a 的名字 +a 的值”。因为唯一索引会引发唯一键错误，必须考虑事务执行顺序 因此假设要在表 t1 上执行 update t1 set a=1 where id=2 语句，a 是唯一索引列，原来的值为 2。coordinator 在解析这个语句的 binlog 的时候，这个事务的 hash 表就有三个项: key=hash_func(db1+t1+“PRIMARY”+2), value=2; 这里 value=2 是因为修改前后的行 id 值不变，出现了两次。 key=hash_func(db1+t1+“a”+2), value=1，表示会影响到这个表 a=2 的行。 key=hash_func(db1+t1+“a”+1), value=1，表示会影响到这个表 a=1 的行。 如果是要操作很多行的大事务的话，按行分发的策略有两个问题： 耗费内存 耗费 CPU 因此需要为单个事务设置行数阈值，比如，如果单个事务更新的行数超过 10 万行，就暂时退化为单线程模式，退化过程的逻辑大概是这样的： coordinator 暂时先 hold 住这个事务； 等待所有 worker 都执行完成，变成空队列； coordinator 直接执行这个事务； 恢复并行模式。 分发策略的约束相比于按表并行分发策略，按行并行策略在决定线程分发的时候，需要消耗更多的计算资源。你可能也发现了，这两个方案其实都有一些约束条件： 要能够从 binlog 里面解析出表名、主键值和唯一索引的值。也就是说，主库的 binlog 格式必须是 row； 表必须有主键； 不能有外键。表上如果有外键，级联更新的行不会记录在 binlog 中，这样冲突检测就不准确。 2.3 MySQL 5.6 版本的并行复制策略只是支持的粒度是按库并行: hash 表里，key 就是数据库名，库的数量相对于表和行少的多，因此也不会耗费什么内存和 CPU 并行效果，取决于压力模型。如果在主库上有多个 DB，并且压力均衡，使用这个策略的效果会很好 不要求 binlog 的格式。因为 statement 格式的 binlog 也可以很容易拿到库名。 2.4 MariaDB 的并行复制策略MariaDB 的并行复制 利用了 redo log 组提交 (group commit) 优化: 能够在同一组里提交的事务，一定不会修改同一行； 主库上可以并行执行的事务，备库上也一定是可以并行执行的。 在实现上，MariaDB 是这么做的： 在一组里面一起提交的事务，有一个相同的 commit_id，下一组就是 commit_id+1； commit_id 直接写到 binlog 里面； 传到备库应用的时候，相同 commit_id 的事务分发到多个 worker 执行； 这一组全部执行完成后，coordinator 再去取下一批。 是，这个策略有一个问题，它并没有实现“真正的模拟主库并发度”这个目标。在主库上，一组事务在 commit 的时候，下一组事务是同时处于“执行中”状态的。在备库上执行的时候，要等第一组事务完全执行完成后，第二组事务才能开始执行，这样系统的吞吐量就不够。另外，这个方案很容易被大事务拖后腿。 2.4 MySQL 5.7 的并行复制策略由参数 slave-parallel-type 来控制并行复制策略： 配置为 DATABASE，表示使用 MySQL 5.6 版本的按库并行策略； 配置为 LOGICAL_CLOCK，表示的就是类似 MariaDB 的策略。不过，MySQL 5.7 这个策略，针对并行度做了优化。 这里面的思考逻辑是这样的: 同时处于“执行状态”的所有事务，不是可以并行的，因为这里面包括了由于锁冲突而处于锁等待状态的事务，他们必须严格按照顺序执行 MariaDB 优化策略的核心，是“所有处于 commit”状态的事务可以并行。事务处于 commit 状态，表示已经通过了锁冲突的检验了 根据两阶段提交，不用等到 commit 阶段，只要能够到达 redo log prepare 阶段，就表示事务已经通过锁冲突的检验了 MySQL 5.7 并行复制策略的思想是： 同时处于 prepare 状态的事务，在备库执行时是可以并行的； 处于 prepare 状态的事务，与处于 commit 状态的事务之间，在备库执行时也是可以并行的。 binlog 的组提交的时候，介绍过两个参数： binlog_group_commit_sync_delay 参数，表示延迟多少微秒后才调用 fsync; binlog_group_commit_sync_no_delay_count 参数，表示累积多少次以后才调用 fsync。 这两个参数是用于故意拉长 binlog 从 write 到 fsync 的时间，以此减少 binlog 的写盘次数。在 MySQL 5.7 的并行复制策略里，它们可以用来制造更多的“同时处于 prepare 阶段的事务”。这样就增加了备库复制的并行度。也就是说，这两个参数，既可以“故意”让主库提交得慢些，又可以让备库执行得快些。在 MySQL 5.7 处理备库延迟的时候，可以考虑调整这两个参数值，来达到提升备库复制并发度的目的。 2.5 MySQL 5.7.22 的并行复制策略MySQL 增加了一个新的并行复制策略，基于 WRITESET 的并行复制。相应地，新增了一个参数 binlog-transaction-dependency-tracking，用来控制是否启用这个新策略。它有三个值: COMMIT_ORDER，表示的就是前面介绍的，根据同时进入 prepare 和 commit 来判断是否可以并行的策略。 WRITESET，表示的是对于事务涉及更新的每一行，计算出这一行的 hash 值，组成集合 writeset。如果两个事务没有操作相同的行，也就是说它们的 writeset 没有交集，就可以并行。 WRITESET_SESSION，是在 WRITESET 的基础上多了一个约束，即在主库上同一个线程先后执行的两个事务，在备库执行的时候，要保证相同的先后顺序。 WRITESET 就是我们前面介绍的基于行的并行复制，不过，MySQL 官方的这个实现还是有很大的优势： writeset 是在主库生成后直接写入到 binlog 里面的，这样在备库执行的时候，不需要解析 binlog 内容（event 里的行数据），节省了很多计算量； 不需要把整个事务的 binlog 都扫一遍才能决定分发到哪个 worker，更省内存； 由于备库的分发策略不依赖于 binlog 内容，所以 binlog 是 statement 格式也是可以的。 当然，对于“表上没主键”和“外键约束”的场景，WRITESET 策略也是没法并行的，也会暂时退化为单线程模型。 官方 MySQL5.7 版本新增的备库并行策略，修改了 binlog 的内容，也就是说 binlog 协议并不是向上兼容的，在主备切换、版本升级的时候需要把这个因素也考虑进去。 2.6 大事务的影响大事务不仅会影响到主库，也是造成备库复制延迟的主要原因之一","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"15 MySQL 主从复制","slug":"mysql/MySQL实战45讲/21_主从复制","date":"2020-03-14T16:00:00.000Z","updated":"2020-05-24T15:56:16.942Z","comments":true,"path":"2020/03/15/mysql/MySQL实战45讲/21_主从复制/","link":"","permalink":"http://yoursite.com/2020/03/15/mysql/MySQL实战45讲/21_主从复制/","excerpt":"主从复制，主备延迟","text":"主从复制，主备延迟 1. 主备的基本原理如图 1 所示就是基本的主备切换流程 在状态 1 中，虽然节点 B 没有被直接访问，但是我依然建议你把节点 B（也就是备库）设置成只读（readonly）模式。这样做，有以下几个考虑： 有时候一些运营类的查询语句会被放到备库上去查，设置为只读可以防止误操作； 防止切换逻辑有 bug，比如切换过程中出现双写，造成主备不一致； 可以用 readonly 状态，来判断节点的角色 readonly 设置对超级 (super) 权限用户是无效的，而用于同步更新的线程，拥有超级权限，因此 readonly 不会影响主从同步的写。 2. binlog 同步过程 图中画出的就是一个 update 语句在节点 A 执行，然后同步到节点 B 的完整流程图。可以看到：主库接收到客户端的更新请求后，执行内部事务的更新逻辑，同时写 binlog。 一个事务日志同步的完整过程是这样的： 在备库 B 上通过 change master 命令，设置主库 A 的 IP、端口、用户名、密码，以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量。 在备库 B 上执行 start slave 命令，这时候备库会启动两个线程，就是图中的 io_thread 和 sql_thread。其中 io_thread 负责与主库建立连接。 主库 A 校验完用户名、密码后，开始按照备库 B 传过来的位置，从本地读取 binlog，发给 B 备库 B 拿到 binlog 后，写到本地文件，称为中转日志（relay log） sql_thread 读取中转日志，解析出日志里的命令，并执行。 3. binlog 的格式binlog 有两种格式，一种是 statement，一种是 row。第三种格式 mixed 是前两种格式的混合。 statement: statement 格式下，记录到 binlog 里的是语句原文 因为 statement 记录的原始语句，而不是具体的行，当使用 delete limit 等情况时，就有可能因为主备执行逻辑(比如执行时选择的索引)不同出现主备不一致的情况。 3.1 row 格式下面使用 mysqlbinlog 查看的 row 格式的 binlog 内容: 1mysqlbinlog -vv data/master.000001 --start-position=8900; row 格式的 binlog 包括以下内容: server id 1，表示这个事务是在 server_id=1 的这个库上执行的。 每个 event 都有 CRC32 的值，这是参数 binlog_checksum 设置成了 CRC32，用于检查 binlog 是否完整 binlog 记录的每个 SQL 都会被翻译成多个 event: Table_map event 显示了接下来要打开的表，map 到数字 226。如果要操作多张表呢？每个表都有一个对应的 Table_map event、都会 map 到一个单独的数字，用于区分对不同表的操作。 Delete_rows event，用于定义删除的行为 mysqlbinlog 的命令中，使用了 -vv 参数是为了把内容都解析出来，所以从结果里面可以看到各个字段的值（比如，@1=4、 @2=4 这些值） binlog_row_image 的默认配置是 FULL，因此 Delete_event 里面，包含了删掉的行的所有字段的值。如果把 binlog_row_image 设置为 MINIMAL，则只会记录必要的信息，在这个例子里，就是只会记录 id=4 这个信息。 最后的 Xid event，用于表示事务被正确地提交了。 当 binlog_format 使用 row 格式的时候，binlog 里面记录了真实删除行的主键 id，这样 binlog 传到备库去的时候，就肯定会删除 id=4 的行，不会有主备删除不同行的问题。 3.2 mixed 格式为什么会有 mixed 这种 binlog 格式的存在场景？ 因为有些 statement 格式的 binlog 可能会导致主备不一致，所以要使用 row 格式。 但 row 格式的缺点是，很占空间。比如你用一个 delete 语句删掉 10 万行数据，用 statement 的话就是一个 SQL 语句被记录到 binlog 中，占用几十个字节的空间。但如果用 row 格式的 binlog，就要把这 10 万条记录都写到 binlog 中。这样做，不仅会占用更大的空间，同时写 binlog 也要耗费 IO 资源，影响执行速度。 所以，MySQL 就取了个折中方案，也就是有了 mixed 格式的 binlog。mixed 格式的意思是，MySQL 自己会判断这条 SQL 语句是否可能引起主备不一致，如果有可能，就用 row 格式，否则就用 statement 格式。 也就是说，mixed 格式可以利用 statment 格式的优点，同时又避免了数据不一致的风险。 但是 MySQL 的 binlog 格式最好还是设置成 row。因为需要 binlog 做数据恢复。 delete 语句，row 格式的 binlog 也会把被删掉的行的整行信息保存起来 insert 语句的 binlog 里会记录所有的字段信息，这些信息可以用来精确定位刚刚被插入的那一行 update 语句，binlog 里面会记录修改前整行的数据和修改后的整行数据。 MariaDB 的Flashback工具正是基于 row 格式的 binlog 做数据恢复的工具。 4. 循环复制问题实际生产上使用比较多的是双 M 结构，也就是下图所示的主备切换流程。 双主模型中存在循环复制的问题: 业务逻辑在节点 A 上更新了一条语句，然后再把生成的 binlog 发给节点 B，节点 B 执行完这条更新语句后也会生成 binlog。（建议把参数 log_slave_updates 设置为 on，表示备库执行 relay log 后生成 binlog，这样任意主库上的 binlog 都可以用作数据恢复） MySQL 这样解决循环复制的问题的: 两个库的 server id 必须不同，如果相同，则它们之间不能设定为主备关系； 从节点 A 更新的事务，binlog 里面记的都是 A 的 server id； 传到节点 B 执行一次以后，节点 B 生成的 binlog 的 server id 也是 A 的 server id； 再传回给节点 A，A 判断到这个 server id 与自己的相同，就不会再处理这个日志。所以，死循环在这里就断掉了 但是即便如此存在下面两种情况一样会导致循环复制问题。 4.1 场景一在一个主库更新事务后，用命令 set global server_id=x 修改了 server_id。等日志再传回来的时候，发现 server_id 跟自己的 server_id 不同，就只能执行了。 4.2 场景二有三个节点的时候，如图 7 所示，trx1 是在节点 B 执行的，因此 binlog 上的 server_id 就是 B，binlog 传给节点 A，然后 A 和 A’搭建了双 M 结构，就会出现循环复制。 这种三节点复制的场景，做数据库迁移的时候会出现。可以在 A 或者 A’上，执行如下命令： 123stop slave；CHANGE MASTER TO IGNORE_SERVER_IDS=(server_id_of_B);start slave; 这样这个节点收到日志后就不会再执行。过一段时间后，再执行下面的命令把这个值改回来。 123stop slave；CHANGE MASTER TO IGNORE_SERVER_IDS=();start slave;","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"14 MYSQL 全表扫描","slug":"mysql/MySQL实战45讲/16_全表扫描","date":"2020-03-13T16:00:00.000Z","updated":"2020-05-24T15:56:12.912Z","comments":true,"path":"2020/03/14/mysql/MySQL实战45讲/16_全表扫描/","link":"","permalink":"http://yoursite.com/2020/03/14/mysql/MySQL实战45讲/16_全表扫描/","excerpt":"全表扫描对 MySQL 有什么影响","text":"全表扫描对 MySQL 有什么影响 1. 对 server 层的影响1.1 发送数据的流程取数据和发数据的流程是这样的： 获取一行，写到 net_buffer 中。这块内存的大小是由参数 net_buffer_length 定义的，默认是 16k。 重复获取行，直到 net_buffer 写满，调用网络接口发出去。 如果发送成功，就清空 net_buffer，然后继续取下一行，并写入 net_buffer。 如果发送成功，就清空 net_buffer，然后继续取下一行，并写入 net_buffer。 可以看到： 一个查询在发送过程中，占用的 MySQL 内部的内存最大就是 net_buffer_length 这么大，并不会达到 200G； socket send buffer 也不可能达到 200G（默认定义 /proc/sys/net/core/wmem_default），如果 socket send buffer 被写满，就会暂停读数据的流程。 MySQL 是“边读边发的”，这就意味着，如果客户端接收得慢，会导致 MySQL 服务端由于结果发不出去，这个事务的执行时间变长。 1.2 查询语句的状态变化一个查询语句的状态变化是这样的: MySQL 查询语句进入执行阶段后，首先把状态设置成“Sending data”； 然后，发送执行结果的列相关的信息（meta data) 给客户端； 再继续执行语句的流程； 执行完成后，把状态设置成空字符串。 也就是说: “Sending data”并不一定是指“正在发送数据”，而可能是处于执行器过程中的任意阶段。比如，锁等待的场景 仅当一个线程处于“等待客户端接收结果”的状态，才会显示”Sending to client” 1.3 客户端接收数据慢对事务的影响如果客户端使用–quick 参数，会使用 mysql_use_result 方法。这个方法是读一行处理一行。假设有一个业务的逻辑比较复杂，每读一行数据以后要处理的逻辑如果很慢，就会导致客户端要过很久才会去取下一行数据，可能就会出现如图 2 所示的 “Sending to client” 这种情况。 对于正常的线上业务来说，如果一个查询的返回结果不会很多的话，我都建议你使用 mysql_store_result 这个接口，直接把查询结果保存到本地内存。当然前提是查询返回结果不多。 另一方面，如果你在自己负责维护的 MySQL 里看到很多个线程都处于“Sending to client”这个状态，就意味着你要让业务开发同学优化查询结果，并评估这么多的返回结果是否合理。 而如果要快速减少处于这个状态的线程的话，将 net_buffer_length 参数设置为一个更大的值是一个可选方案。 2. 全表扫描对 InnoDB 的影响Innodb 内存的数据页是在 Buffer Pool (BP) 中管理的，在 WAL 里 Buffer Pool 起到了两个作用: 加速更新 加速查询 而 Buffer Pool 对查询的加速效果，依赖于一个重要的指标，即：内存命中率。一般情况下，一个稳定服务的线上系统，要保证响应时间符合要求的话，内存命中率要在 99% 以上。 show engine innodb status 的 Buffer pool hit rate显示的就是当前的命中率。 2.1 innodb_buffer_pool_sizeInnoDB Buffer Pool 的大小是由参数 innodb_buffer_pool_size 确定的，一般建议设置成可用物理内存的 60%~80%。 InnoDB 内存管理用的是最近最少使用 (Least Recently Used, LRU) 算法，这个算法的核心就是淘汰最久未使用的数据。为了避免一次全表扫描导致所有的缓存失效，InnoDB 对 LRU 算法做了改进。 在 InnoDB 实现上，按照 5:3 的比例把整个 LRU 链表分成了 young 区域和 old 区域。图中 LRU_old 指向的就是 old 区域的第一个位置，是整个链表的 5/8 处。也就是说，靠近链表头部的 5/8 是 young 区域，靠近链表尾部的 3/8 是 old 区域。LRU 算法执行流程如下: 图 7 中状态 1，要访问数据页 P3，由于 P3 在 young 区域，将其移到链表头部，变成状态 2 之后要访问一个新的不存在于当前链表的数据页，这时候依然是淘汰掉数据页 Pm，但是新插入的数据页 Px，是放在 LRU_old 处 处于 old 区域的数据页，每次被访问的时候都要做下面这个判断： 若这个数据页在 LRU 链表中存在的时间超过了 1 秒，就把它移动到链表头部； 如果这个数据页在 LRU 链表中存在的时间短于 1 秒，位置保持不变。 1 秒这个时间，是由参数 innodb_old_blocks_time 控制的。其默认值是 1000，单位毫秒 因此进入yong区域的缓存需要满足两个条件: 已经存在 old 缓存内 被第二次访问 两次访问之间的间隔超过 1s 这个策略，就是为了处理类似全表扫描的操作量身定制的。扫描大表的过程中，虽然也用到了 Buffer Pool，但是对 young 区域完全没有影响，从而保证了 Buffer Pool 响应正常业务的查询命中率。 全表扫描还是比较耗费 IO 资源的，所以业务高峰期还是不能直接在线上主库执行全表扫描的。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"13 MYSQL 索引选择","slug":"mysql/MySQL实战45讲/15_优化器","date":"2020-03-12T16:00:00.000Z","updated":"2020-05-24T15:56:08.736Z","comments":true,"path":"2020/03/13/mysql/MySQL实战45讲/15_优化器/","link":"","permalink":"http://yoursite.com/2020/03/13/mysql/MySQL实战45讲/15_优化器/","excerpt":"MySQL 如何选择索引","text":"MySQL 如何选择索引 1. 优化器逻辑选择索引是优化器的工作。而优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句。优化器会考虑扫描行数、是否使用临时表、是否排序等因素进行综合判断。 2. 扫描行数MySQL 在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数。这个统计信息就是索引的“区分度”。 一个索引上不同的值越多，这个索引的区分度就越好。 一个索引上不同的值的个数，我们称之为“基数”（cardinality）。这个基数越大，索引的区分度越好。 2.1 索引区分度show index from t 可以查看索引的基数。 123456789101112131415MariaDB [tsong]&gt; show index from course \\G*************************** 1. row *************************** Table: course Non_unique: 0 Key_name: PRIMARY Seq_in_index: 1 Column_name: id Collation: A Cardinality: 3 # 基数 Sub_part: NULL Packed: NULL Null: Index_type: BTREE Comment:Index_comment: 那 mysql 是如何计算索引的基数呢？ mysql 用的采样统计的方法: InnoDB 默认会选择 N 个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数 当变更的数据行数超过 1/M 的时候，会自动触发重新做一次索引统计 MySQL 中，有两种存储索引统计的方式，通过设置参数 innodb_stats_persistent 的值来选择： =ON: 表示统计信息会持久化存储。这时，默认的 N 是 20，M 是 10 =OFF: 表示统计信息只存储在内存中。这时，默认的 N 是 8，M 是 16 采样统计，这个基数都是很容易不准的，如果索引统计不准确，可以使用 analyze table t 重新统计索引信息 索引统计只是一个输入，对于一个具体的语句来说，优化器还要判断，执行这个语句本身要扫描多少行。同时使用普通索引需要把回表的代价算进去。 2.2 索引选择异常和处理索引选择异常有以下几种处理方法: 采用 force index 强行选择一个索引，使用 force index 最主要的问题是变更的及时性 修改语句，引导 MySQL 使用我们期望的索引 我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"12 MySQL 性能优化","slug":"mysql/MySQL实战45讲/14_性能优化","date":"2020-03-11T16:00:00.000Z","updated":"2020-05-24T15:56:04.409Z","comments":true,"path":"2020/03/12/mysql/MySQL实战45讲/14_性能优化/","link":"","permalink":"http://yoursite.com/2020/03/12/mysql/MySQL实战45讲/14_性能优化/","excerpt":"MySQL 中怎么做问题追踪与性能优化","text":"MySQL 中怎么做问题追踪与性能优化 1. 问题排查的工具mysql 中有以下几种问题排查的工具: show processlist show engine innodb status information_schema.innodb_trx optimizer_trace 慢查询日志 performance_schema 和 sys 系统库 1. show processlistshow processlist 作用: 查看当前连接的线程的状态 1.1 等待 MDL 锁 上面是一个等待 MDL 写锁的示例。这类问题的处理方式，就是找到谁持有 MDL 写锁，然后把它 kill 掉。但是，由于在 show processlist 的结果里面，持有 MDL 写锁线程的 Command 列可能是“Sleep”，导致查找起来很不方便。 通过查询 sys.schema_table_lock_waits 这张表，我们就可以直接找出造成阻塞的 process id，把这个连接用 kill 命令断开即可。 不过启用 performance_schema 和 sys(mariadb 中为 information_schema) 系统库需要 MySQL 在启动时设置 performance_schema=on，相比于设置为 off 会有 10% 左右的性能损失。 1.2 等 flushflush tables: flush tables t with read lock: 只关闭表 t flush tables with read lock: 关闭 MySQL 里所有打开的表。 正常这两个语句执行起来都很快，除非它们也被别的线程堵住了 出现上面 Waiting for table flush 状态的可能情况是：有一个 flush tables 命令被别的语句堵住了，然后它又堵住了我们的 select 语句。 1.3 innodb 行锁 state 为 statistics 的线程被阻塞。这个问题并不难分析，但问题是怎么查出是谁占着这个写锁。如果你用的是 MySQL 5.7 版本，可以通过 sys.innodb_lock_waits 表查到。 可以看到，4 号线程是造成堵塞的罪魁祸首。而干掉这个罪魁祸首的方式，就是 KILL QUERY 4 或 KILL 4。 不过，这里不应该显示“KILL QUERY 4”。这个命令表示停止 4 号线程当前正在执行的语句，而这个方法其实是没有用的。因为占有行锁的是 update 语句，这个语句已经是之前执行完成了的，现在执行 KILL QUERY，无法让这个事务去掉 id=1 上的行锁。 实际上，KILL 4 才有效，也就是说直接断开这个连接。这里隐含的一个逻辑就是，连接被断开的时候，会自动回滚这个连接里面正在执行的线程，也就释放了 id=1 上的行锁。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"11 SQL 一些常见的错误用法","slug":"mysql/MySQL实战45讲/13_SQL错误用法","date":"2020-03-10T16:00:00.000Z","updated":"2020-05-24T15:56:01.111Z","comments":true,"path":"2020/03/11/mysql/MySQL实战45讲/13_SQL错误用法/","link":"","permalink":"http://yoursite.com/2020/03/11/mysql/MySQL实战45讲/13_SQL错误用法/","excerpt":"对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。","text":"对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。 试验环境我们用下面两张表作为我们测试 SQL 用法的试验环境: 1234567891011121314151617181920# 交易信息表mysql&gt; CREATE TABLE `tradelog` ( `id` int(11) NOT NULL, `tradeid` varchar(32) DEFAULT NULL, `operator` int(11) DEFAULT NULL, `t_modified` datetime DEFAULT NULL, PRIMARY KEY (`id`), KEY `tradeid` (`tradeid`), KEY `t_modified` (`t_modified`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;# 交易详情表mysql&gt; CREATE TABLE `trade_detail` ( `id` int(11) NOT NULL, `tradeid` varchar(32) DEFAULT NULL, `trade_step` int(11) DEFAULT NULL, /*操作步骤*/ `step_info` varchar(32) DEFAULT NULL, /*步骤信息*/ PRIMARY KEY (`id`), KEY `tradeid` (`tradeid`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 1.条件字段函数操作类似下面的 SQL，对索引字段做了函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。 12# t_modified 上存在索引mysql&gt; select count(*) from tradelog where month(t_modified)=7; 要注意的是，优化器并不是要放弃使用这个索引。在这个例子里，放弃了树搜索功能，优化器可以选择遍历主键索引，也可以选择遍历索引 t_modified，优化器对比索引大小后发现，索引 t_modified 更小，遍历这个索引比遍历主键索引来得更快。因此最终还是会选择索引 t_modified。 我们就要把 SQL 语句改成基于字段本身的范围查询。按照下面这个写法，优化器就能按照我们预期的，用上 t_modified 索引的快速定位能力了。 1234mysql&gt; select count(*) from tradelog where -&gt; (t_modified &gt;= '2016-7-1' and t_modified&lt;'2016-8-1') or -&gt; (t_modified &gt;= '2017-7-1' and t_modified&lt;'2017-8-1') or -&gt; (t_modified &gt;= '2018-7-1' and t_modified&lt;'2018-8-1'); 不过优化器在个问题上确实有“偷懒”行为，即使是对于不改变有序性的函数，也不会考虑使用索引。比如，对于 select * from tradelog where id + 1 = 10000 这个 SQL 语句，这个加 1 操作并不会改变有序性，但是 MySQL 优化器还是不能用 id 索引快速定位到 9999 这一行。所以，需要你在写 SQL 语句的时候，手动改写成 where id = 10000 -1 才可以。 2. 隐式类型转换现在这里就有两个问题： 数据类型转换的规则是什么？ 为什么有数据类型转换，就需要走全索引扫描？ 2.1 类型转换规则类型装换的规则有一个简单地额判断法方法，看 select “10” &gt; 9 的结果： 如果规则是“将字符串转成数字”，那么就是做数字比较，结果应该是 1； 如果规则是“将数字转成字符串”，那么就是做字符串比较，结果应该是 0。 2.2 有类型转换，需要走全表扫描试验一下便知道在 MySQL 中，字符串和数字做比较的话，是将字符串转换成数字。 12345# 示例表同上mysql&gt; select * from tradelog where tradeid=110717;# 上面的 SQL 等同于mysql&gt; select * from tradelog where CAST(tradid AS signed int) = 110717; 也就是说，上面这条语句触发了我们上面说到的规则：对索引字段做函数操作，优化器会放弃走树搜索功能。 3. 隐式字符编码转换如果要查询 id=2 的交易的所有操作步骤信息，SQL 语句可以这么写：1mysql&gt; select d.* from tradelog l, trade_detail d where d.tradeid=l.tradeid and l.id=2; /*语句Q1*/ 使用 explain 观察这个 SQL 的执行你就会发现并没有使用 trade_detail tradeid 上的索引，而是作的全表扫描。而原因就是这两个表的字符集不同，一个是 utf8，一个是 utf8mb4。 单独看步骤二，相当于执行 SQL select * from trade_detail where tradeid=$L2.tradeid.value;其中，$L2.tradeid.value 的字符集是 utf8mb4。字符集 utf8mb4 是 utf8 的超集，所以当这两个类型的字符串在做比较的时候，MySQL 内部的操作是，先把 utf8 字符串转成 utf8mb4 字符集，再做比较。 因此， 在执行上面这个语句的时候，需要将被驱动数据表里的字段一个个地转换成 utf8mb4，再跟 L2 做比较。也就是说，实际上这个语句等同于下面这个写法： 1select * from trade_detail where CONVERT(traideid USING utf8mb4)=$L2.tradeid.value; 这就再次触发了我们上面说到的原则：对索引字段做函数操作，优化器会放弃走树搜索功能。到这里，你终于明确了，字符集不同只是条件之一，连接过程中要求在被驱动表的索引字段上加函数操作，是直接导致对被驱动表做全表扫描的原因。 如果要对上面的额语句作优化，有两种常见的做法: 比较常见的优化方法是，把 trade_detail 表上的 tradeid 字段的字符集也改成 utf8mb4，这样就没有字符集转换的问题了。 如果能够修改字段的字符集的话，是最好不过了。但如果数据量比较大， 或者业务上暂时不能做这个 DDL 的话，那就只能采用修改 SQL 语句的方法了。 12# 主动把 l.tradeid 转成 utf8，就避免了被驱动表上的字符编码转换mysql&gt; select d.* from tradelog l , trade_detail d where d.tradeid=CONVERT(l.tradeid USING utf8) and l.id=2; 4. 字符串截断12345678910mysql&gt; CREATE TABLE `table_a` ( `id` int(11) NOT NULL, `b` varchar(10) DEFAULT NULL, PRIMARY KEY (`id`), KEY `b` (`b`)) ENGINE=InnoDB;# 假设现在表里面，有 100 万行数据，其中有 10 万行数据的 b 的值是’1234567890’mysql&gt; select * from table_a where b='1234567890abcd'; mysql 既不会判断字段 b 定义的是 varchar(10)，小于 “1234567890abcd” 长度直接返回空，也不是直接把’1234567890abcd’拿到索引里面去做匹配。而是: 在传给引擎执行的时候，做了字符截断。因为引擎里面这个行只定义了长度是 10，所以只截了前 10 个字节，就是’1234567890’进去做匹配； 因为是 select *， 所以要做 10 万次回表； 但是每次回表以后查出整行，到 server 层一判断，b 的值都不是’1234567890abcd’; 返回结果是空。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"10 MySQL如何显示随机消息","slug":"mysql/MySQL实战45讲/12_随机","date":"2020-03-09T16:00:00.000Z","updated":"2020-05-24T15:55:57.311Z","comments":true,"path":"2020/03/10/mysql/MySQL实战45讲/12_随机/","link":"","permalink":"http://yoursite.com/2020/03/10/mysql/MySQL实战45讲/12_随机/","excerpt":"random 存在哪些问题","text":"random 存在哪些问题 背景从一个单词表中随机选出三个单词。这个表的建表语句和初始数据的命令如下：1234567891011121314151617181920mysql&gt; CREATE TABLE `words` ( `id` int(11) NOT NULL AUTO_INCREMENT, `word` varchar(64) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB;delimiter ;;create procedure idata()begin declare i int; set i=0; while i&lt;10000 do insert into words(word) values(concat(char(97+(i div 1000)), char(97+(i % 1000 div 100)), char(97+(i % 100 div 10)), char(97+(i % 10)))); set i=i+1; end while;end;;delimiter ;call idata(); 1. 方法一: order by rand()1.1 内存临时表select word from words order by rand() limit 3; 这个语句的意思很直白，但执行流程却有点复杂的。 排序算法的选择: 对于 InnoDB 表来说，执行全字段排序会减少磁盘访问，因此会被优先选择。 对于内存表，回表过程只是简单地根据数据行的位置，直接访问内存得到数据，根本不会导致多访问磁盘。那么它会优先考虑的，就是用于排序的行越小越好了，所以，MySQL 这时就会选择 rowid 排序。 因此上面这个 SQL 的执行过程是这样的: 创建一个临时表。这个临时表使用的是 memory 引擎，表里有两个字段，第一个字段是 double 类型，为了后面描述方便，记为字段 R，第二个字段是 varchar(64) 类型，记为字段 W。并且，这个表没有建索引。 从 words 表中，按主键顺序取出所有的 word 值。对于每一个 word 值，调用 rand() 函数生成一个大于 0 小于 1 的随机小数，并把这个随机小数和 word 分别存入临时表的 R 和 W 字段中，到此，扫描行数是 10000。 现在临时表有 10000 行数据了，接下来你要在这个没有索引的内存临时表上，按照字段 R 排序 初始化 sort_buffer。sort_buffer 中有两个字段，一个是 double 类型，另一个是整型 从内存临时表中一行一行地取出 R 值和位置信息 分别存入 sort_buffer 中的两个字段里。这个过程要对内存临时表做全表扫描，此时扫描行数增加 10000，变成了 20000。 在 sort_buffer 中根据 R 的值进行排序。注意，这个过程没有涉及到表操作，所以不会增加扫描行数 排序完成后，取出前三个结果的位置信息，依次到内存临时表中取出 word 值，返回给客户端。这个过程中，访问了表的三行数据，总扫描行数变成了 20003。 图中的 POS 位置信息表示的是：每个引擎用来唯一标识数据行的信息。 对于有主键的 InnoDB 表来说，这个 rowid 就是主键 ID； 对于没有主键的 InnoDB 表来说，这个 rowid 就是由系统生成的； MEMORY 引擎不是索引组织表。在这个例子里面，你可以认为它就是一个数组。因此，这个 rowid 其实就是数组的下标。 order by rand() 使用了内存临时表，内存临时表排序的时候使用了 rowid 排序方法 1.2 磁盘临时表tmp_table_size 这个配置限制了内存临时表的大小，默认值是 16M。如果临时表大小超过了 tmp_table_size，那么内存临时表就会转成磁盘临时表。 磁盘临时表使用的引擎默认是 InnoDB，是由参数 internal_tmp_disk_storage_engine 控制的。当使用磁盘临时表的时候，对应的就是一个没有显式索引的 InnoDB 表的排序过程。 1234567891011set tmp_table_size=1024;set sort_buffer_size=32768;set max_length_for_sort_data=16;/* 打开 optimizer_trace，只对本线程有效 */SET optimizer_trace='enabled=on'; /* 执行语句 */select word from words order by rand() limit 3;/* 查看 OPTIMIZER_TRACE 输出 */SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\\G max_length_for_sort_data 设置成 16，小于 word 字段的长度定义，所以我们看到 sort_mode 里面显示的是 rowid 排序 SQL 语句，只需要取 R 值最小的 3 个 rowid,MySQL 使用了优先队列排序算法，而不是归并排序，所以filesort_priority_queue_optimization 这个部分的 chosen=true，就表示使用了优先队列排序算法，这个过程不需要临时文件，因此对应的 number_of_tmp_files 是 0。 什么时候选择优先队列排序算法？如果 limit n * 待排序行的大小(上面的大小就是字段R, rowid) 小于 sort_buffer_size 就会使用优先队列排序算法。 1.3 总结不论是使用哪种类型的临时表，order by rand() 这种写法都会让计算过程非常复杂，需要大量的扫描行数，因此排序过程的资源消耗也会很大。 2. 随机排序方法为了得到严格随机的结果，你可以用下面这个流程: 取得整个表的行数，记为 C； 使用 Y = floor(C * rand())，得到 Y1、Y2、Y3；floor 函数在这里的作用，就是取整数部分。 再执行三个 limit Y, 1 语句得到三行数据。 1234567mysql&gt; select count(*) into @C from t;set @Y1 = floor(@C * rand());set @Y2 = floor(@C * rand());set @Y3 = floor(@C * rand());select * from t limit @Y1，1； //在应用代码里面取Y1、Y2、Y3值，拼出SQL后执行select * from t limit @Y2，1；select * from t limit @Y3，1； 进一步优化的方法是取 Y1、Y2 和 Y3 里面最大的一个数，记为 M，最小的一个数记为 N，然后执行下面这条 SQL 语句：select * from t limit N, M-N+1; 如果返回的数据太多，也可以先取回 id 值，在应用中确定了三个 id 值以后，再执行三次 where id=X 的语句。 2.1 为什么使用 floor 向下取整123mysql&gt; select max(id),min(id) into @M,@N from t ;set @X= floor((@M-@N+1)*rand() + @N);select * from t where id &gt;= @X limit 1; 这个算法本身并不严格满足题目的随机要求，因为 ID 中间可能有空洞，因此选择不同行的概率不一样，不是真正的随机。比如你有 4 个 id，分别是 1、2、4、5，如果按照上面的方法，那么取到 id=4 的这一行的概率是取得其他行概率的两倍。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"9 MYSQL 自增值的上线","slug":"mysql/MySQL实战45讲/09_自增值上线","date":"2020-03-08T16:00:00.000Z","updated":"2020-05-24T15:50:24.229Z","comments":true,"path":"2020/03/09/mysql/MySQL实战45讲/09_自增值上线/","link":"","permalink":"http://yoursite.com/2020/03/09/mysql/MySQL实战45讲/09_自增值上线/","excerpt":"MySQL 里面的几种自增 id，它们的值达到上限以后，会出现什么情况。","text":"MySQL 里面的几种自增 id，它们的值达到上限以后，会出现什么情况。 1. 表定义自增值 id表定义的自增值达到上限后的逻辑是：再申请下一个 id 时，得到的值保持不变。因此就会出现主键冲突错误。 232-1（4294967295）不是一个特别大的数，因此在建表的时候你需要考察你的表是否有可能达到这个上限，如果有可能，就应该创建成 8 个字节的 bigint unsigned。 2. InnoDB 系统自增 row_id如果你创建的 InnoDB 表没有指定主键，那么 InnoDB 会给你创建一个不可见的，长度为 6 个字节的 row_id。 InnoDB 维护了一个全局的 dict_sys.row_id 值，所有无主键的 InnoDB 表，每插入一行数据，都将当前的 dict_sys.row_id 值作为要插入数据的 row_id，然后把 dict_sys.row_id 的值加 1。 在代码实现时 row_id 是一个长度为 8 字节的无符号长整型 (bigint unsigned)。但是，InnoDB 在设计时，给 row_id 留的只是 6 个字节的长度，这样写到数据表中时只放了最后 6 个字节，所以 row_id 能写到数据表中的值，就有两个特征： row_id 写入表中的值范围，是从 0 到 248-1； 当 dict_sys.row_id=248时，如果再有插入数据的行为要来申请 row_id，拿到以后再取最后 6 个字节的话就是 0。 也就是说，写入表的 row_id 是从 0 开始到 248-1。达到上限后，下一个值就是 0，然后继续循环。 在 InnoDB 逻辑里，申请到 row_id=N 后，就将这行数据写入表中；如果表中已经存在 row_id=N 的行，新写入的行就会覆盖原有的行。 从这个角度看，我们还是应该在 InnoDB 表中主动创建自增主键。因为，表自增 id 到达上限后，再插入数据时报主键冲突错误，是更能被接受的。 毕竟覆盖数据，就意味着数据丢失，影响的是数据可靠性；报主键冲突，是插入失败，影响的是可用性。而一般情况下，可靠性优先于可用性。 3. Xidredo log 和 binlog 相配合的时候，提到了它们有一个共同的字段叫作 Xid。 MySQL 内部维护了一个全局变量 global_query_id，每次执行语句的时候将它赋值给 Query_id，然后给这个变量加 1。如果当前语句是这个事务执行的第一条语句，那么 MySQL 还会同时把 Query_id 赋值给这个事务的 Xid。 而 global_query_id 是一个纯内存变量，重启之后就清零了。所以你就知道了，在同一个数据库实例中，不同事务的 Xid 也是有可能相同的。 但是 MySQL 重启之后会重新生成新的 binlog 文件，这就保证了，同一个 binlog 文件里，Xid 一定是惟一的。 global_query_id 达到上限后，就会继续从 0 开始计数。从理论上讲，还是就会出现同一个 binlog 里面出现相同 Xid 的场景。因为 global_query_id 定义的长度是 8 个字节，这个自增值的上限是 2^64-1。要出现这种情况，必须是下面这样的过程： 执行一个事务，假设 Xid 是 A； 接下来执行 264次查询语句，让 global_query_id 回到 A； 再启动一个事务，这个事务的 Xid 也是 A。 不过，264这个值太大了，大到你可以认为这个可能性只会存在于理论上。 4. Innodb trx_idXid 和 InnoDB 的 trx_id 是两个容易混淆的概念。 Xid 是由 server 层维护的。InnoDB 内部使用 Xid，就是为了能够在 InnoDB 事务和 server 之间做关联。 InnoDB 自己的 trx_id，是另外维护的。这个 trx_id 就是事务id(transaction id) InnoDB 内部维护了一个 max_trx_id 全局变量，每次需要申请一个新的 trx_id 时，就获得 max_trx_id 的当前值，然后并将 max_trx_id 加 1。 InnoDB 数据可见性的核心思想是：每一行数据都记录了更新它的 trx_id，当一个事务读到一行数据的时候，判断这个数据是否可见的方法，就是通过事务的一致性视图与这行数据的 trx_id 做对比。 对于正在执行的事务，你可以从 information_schema.innodb_trx 表中看到事务的 trx_id。 4.1 trx_id 的分配策略 session B 里，我从 innodb_trx 表里查出的这两个字段，第二个字段 trx_mysql_thread_id 就是线程 id。显示线程 id，是为了说明这两次查询看到的事务对应的线程 id 都是 5，也就是 session A 所在的线程。 T2 时刻显示的 trx_id 是一个很大的数；T4 时刻显示的 trx_id 是 1289，看上去是一个比较正常的数字。 这是因为在 T1 时刻，session A 还没有涉及到更新，是一个只读事务。而对于只读事务，InnoDB 并不会分配 trx_id。也就是说： 在 T1 时刻，trx_id 的值其实就是 0。而这个很大的数，只是显示用的。 直到 session A 在 T3 时刻执行 insert 语句的时候，InnoDB 才真正分配了 trx_id。所以，T4 时刻，session B 查到的这个 trx_id 的值就是 1289 需要注意的是，除了显而易见的修改类语句外，如果在 select 语句后面加上 for update，这个事务也不是只读事务。 trx_id 的增加: update 和 delete 语句除了事务本身，还涉及到标记删除旧数据，也就是要把数据放到 purge 队列里等待后续物理删除，这个操作也会把 max_trx_id+1， 因此在一个事务中至少加 2； InnoDB 的后台操作，比如表的索引信息统计这类操作，也是会启动内部事务的，因此你可能看到，trx_id 值并不是按照加 1 递增的。 4.3 T2 时刻查到的这个很大的数字是怎么来的呢？这个数字是每次查询的时候由系统临时计算出来的。它的算法是：把当前事务的 trx 变量的指针地址转成整数，再加上 248。使用这个算法，就可以保证以下两点： 因为同一个只读事务在执行期间，它的指针地址是不会变的，所以不论是在 innodb_trx 还是在 innodb_locks 表里，同一个只读事务查出来的 trx_id 就会是一样的。 如果有并行的多个只读事务，每个事务的 trx 变量的指针地址肯定不同。这样，不同的并发只读事务，查出来的 trx_id 就是不同的。 为什么还要再加上 248呢？在显示值里面加上 248，目的是要保证只读事务显示的 trx_id 值比较大，正常情况下就会区别于读写事务的 id。但是，trx_id 跟 row_id 的逻辑类似，定义长度也是 8 个字节。因此，在理论上还是可能出现一个读写事务与一个只读事务显示的 trx_id 相同的情况。不过这个概率很低，并且也没有什么实质危害，可以不管它。 只读事务不分配 trx_id，有什么好处 一个好处是，这样做可以减小事务视图里面活跃事务数组的大小。因为当前正在运行的只读事务，是不影响数据的可见性判断的。所以，在创建事务的一致性视图时，InnoDB 就只需要拷贝读写事务的 trx_id。 另一个好处是，可以减少 trx_id 的申请次数。在 InnoDB 里，即使你只是执行一个普通的 select 语句，在执行过程中，也是要对应一个只读事务的。所以只读事务优化后，普通的查询语句不需要申请 trx_id，就大大减少了并发事务申请 trx_id 的锁冲突。 4.3 trx_id 到达上线会怎么样max_trx_id 会持久化存储，重启也不会重置为 0，那么从理论上讲，只要一个 MySQL 服务跑得足够久，就可能出现 max_trx_id 达到 248-1 的上限，然后从 0 开始的情况。 当达到这个状态后，MySQL 就会持续出现一个脏读的 bug，我们来复现一下这个 bug。我们来复现这个场景 由于我们已经把系统的 max_trx_id 设置成了 2^48-1，所以在 session A 启动的事务 TA 的低水位就是 248-1 在 T2 时刻，session B 执行第一条 update 语句的事务 id 就是 2^48-1，而第二条 update 语句的事务 id 就是 0 了，这条 update 语句执行后生成的数据版本上的 trx_id 就是 0。 在 T3 时刻，session A 执行 select 语句的时候，判断可见性发现，c=3 这个数据版本的 trx_id，小于事务 TA 的低水位，因此认为这个数据可见。 由于低水位值会持续增加，而事务 id 从 0 开始计数，就导致了系统在这个时刻之后，所有的查询都会出现脏读的。 并且，MySQL 重启时 max_trx_id 也不会清 0，也就是说重启 MySQL，这个 bug 仍然存在。 5. thread_idthread_id 的逻辑很好理解：系统保存了一个全局变量 thread_id_counter，每新建一个连接，就将 thread_id_counter 赋值给这个新连接的线程变量。 thread_id_counter 定义的大小是 4 个字节，因此达到 232-1 后，它就会重置为 0，然后继续增加。但是，你不会在 show processlist 里看到两个相同的 thread_id。 这，是因为 MySQL 设计了一个唯一数组的逻辑，给新线程分配 thread_id 的时候，逻辑代码是这样的：1234do &#123; new_id= thread_id_counter++;&#125; while (!thread_ids.insert_unique(new_id).second);","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"8 insert 语句的锁","slug":"mysql/MySQL实战45讲/08_insert锁","date":"2020-03-07T16:00:00.000Z","updated":"2020-05-24T15:50:18.794Z","comments":true,"path":"2020/03/08/mysql/MySQL实战45讲/08_insert锁/","link":"","permalink":"http://yoursite.com/2020/03/08/mysql/MySQL实战45讲/08_insert锁/","excerpt":"insert select 为什么有这么多锁？","text":"insert select 为什么有这么多锁？ 1. insert … select 语句1234567891011121314CREATE TABLE `t` ( `id` int(11) NOT NULL AUTO_INCREMENT, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `c` (`c`)) ENGINE=InnoDB;insert into t values(null, 1,1);insert into t values(null, 2,2);insert into t values(null, 3,3);insert into t values(null, 4,4);create table t2 like t 在可重复读隔离级别下，binlog_format=statement 时, insert into t2(c,d) select c,d from t; 需要对表 t 的所有行和间隙加锁。原因还是日志和数据的一致性。 如果没有锁的话，就可能出现 session B 的 insert 语句先执行，但是后写入 binlog 的情况。于是，在 binlog_format=statement 的情况下就会出现日志与数据的不一致。 2. insert 循环写入执行 insert … select 的时候，对目标表也不是锁全表，而是只锁住需要访问的资源。假设要往表 t2 中插入一行数据，这一行的 c 值是表 t 中 c 值的最大值加 1。1insert into t2(c,d) (select c+1, d from t force index(c) order by c desc limit 1); 这个语句的加锁范围，就是表 t 索引 c 上的 (3,4]和 (4,supremum]这两个 next-key lock，以及主键索引上 id=4 这一行。 如果我们是要把这样的一行数据插入到表 t 中的话：1insert into t(c,d) (select c+1, d from t force index(c) 使用 explain，查看二进制日志，以及 Innodb_rows_read Extra 字段可以看到“Using temporary”字样，表示这个语句用到了临时表 二进制日志显示执行过程中读取了 5 行 Innodb_rows_read 的值增加了 4。因为默认临时表是使用 Memory 引擎的，所以这 4 行查的都是表 t 也就是说对上面这条语句对表 t 做了全表扫描。执行过程如下: 创建临时表，表里有两个字段 c 和 d。 按照索引 c 扫描表 t，依次取 c=4、3、2、1，然后回表，读到 c 和 d 的值写入临时表。这时，Rows_examined=4。 由于语义里面有 limit 1，所以只取了临时表的第一行，再插入到表 t 中。这时，Rows_examined 的值加 1，变成了 5 也就是说，这个语句会导致在表 t 上做全表扫描，并且会给索引 c 上的所有间隙都加上共享的 next-key lock。所以，这个语句执行期间，其他事务不能在这个表上插入数据。 为什么需要临时表，原因是这类一边遍历数据，一边更新数据的情况，如果读出来的数据直接写回原表，就可能在遍历过程中，读到刚刚插入的记录，新插入的记录如果参与计算逻辑，就跟语义不符。 由于实现上这个语句没有在子查询中就直接使用 limit 1，从而导致了这个语句的执行需要遍历整个表 t。因此我们可以使用下面的 sql 进行优化 1234create temporary table temp_t(c int,d int) engine=memory;insert into temp_t (select c+1, d from t force index(c) order by c desc limit 1);insert into t select * from temp_t;drop table temp_t; 3. insert 唯一键冲突 这个例子也是在可重复读（repeatable read）隔离级别下执行的。可以看到，session B 要执行的 insert 语句进入了锁等待状态。也就是说，session A 执行的 insert 语句，发生唯一键冲突的时候，并不只是简单地报错返回，还在冲突的索引上加了锁。 一个 next-key lock 就是由它右边界的值定义的。这时候，session A 持有索引 c 上的 (5,10]共享 next-key lock（读锁）。 至于为什么要加这个读锁，没有找到合理的解释。从作用上来看，这样做可以避免这一行被别的事务删掉。 3.1 唯一键冲突导致的死锁场景 在 session A 执行 rollback 语句回滚的时候，session C 几乎同时发现死锁并返回。这个死锁产生的逻辑是这样的： 在 T1 时刻，启动 session A，并执行 insert 语句，此时在索引 c 的 c=5 上加了记录锁。 在 T2 时刻，session B 要执行相同的 insert 语句，发现了唯一键冲突，加上读锁；同样地，session C 也在索引 c 上，c=5 这一个记录上，加了读锁。 T3 时刻，session A 回滚。这时候，session B 和 session C 都试图继续执行插入操作，都要加上写锁。两个 session 都要等待对方的行锁，所以就出现了死锁 4. insert into … on duplicate key update如果将上面的session A 冲突改写成 insert into t values(11,10,10) on duplicate key update d=100; 就会给索引 c 上 (5,10] 加一个排他的 next-key lock（写锁）。 insert into … on duplicate key update 这个语义的逻辑是，插入一行数据，如果碰到唯一键约束，就执行后面的更新语句。注意，如果有多个列违反了唯一性约束，就会按照索引的顺序，修改跟第一个索引冲突的行。 5. 小结insert … select 是很常见的在两个表之间拷贝数据的方法。你需要注意，在可重复读隔离级别下 binlog=statement，这个语句会给 select 的表里扫描到的记录和间隙加读锁。 而如果 insert 和 select 的对象是同一个表，则有可能会造成循环写入。这种情况下，我们需要引入用户临时表来做优化。 insert 语句如果出现唯一键冲突，会在冲突的唯一值上加共享的 next-key lock(S 锁)。因此，碰到由于唯一键约束导致报错后，要尽快提交或回滚事务，避免加锁时间过长。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"7 MYSQL 索引","slug":"mysql/MySQL实战45讲/07_索引","date":"2020-03-06T16:00:00.000Z","updated":"2020-05-24T15:49:52.347Z","comments":true,"path":"2020/03/07/mysql/MySQL实战45讲/07_索引/","link":"","permalink":"http://yoursite.com/2020/03/07/mysql/MySQL实战45讲/07_索引/","excerpt":"B+树索引","text":"B+树索引 1. InnoDB 的索引模型实现索引的方式有很多方式，N 叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了。在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。每一个索引在 InnoDB 里面对应一棵 B+ 树。 根据叶子节点的内容，索引类型分为主键索引和非主键索引。 主键索引: 又称聚簇索引 clustered index，叶子节点保存的是整行数据。 非主键索引: 又称为二级索引 secondary index，叶子节保存的是主键的值。InnoDB会把主键字段放到索引定义字段后面，当然同时也会去重。 下面是一个示例，可以帮助我们理解主键，非主键索引的关系:12345678910CREATE TABLE `geek` ( `a` int(11) NOT NULL, `b` int(11) NOT NULL, `c` int(11) NOT NULL, `d` int(11) NOT NULL, PRIMARY KEY (`a`,`b`), KEY `c` (`c`), KEY `ca` (`c`,`a`), KEY `cb` (`c`,`b`)) ENGINE=InnoDB; 索引: 主键: 是 a,b 字段的聚簇索引，相当于 order by a,b 索引 c: 按 c 排序，同时记录主键，因为主键有排序，所以相当于 order by c,a,b 索引 ca: 先按 c 排序，再按 a 排序，同时记录主键，主键部分是 b，不是 ab，而是只有 b,相当于 order by c,a 索引 cb: 先按 c 排序，在按 b 排序，同时记录主键，主键部分只有 a，相当于 order by c,b 所以索引 ca 这里是重复的，应该被删除。 1.1 主键的选择由于树的有序性，并且每个叶子节点对应的数据页所能容纳的行数是有限制的，因此在数据的插入和删除过程中就会发生页的分裂和合并，因而会影响数据更新的性能。所以大多数情况下我们都建议使用自增主键 自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。同时每个非主键索引的叶子节点上都是主键的值。显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。 1.2 索引优化B+树索引有四种常见的优化方式: 覆盖索引: 索引包含的字段能够覆盖查询的需求，不用回表进行二次查询 左前缀索引: 合理调整索引字段的顺序，可以提高索引复用率，减少索引个数 常用的查询字段或者查询组合应该应该靠前 其次，考虑空间因素，例如name 字段是比 age 字段大的 ，应该创建一个（name,age) 的联合索引和一个 (age) 的单字段索引，而不是 (age, name) (name) 两个索引 索引下推: MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数 索引可能因为删除，或者页分裂等原因，导致数据页有空洞，适时地重建索引，把数据按顺序插入，这样页面的利用率最高，也就是索引更紧凑、更省空间。 对于主键索引，不论是删除主键还是创建主键，都会将整个表重建，因此主键索引的重建应该使用 alter table T engine=InnoDB，而不是 drop primary key, add primary key 2. 普通索引与唯一索引通过上面我们已经知道了Innodb B+ 树索引的基本机构，那对于普通索引和唯一索引我们应该怎么选择呢。我们就从这两种索引对查询语句和更新语句的性能影响来进行分析。 2.1 查询分析假设我们要执行 select id from T where k=5，字段 k 上有索引 对于普通索引来说，查找到满足条件的第一个记录 (5,500) 后，需要查找下一个记录，直到碰到第一个不满足 k=5 条件的记录。 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。 我们知道 InnoDB 的数据是按数据页为单位来读写的，对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针寻找和一次计算。所以二者的性能相差微乎其微 2.2 更新分析对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。而这必须要将数据页读入内存才能判断。因此，唯一索引的更新就不能使用 change buffer，实际上也只有普通索引可以使用。change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。 2.3 索引选择这两类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响，所以，我建议你尽量选择普通索引。即使是对于 changer buffer 不适用的场景，比如所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭 change buffer。而在其他情况下，change buffer 都能提升更新性能。 特别地，在使用机械硬盘时，change buffer 这个机制的收效是非常显著的。所以，当你有一个类似“历史数据”的库，并且出于成本考虑用的是机械硬盘时，那你应该特别关注这些表里的索引，尽量使用普通索引，然后把 change buffer 尽量开大，以确保这个“历史数据”表的数据写入速度。 最后关于普通索引和唯一索引的选择，首先，业务正确性优先。如果需要数据库保证数据唯一性，肯定是优先选择唯一索引 在一些“归档库”的场景，你是可以考虑使用普通索引的。比如，线上数据只需要保留半年，然后历史数据保存在归档库。这时候，归档数据已经是确保没有唯一键冲突了。要提高归档效率，可以考虑把表里面的唯一索引改成普通索引。 3. 如何给字符串添加索引MySQL 是支持前缀索引的，也就是说，你可以定义字符串的一部分作为索引。默认地，如果你创建索引的语句不指定前缀长度，那么索引就会包含整个字符串。指定前缀创建索引的好处是占用的空间会更小，可能会增加额外的记录扫描次数。 选择多长的前缀作为索引取决于索引的区分度，区分度越高，重复值越少，检索效率越高。长度越长区分度肯定越好，但是，索引选取的越长，占用的磁盘空间就越大，相同的数据页能放下的索引值就越少，搜索的效率也就会越低。我们可以通过统计索引上有多少个不同的值来判断要使用多长的前缀: 12345678910# 计算待索引列上有多少不同的值mysql&gt; select count(distinct email) as L from SUser;# 查看不同长度前缀有多少不同的值mysql&gt; select count(distinct left(email,4)）as L4, count(distinct left(email,5)）as L5, count(distinct left(email,6)）as L6, count(distinct left(email,7)）as L7,from SUser; 使用前缀索引很可能会损失区分度，所以你需要预先设定一个可以接受的损失比例，比如 5%，在可接受的范围内选择最短的长度构建索引。 除了区分度的影响，因为系统并不确定前缀索引的定义是否截断了完整信息，所以总是要回表查询整行数据，因此也就无法使用覆盖索引。这也是是否使用前缀索引需要考虑的因素。 有时候，我们会遇到前缀的区分度不够好的情况，使用太长的前缀，又会占用太多的存储空间。此时我们可以采用倒序存储或者 hash 字段方式创建索引。因此使用字符串创建索引，有以下几种方式: 直接创建完整索引，这样可能比较占用空间； 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引； 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题，不支持范围扫描； 创建 hash 字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描。 3.1 字符串索引使用示例1234567891011# 创建前缀索引mysql&gt; alter table SUser add index index1(email);mysql&gt; alter table SUser add index index2(email(6));# 倒序索引，每次写和读的时候，都需要额外调用一次 reverse 函数mysql&gt; select field_list from t where id_card = reverse('input_id_card_string');# hash 字段mysql&gt; alter table t add id_card_crc int unsigned, add index(id_card_crc);# hash 值可能存在冲突，必须在查询条件加上原始字段mysql&gt; select field_list from t where id_card_crc=crc32('input_id_card_string') and id_card='input_id_card_string'","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"6 MySQL 幻读与间隙锁","slug":"mysql/MySQL实战45讲/06_间隙锁","date":"2020-03-05T16:00:00.000Z","updated":"2020-05-24T15:50:11.754Z","comments":true,"path":"2020/03/06/mysql/MySQL实战45讲/06_间隙锁/","link":"","permalink":"http://yoursite.com/2020/03/06/mysql/MySQL实战45讲/06_间隙锁/","excerpt":"幻读","text":"幻读 1. 幻读幻读指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。对于幻读需要在注意: 在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。因此，幻读在“当前读”下才会出现。 修改结果，被之后的 select 语句用“当前读”看到，不能称为幻读。幻读仅专指“新插入的行” 锁的设计是为了保证数据的一致性。而这个一致性，不止是数据库内部数据状态在此刻的一致性，还包含了数据和日志在逻辑上的一致性。基于行锁的并发控制，只能保护已经存在的行，但是对于新插入的行就会出现未保护的临界状态。 行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。因此，为了解决幻读问题，InnoDB 只好引入新的锁，也就是间隙锁 (Gap Lock)。 2. 间隙锁间隙锁，锁的就是两个值之间的空隙。数据行是可以加上锁的实体，数据行之间的间隙，也是可以加上锁的实体。但是间隙锁跟我们之前碰到过的锁都不太一样。 比如行锁，分成读锁和写锁。写锁跟任何其他读锁和写锁都是冲突的，也就是说，跟行锁有冲突关系的是“另外一个行锁”。但是间隙锁不一样，跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作。间隙锁之间都不存在冲突关系。即间隙锁保护的是这个间隙，不允许插入值。但，它们之间是不冲突的。 2.1 加锁范围我们创建下表，这个表除了主键 id 外，还有一个索引 c，初始化语句在表中插入了 6 行数据。 12345678910CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`)) ENGINE=InnoDB;insert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25); 间隙锁，锁的就是两个值之间的空隙。表 t，初始化插入了 6 个记录，这就产生了 7 个间隙。 间隙锁和行锁合称 next-key lock，每个 next-key lock 是前开后闭区间。也就是说，如果用 select * from t for update 要把整个表所有记录锁起来，就形成了 7 个 next-key lock，分别是 (-∞,0]、(0,5]、(5,10]、(10,15]、(15,20]、(20, 25]、(25, +supremum]。+∞是开区间，实现上，InnoDB 给每个索引加了一个不存在的最大值 supremum，这样就符合前面说的“都是前开后闭区间”。 2.2 间隙锁的理解间隙锁的引入，可能会导致同样的语句锁住更大的范围，这其实是影响了并发度的。 间隙锁是在可重复读隔离级别下才会生效的。所以，你如果把隔离级别设置为读提交的话，就没有间隙锁了。但同时，你要解决可能出现的数据和日志不一致问题，需要把 binlog 格式设置为 row。这，也是现在不少公司使用的配置组合。 读提交隔离级别加 binlog_format=row 的组合和可重复度隔离级别应该如何选择，跟业务场景有关。如果读提交隔离级别够用，也就是说，业务不需要可重复读的保证，这样考虑到读提交下操作数据的锁范围更小（没有间隙锁），这个选择是合理的。 3. 加锁规则因为间隙锁在可重复读隔离级别下才有效，下面的规则，若没有特殊说明，默认是可重复读隔离级别。丁老师总结的加锁规则里面，包含了两个“原则”、两个“优化”和一个“bug”: 原则 1：加锁的基本单位是 next-key lock。希望你还记得，next-key lock 是前开后闭区间。 原则 2：查找过程中访问到的对象才会加锁。 优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。 优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。 一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。 理解这个规则需要注意两点: 无论是等值查询，还是范围查询，在执行过程中，首先都要通过树搜索的方式定位记录，定位记录用的就是“等值查询”的方法。没有 desc 以下限值进行定位，有 desc 时以上线值进行定位。 锁是“在执行过程中一个一个加的”，而不是一次性加上去的 间隙锁本身是不互斥的 所谓“间隙”，其实根本就是由“这个间隙右边的那个记录”定义的。 使用 show engine innodb status 可以查看锁等待，死锁等信息。 下面的表 t 是上面 2.1节 创建并初始化的表。 3.1 等值查询间隙锁第一个例子是关于等值条件操作间隙： 由于表 t 中没有 id=7 的记录，因此加锁为范围: 根据原则 1，加锁单位是 next-key lock，session A 加锁范围就是 (5,10] 同时根据优化 2，这是一个等值查询 (id=7)，而 id=10 不满足查询条件，next-key lock 退化成间隙锁，因此最终加锁的范围是 (5,10) 3.2 非唯一索引等值锁第二个例子是关于覆盖索引上的锁： 这里 session A 要给索引 c 上 c=5 的这一行加上读锁: 根据原则 1，加锁单位是 next-key lock，因此会给 (0,5]加上 next-key lock。 要注意 c 是普通索引，因此仅访问 c=5 这一条记录是不能马上停下来的，需要向右遍历，查到 c=10 才放弃。根据原则 2，访问到的都要加锁，因此要给 (5,10]加 next-key lock 但是同时这个符合优化 2：等值判断，向右遍历，最后一个值不满足 c=5 这个等值条件，因此退化成间隙锁 (5,10)。 根据原则 2 ，只有访问到的对象才会加锁，这个查询使用覆盖索引，并不需要访问主键索引，所以主键索引上没有加任何锁，这就是为什么 session B 的 update 语句可以执行完成。 在这个例子中，lock in share mode 只锁覆盖索引，但是如果是 for update 就不一样了。 执行 for update 时，系统会认为你接下来要更新数据，因此会顺便给主键索引上满足条件的行加上行锁。满足条件的行指的是索引 c 上所有被加锁的节点对应的主键 id。 锁是加在索引上的；同时，它给我们的指导是，如果你要用 lock in share mode 来给行加读锁避免数据被更新的话，就必须得绕过覆盖索引的优化，在查询字段中加入索引中不存在的字段。比如，将 session A 的查询语句改成 select d from t where c=5 lock in share mode。 3.3 主键索引范围锁第三个例子是关于范围查询的。 session A 的加锁范围: 开始执行的时候，要找到第一个 id=10 的行，因此本该是 next-key lock(5,10]。 根据优化 1， 主键 id 上的等值条件，退化成行锁，只加了 id=10 这一行的行锁 范围查找就往后继续找，找到 id=15 这一行停下来，因此需要加 next-key lock(10,15]。 需要注意一点，首次 session A 定位查找 id=10 的行的时候，是当做等值查询来判断的，而向右扫描到 id=15 的时候，用的是范围查询判断。这就是上面所说的 bug，唯一索引上的范围查询会访问到不满足条件的第一个值为止。并且不是等值查询，也不会优化。 3.4 非唯一索引范围锁 这次 session A 用字段 c 来判断，加锁规则跟案例三唯一的不同是： 在第一次用 c=10 定位记录的时候，索引 c 上加了 (5,10]这个 next-key lock 后，由于索引 c 是非唯一索引，没有优化规则，也就是说不会蜕变为行锁， 最终 sesion A 加的锁是，索引 c 上的 (5,10] 和 (10,15] 这两个 next-key lock。 3.5 唯一索引范围锁 bug ession A 是一个范围查询: 按照原则 1 的话，应该是索引 id 上只加 (10,15]这个 next-key lock，并且因为 id 是唯一键，所以循环判断到 id=15 这一行就应该停止了。 但是实现上，InnoDB 会往前扫描到第一个不满足条件的行为止，也就是 id=20。而且由于这是个范围扫描，因此索引 id 上的 (15,20]这个 next-key lock 也会被锁上。 从这个例子也可以看出，在加锁时等值查询加锁，范围查询加锁时独立，并且取的是最后的交集。 3.6 非唯一索引上存在”等值”的例子接下来的例子，是为了更好地说明“间隙”这个概念。这里，我给表 t 插入一条新记录。1mysql&gt; insert into t values(30,10,30); 新插入的这一行 c=10，也就是说现在表里有两个 c=10 的行。你要知道，由于非唯一索引上包含主键的值，所以是不可能存在“相同”的两行的。 虽然有两个 c=10，但是它们的主键值 id 是不同的（分别是 10 和 30），因此这两个 c=10 的记录之间，也是有间隙的。 现在，我们来看一下案例六。这次我们用 delete 语句来验证。注意，delete 语句加锁的逻辑，其实跟 select … for update 是类似的 此时的加锁范围: session A 在遍历的时候，先访问第一个 c=10 的记录。同样地，根据原则 1，这里加的是 (c=5,id=5) 到 (c=10,id=10) 这个 next-key lock。 然后，session A 向右查找，直到碰到 (c=15,id=15) 这一行，循环才结束。根据优化 2，这是一个等值查询，向右查找到了不满足条件的行，所以会退化成 (c=10,id=10) 到 (c=15,id=15) 的间隙锁。 这个蓝色区域左右两边都是虚线，表示开区间，即 (c=5,id=5) 和 (c=15,id=15) 这两行上都没有锁。 3.7 limit 语句加锁例子 6 也有一个对照案例，场景如下所示： 案例七里的 delete 语句明确加了 limit 2 的限制，因此在遍历到 (c=10, id=30) 这一行之后，满足条件的语句已经有两条，循环就结束了。 索引 c 上的加锁范围就变成了从（c=5,id=5) 到（c=10,id=30) 这个前开后闭区间，如下图所示： 这个例子对我们实践的指导意义就是，在删除数据的时候尽量加 limit。这样不仅可以控制删除数据的条数，让操作更安全，还可以减小加锁的范围。 3.8 一个死锁的例子前面的例子中，我们在分析的时候，是按照 next-key lock 的逻辑来分析的，因为这样分析比较方便。最后我们再看一个案例，目的是说明：next-key lock 实际上是间隙锁和行锁加起来的结果。 我们按时间顺序来分析一下为什么是这样的结果。 session A 启动事务后执行查询语句加 lock in share mode，在索引 c 上加了 next-key lock(5,10] 和间隙锁 (10,15)； session B 的 update 语句也要在索引 c 上加 next-key lock(5,10] ，进入锁等待； 然后 session A 要再插入 (8,8,8) 这一行，被 session B 的间隙锁锁住。由于出现了死锁，InnoDB 让 session B 回滚。 你可能会问，session B 的 next-key lock 不是还没申请成功吗？其实是这样的，session B 的“加 next-key lock(5,10] ”操作，实际上分成了两步，先是加 (5,10) 的间隙锁，加锁成功；然后加 c=10 的行锁，这时候才被锁住的。也就是说，我们在分析加锁规则的时候可以用 next-key lock 来分析。但是要知道，具体执行的时候，是要分成间隙锁和行锁两段来执行的。 3.9 读提交的加锁规则我们上面的所有案例都是在可重复读隔离级别 (repeatable-read) 下验证的。同时，可重复读隔离级别遵守两阶段锁协议，所有加锁的资源，都是在事务提交或者回滚的时候才释放的。 在最后的案例中，你可以清楚地知道 next-key lock 实际上是由间隙锁加行锁实现的。如果切换到读提交隔离级别 (read-committed) 的话，就好理解了，过程中去掉间隙锁的部分，也就是只剩下行锁的部分。 其实读提交隔离级别在外键场景下还是有间隙锁，相对比较复杂。 另外，在读提交隔离级别下还有一个优化，即：语句执行过程中加上的行锁，在语句执行完成后，就要把“不满足条件的行”上的行锁直接释放了，不需要等到事务提交。 也就是说，读提交隔离级别下，锁的范围更小，锁的时间更短，这也是不少业务都默认使用读提交隔离级别的原因。 3.10 非索引列加锁对于非索引列而言，因为无法直接精确定位值的位置，因此只能进行全表扫描: 在可重复读隔离级别下，会对所有行和间隙加锁 在读提交隔离级别下，语句执行过程中会对所有行加上行锁，在语句执行完成后，就要把“不满足条件的行”上的行锁直接释放了，不需要等到事务提交。 123begin;select * from t where d=5 for update;commit; 上面这个例子，在读提交隔离级别下，在 select * from t where d=5 for update; 的执行过程中，所有行都会被加锁；语句执行完成之后，只会对 d=5 这一行加锁直至事务提交释放。 3.11 desc 语句加锁 session A 的 select 语句加了哪些锁： 由于是 order by c desc，第一个要定位的是索引 c 上“最右边的”c=20 的行，所以会加上间隙锁 (20,25) 和 next-key lock (15,20]。 在索引 c 上向左遍历，要扫描到 c=10 才停下来，所以 next-key lock 会加到 (5,10]，这正是阻塞 session B 的 insert 语句的原因。 在扫描过程中，c=20、c=15、c=10 这三行都存在值，由于是 select *，所以会在主键 id 上加三个行锁。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"5 MYSQL 事务","slug":"mysql/MySQL实战45讲/05_事务","date":"2020-03-04T16:00:00.000Z","updated":"2020-05-24T15:55:49.662Z","comments":true,"path":"2020/03/05/mysql/MySQL实战45讲/05_事务/","link":"","permalink":"http://yoursite.com/2020/03/05/mysql/MySQL实战45讲/05_事务/","excerpt":"事务的隔离性和回滚日志","text":"事务的隔离性和回滚日志 1.事务的隔离性事务的隔离级别包括: 读未提交: read uncommitted，一个事务还没提交时，它做的变更就能被别的事务看到 读提交: read committed，一个事务提交之后，它做的变更才会被其他事务看到 可重复读: repeatable read，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的 和串行化: 对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行 在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。 “可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图 在“读提交”隔离级别下，这个视图是在每个 SQL 语句开始执行的时候创建的。 “读未提交”隔离级别下直接返回记录上的最新值，没有视图概念 “串行化”隔离级别下直接用加锁的方式来避免并行访问 MySQL 中数据的隔离级别由参数 transaction-isolation 配置 1.1 MVCC 与回滚日志在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。 不同时刻启动的事务会有不同的 read-view。不同的 read-view 之间是不会相互影响的。同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。 系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的 read-view 的时候。 为什么不要使用长事务 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。 在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。 除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库。 1.2 事务提交与管理autocommit: 是否自动提交事务 =0: 关闭事务的自动提交，意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。如果是长连接，就可能导致意外的长事务。 =1: 开始事务自动提交，事务启动需要显示使用 begin 或 start transaction配套的提交语句是 commit，回滚语句是 rollback。对于频繁使用事务的业务，可以使用 commit work and chain 语法，在事务提交时，自动开启一个新的事务，以减少 begin 语句的交互次数 可以在 information_schema 库的 innodb_trx 这个表中查询长事务，比如 1select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))&gt;60 事务启动的时机begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用 start transaction with consistent snapshot 这个命令。 第一种启动方式，一致性视图是在执行第一个快照读语句时创建的； 第二种启动方式，一致性视图是在执行 start transaction with consistent snapshot 时创建的。 1.3 如何避免长事务对业务的影响从应用开发端来看： 确认是否使用了 set autocommit=0。这个确认工作可以在测试环境中开展，把 MySQL 的 general_log 开起来，然后随便跑一个业务逻辑，通过 general_log 的日志来确认。一般框架如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它改成 1。 确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用 begin/commit 框起来。我见过有些是业务并没有这个需要，但是也把好几个 select 语句放到了事务中。这种只读事务可以去掉。 业务连接数据库的时候，根据业务本身的预估，通过 SET MAX_EXECUTION_TIME 命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。 其次，从数据库端来看： 监控 information_schema.Innodb_trx 表，设置长事务阈值，超过就报警 / 或者 kill；Percona 的 pt-kill 这个工具不错，推荐使用； 在业务功能测试阶段要求输出所有的 general_log，分析日志行为提前发现问题； 如果使用的是 MySQL 5.6 或者更新版本，把 innodb_undo_tablespaces 设置成 2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。 2.事务的一致性读视图在 MySQL 里，有两个“视图”的概念： 一个是 view，它是一个用查询语句定义的虚拟表 另一个是 InnoDB 在实现 MVCC 时用到的一致性读视图，即 consistent read view，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现 2.1 MVCC 和一致性视图MVCC InnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id 每行数据也都是有多个版本的，每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的row trx_id 旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它 也就是说，数据表中的一行记录，其实可能有多个版本 (row)，每个版本有自己的 row trx_id。下面是一个记录被多个事务连续更新后的状态 图中虚线框里是同一行数据的 4 个版本，当前最新版本是 V4 三个虚线箭头，就是 undo log； V1、V2、V3 并不是物理上真实存在的，而是每次需要的时候根据当前版本和 undo log 计算出来的 一致性视图InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。数组里面事务 ID 的最小值记为低水位，当前系统里面已经创建过的事务 ID 的最大值加 1 记为高水位。 这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。而数据版本的可见性规则，就是基于数据的 row trx_id 和这个一致性视图的对比结果得到的。这个视图数组把所有的 row trx_id 分成了几种不同的情况。 对于当前事务的启动瞬间来说，一个数据版本的 row trx_id，有以下几种可能： 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的； 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的； 如果落在黄色部分，那就包括两种情况a. 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见； 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见 上面的规则翻译一下: 一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况： 版本未提交，不可见； 版本已提交，但是是在视图创建后提交的，不可见； 版本已提交，而且是在视图创建前提交的，可见。 InnoDB 利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。 视图的更新读数据时是按照上述规则的一致性读，但是更新数据是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。因为更新时如果根据数据的历史版本去更新，就会导致当前最新版本出现数据丢失。 除了 update 语句外，select 语句如果加锁，也是当前读。 select + lock in share mode: 加读锁(S 锁，共享锁） select + for update: 加写锁（X 锁，排他锁） 而 update 更新语句的当前读也会给当前最新版本的数据加上读锁。 更新与两阶段锁 假设有上面一组更新事务: 虽然事务 C’还没提交，但是 (1,2) 这个版本也已经生成了，并且是当前的最新版本。 事务 C’没提交，也就是说 (1,2) 这个版本上的写锁还没释放 而事务 B 是当前读，必须要读最新版本，而且必须加锁，因此就被锁住了，必须等到事务 C’释放这个锁，才能继续它的当前读。 到这里，我们把一致性读、当前读和行锁就串起来了。 RR 与 RC可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。 而读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是： 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图； 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"4 MYSQL 锁","slug":"mysql/MySQL实战45讲/04_锁","date":"2020-03-03T16:00:00.000Z","updated":"2020-05-24T15:49:33.170Z","comments":true,"path":"2020/03/04/mysql/MySQL实战45讲/04_锁/","link":"","permalink":"http://yoursite.com/2020/03/04/mysql/MySQL实战45讲/04_锁/","excerpt":"全局锁 - 表锁 - 行锁","text":"全局锁 - 表锁 - 行锁 1. 全局锁全局锁: 作用: 对整个数据库实例加锁 加锁: Flush tables with read lock 解锁: unlock tables，客户端断开时会自动释放锁 场景: 全库逻辑备份，即把整库每个表都 select 出来存成文本 加锁范围: 数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句都会被阻塞 做全库备份时，对于 Innodb，通过可重复度隔离级别我们就可以获取数据库的一致视图，但是对 于MyISAM 这些不支持事务的存储引擎，只能使用 Flush tables with read lock 让整个库处于只读状态。 既然要全库只读，为什么不使用 set global readonly=true 的方式呢？确实 readonly 方式也可以让全库进入只读状态，但我还是会建议你用 FTWRL 方式，主要有两个原因： 在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大 在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。 2.表级锁MySQL 表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。 表锁: 加锁: lock tables t read/write tables 后面 s 可省略 read/writer 实际上加的 MDL 读锁/写锁 解锁: unlock tables，客户端断开时会自动释放锁 加锁范围: 除了会限制别的线程的读写外，也限定了本线程接下来的操作对象，如果在某个线程 A 中执行 lock tables t1 read, t2 write;线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。连写 t1 都不允许，自然也不能访问其他表，线程A只能访问他锁定的表 元数据锁: 加锁: MDL 不需要显式使用，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。MDL 的作用是，保证读写的正确性 解锁: 事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放 在做表结构变更的时候，如果操作不慎，就会导致锁住线上查询和更新。 session A 先启动，这时候会对表 t 加一个 MDL 读锁 由于 session B 需要的也是 MDL 读锁，因此可以正常执行 因为 session A 的 MDL 读锁还没有释放，而 session C 需要 MDL 写锁，因此只能被阻塞 session D 等后续所有对表的增删改查操作都需要先申请 MDL 读锁，就都被锁住，等于这个表现在完全不可读写了 如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新 session 再请求的话，这个库的线程很快就会爆满。 因为MDL锁只有在事务提交之后才会释放，因此对于存在长事务，或者操作非常频繁的表做DDL时要非常小心。好的做法是: 在 MySQL 的 information_schema 库的 innodb_trx 表中，可以查到当前执行中的事务。要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务 在 alter table 语句里面设定等待时间: 12ALTER TABLE tbl_name NOWAIT add column ...ALTER TABLE tbl_name WAIT N add column ... 3.行锁行锁就是针对数据表中行记录的锁。MySQL 的行锁是在引擎层由各个引擎自己实现的。MyISAM 引擎就不支持行锁。 3.1 两阶段锁在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。 根据两阶段锁协议，不论你怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才释放的。如果把最可能造成冲突的锁语句放在最后面，这个锁的占用的时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。 3.2 死锁和死锁检测当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。当出现死锁以后，有两种策略： 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。 但是这两种策略都各有利弊:innodb_lock_wait_timeout: 默认值是 50s，对于在线服务来说，这个等待时间往往是无法接受的。我们又不可能直接把这个时间设置成一个很小的值，因为如果是简单的锁等待而不是死锁，过短的超时时间会造成很多误伤。正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且 innodb_deadlock_detect 的默认值本身就是 on。 主动死锁检测: 在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。假如所有事务都要更新同一行，每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n) 的操作。假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的 CPU 资源。因此，你就会看到 CPU 利用率很高，但是每秒却执行不了几个事务。 热点行更新导致的性能问题热点行更新导致的性能问题的症结在于，死锁检测要耗费大量的 CPU 资源。解决办法有如下几种: 如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关掉死锁检测意味着可能会出现大量的超时，这是业务有损的。 另一个思路是控制访问相同资源的并发事务量 比如同一行同时最多只有 10 个线程在更新，那么死锁检测的成本很低。并发控制要做在数据库服务端。如果有中间件，可以考虑在中间件实现；如果能修改 MySQL 源码的人，也可以做在 MySQL 里面。基本思路就是，对于相同行的更新，在进入引擎之前排队。这样在 InnoDB 内部就不会有大量的死锁检测工作了。 考虑通过将一行改成逻辑上的多行来减少锁冲突，但是需要根据业务逻辑做详细设计 4.MDL与主从同步问题问题: 当备库用–single-transaction 做逻辑备份的时候，如果从主库的 binlog 传来一个 DDL 语句会怎么样？ 假设这个 DDL 是针对表 t1 的，备份过程的语句如下:123456789101112Q1:SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ; # 确保 RR（可重复读）隔离级别Q2:START TRANSACTION WITH CONSISTENT SNAPSHOT； # 得到一个一致性视图/* other tables */ Q3:SAVEPOINT sp; # 设置一个保存点/* 时刻 1 */Q4:show create table `t1`; # 拿到表结构/* 时刻 2 */Q5:SELECT * FROM `t1`; # 正式导数据 /* 时刻 3 */Q6:ROLLBACK TO SAVEPOINT sp; # 释放 t1 的 MDL 锁 /* 时刻 4 *//* other tables */ DDL 从主库传过来的时间按照效果不同: 如果在 Q4 语句执行之前到达，现象：没有影响，备份拿到的是 DDL 后的表结构。 如果在“时刻 2”到达，则表结构被改过，Q5 执行的时候，报 Table definition has changed, please retry transaction，现象：mysqldump 终止； 如果在“时刻 2”和“时刻 3”之间到达，mysqldump 占着 t1 的 MDL 读锁，binlog 被阻塞，现象：主从延迟，直到 Q6 执行完成。 从“时刻 4”开始，mysqldump 释放了 MDL 读锁，现象：没有影响，备份拿到的是 DDL 前的表结构 5. 如何查看锁等待如果一条语句长时间不返回，一般碰到这种情况的话，大概率是表 t 被锁住了。一般首先执行一下 show processlist 命令，看看当前语句处于什么状态。然后我们再针对每种状态，去分析它们产生的原因、如何复现，以及如何处理。下面我们将分成如下几种情况来分析如何查看和解决锁等待: MDL 锁等待 5.1 MDL 锁等待 show processlist 出现 Waiting for table metadata lock 时，表示的是，现在有一个线程正在表 t 上请求或者持有 MDL 写锁，导致 select 语句被堵住。 这类问题的处理方式，就是找到谁持有 MDL 写锁，然后把它 kill 掉。 但是，由于在 show processlist 的结果里面，session A 的 Command 列是“Sleep”，导致查找起来很不方便。不过有了 performance_schema 和 sys 系统库以后，就方便多了。（MySQL 启动时需要设置 performance_schema=on，相比于设置为 off 会有 10% 左右的性能损失) 5.2 等待 flush 这个状态表示的是，现在有一个线程正要对表 t 做 flush 操作。MySQL 里面对表做 flush 操作的用法，一般有以下两个，如果指定表 t 的话，代表的是只关闭表 t；如果没有指定具体的表名，则表示关闭 MySQL 里所有打开的表。 123flush tables t with read lock;flush tables with read lock; 但是正常这两个语句执行起来都很快，除非它们也被别的线程堵住了。所以，出现 Waiting for table flush 状态的可能情况是：有一个 flush tables 命令被别的语句堵住了，然后它又堵住了我们的 select 语句。这个例子很简单，通过 processlist 直接就可以看出导致阻塞的语句。 5.3 等行锁 通过 lock in share mode 可以判断，sql 语句正在等待行锁。但问题是怎么查出是谁占着这个写锁。如果你用的是 MySQL 5.7 版本，可以通过 sys.innodb_lock_waits 表查到。 blocking_pid 显示 4 号线程是造成堵塞的罪魁祸首。而干掉这个罪魁祸首的方式，就是 KILL QUERY 4 或 KILL 4。 KILL QUERY 4: 表示停止 4 号线程当前正在执行的语句，而这个方法其实是没有用的。因为占有行锁的是 update 语句，这个语句已经是之前执行完成了的，现在执行 KILL QUERY，无法让这个事务去掉 id=1 上的行锁 KILL 4: 表示直接断开这个连接。连接被断开的时候，会自动回滚这个连接里面正在执行的线程，也就释放了 id=1 上的行锁。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"3 Innodb 表空间回收","slug":"mysql/MySQL实战45讲/03_Innodb表空间的回收","date":"2020-03-02T16:00:00.000Z","updated":"2020-05-24T15:50:02.073Z","comments":true,"path":"2020/03/03/mysql/MySQL实战45讲/03_Innodb表空间的回收/","link":"","permalink":"http://yoursite.com/2020/03/03/mysql/MySQL实战45讲/03_Innodb表空间的回收/","excerpt":"我的数据库占用空间太大，我把一个最大的表删掉了一半的数据，怎么表文件的大小还是没变？","text":"我的数据库占用空间太大，我把一个最大的表删掉了一半的数据，怎么表文件的大小还是没变？ 1.innodb_file_per_table一个 InnoDB 表包含两部分，即：表结构定义和数据。在 MySQL 8.0 版本以前，表结构是存在以.frm 为后缀的文件里。而 MySQL 8.0 版本，则已经允许把表结构定义放在系统数据表中了。 表数据既可以存在共享表空间里，也可以是单独的文件。这个行为是由参数 innodb_file_per_table 控制的。 innodb_file_per_table=off 位置: 所有数据库的所有存储引擎为Innodb的表使用同一个表空间文件 datadir/ibdata[N]: 共用的表空间文件，用于保存所有Innodb表的数据和索引 数据库目录/db_name.frm: 表结构定义保存在各个数据库目录下 特性: 不支持单表导入等高级特性 innodb_file_per_table=on 位置: 每表使用单独的表空间文件，位于各个数据库目录下 db_name.ibd: 表单独的表空间文件，用于存储单独表的数据和索引 db_name.frm: 用于存储表结构定义 建议将 innodb_file_table 设置为 ON，因为一个表单独存储为一个文件更容易管理，而且在你不需要这个表的时候，通过 drop table 命令，系统就会直接删除这个文件。而如果是放在共享表空间中，即使表删掉了，空间也是不会回收的。 2. 数据的删除流程InnoDB 里的数据都是用 B+ 树的结构组织的，数据存储在磁盘页中。数据的删除就分成了两种情况: 删除一个记录: InnoDB 引擎只会把记录标记为删除，有符合范围条件的数据插入时，这个记录会被复用 一个数据页内的所有记录都被删除，整个数据页就可以被复用了。 但是记录跟页的复用是有区别的: 记录的复用，只限于符合范围条件的数据。 页的复用可以复用到任何位置 如果相邻两个数据页利用率都很小，系统会把这两个页上的数据合到其中一个页上，另外一个数据页就被标记为可复用 所以 delete 删除命令只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的。也就是说，通过 delete 命令是不能回收表空间的。可以复用，而没有被使用的空间，看起来就像是“空洞”。 不止是删除数据会造成空洞，插入数据也会。随机的数据插入会导致页分裂，另外，更新索引上的值，可以理解为删除一个旧的值，再插入一个新值。不难理解，这也是会造成空洞的。 经过大量增删改的表，都是可能是存在空洞的。所以，如果能够把这些空洞去掉，就能达到收缩表空间的目的。而重建表，就可以达到这样的目的。重建表就可以达到这样的目的。在重建表的时候，InnoDB 不会把整张表占满，每个页留了 1/16 给后续的更新用。 3.重建表回收表空间可以使用 alter table A engine=InnoDB 命令来重建表。这个语句在不同的 MySQL 版本中行为是不同的。 3.1 MySQL&lt;5.5alter table A engine=InnoDB 的执行流程如下: 转存数据: 新建一个与表 A 结构相同的表 B，然后按照主键 ID 递增的顺序，把数据一行一行地从表 A 里读出来再插入到表 B 中 交换表名 删除旧表 注意: 临时数据存放在 tmp_table 中，这是一个临时表，是在 server 层创建的 新版本中等同于执行命令 alter table t engine=innodb,ALGORITHM=inplace; 新建一个与表 A 结构相同的表 B，然后按照主键 ID 递增的顺序，把数据一行一行地从表 A 里读出来再插入到表 B 中。 花时间最多的步骤是往临时表插入数据的过程，如果在这个过程中，有新的数据要写入到表 A 的话，就会造成数据丢失。因此，在整个 DDL 过程中，表 A 中不能有更新。也就是说，这个 DDL 不是 Online 的。 3.2 MySQL&gt;=5.5注意: 临时数据存放在 tmp_file 中，“tmp_file”里的，这个临时文件是 InnoDB 在内部创建出来的 等同于命令 alter table t engine=innodb,ALGORITHM=inplace; MySQL 5.6 版本开始引入的 Online DDL，对这个操作流程做了优化。重建表的流程如下： 建立一个临时文件，扫描表 A 主键的所有数据页； 用数据页中表 A 的记录生成 B+ 树，存储到临时文件中； 生成临时文件的过程中，将所有对 A 的操作记录在一个日志文件（row log）中，对应的是图中 state2 的状态； 临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表 A 相同的数据文件，对应的就是图中 state3 的状态； 用临时文件替换表 A 的数据文件。 图 4 的流程中，alter 语句在启动的时候需要获取 MDL 写锁，但是这个写锁在真正拷贝数据之前就退化成读锁了。MDL 读锁不会阻塞增删改操作，同时保护自己，禁止其他线程对这个表同时做 DDL。 由于日志文件记录和重放操作这个功能的存在，这个方案在重建表的过程中，允许对表 A 做增删改操作。这也就是 Online DDL 名字的来源。 上述的这些重建方法都会扫描原表数据和构建临时文件。对于很大的表来说，这个操作是很消耗 IO 和 CPU 资源的。因此，如果是线上服务，你要很小心地控制操作时间。如果想要比较安全的操作的话，推荐使用 GitHub 开源的 gh-ost 4. 三种重建表的语法optimize table、analyze table 和 alter table: 从 MySQL 5.6 版本开始，alter table t engine = InnoDB（也就是 recreate）默认的就是ALGORITHM=inplace 的过程 analyze table t 其实不是重建表，只是对表的索引信息做重新统计，没有修改数据，这个过程中加了 MDL 读锁； optimize table t 等于 recreate+analyze","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"2 MySQL 如何保证数据不丢失","slug":"mysql/MySQL实战45讲/02_mysql如何保证数据不丢失","date":"2020-03-01T16:00:00.000Z","updated":"2020-05-24T15:49:22.516Z","comments":true,"path":"2020/03/02/mysql/MySQL实战45讲/02_mysql如何保证数据不丢失/","link":"","permalink":"http://yoursite.com/2020/03/02/mysql/MySQL实战45讲/02_mysql如何保证数据不丢失/","excerpt":"redo log，bin log 的写入流程","text":"redo log，bin log 的写入流程 前面我们介绍了 WAL 机制，得到的结论是：只要 redo log 和 binlog 保证持久化到磁盘，就能确保 MySQL 异常重启后，数据可以恢复。今天，我们就再一起看看 MySQL 写入 binlog 和 redo log 的流程，看看 MySQL 是如何保证数据不丢失的。 1. binlog 写入机制binlog 的写入逻辑比较简单：事务执行过程中，先把日志写到 binlog cache，事务提交的时候，再把 binlog cache 写到 binlog 文件中。一个事务的 binlog 是不能被拆开的，因此不论这个事务多大，也要确保一次性写入。 系统给 binlog cache 分配了一片内存，每个线程一个，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。 事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 中，并清空 binlog cache。 整个写入的过程如上图，可以看出: 每个线程有自己 binlog cache，但是共用同一份 binlog 文件。 write，指的就是指把日志写入到文件系统的 page cache，并没有把数据持久化到磁盘，所以速度比较快 fsync，才是将数据持久化到磁盘的操作。一般情况下，我们认为 fsync 才占磁盘的 IOPS write 和 fsync 的时机，是由参数 sync_binlog 控制的： write 和 fsync 的时机，是由参数 sync_binlog 控制的： sync_binlog=1 的时候，表示每次提交事务都会执行 fsync； sync_binlog=N(N&gt;1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。 在出现 IO 瓶颈的场景里，将 sync_binlog 设置成一个比较大的值，可以提升性能。在实际的业务场景中，考虑到丢失日志量的可控性，一般不建议将这个参数设成 0，比较常见的是将其设置为 100~1000 中的某个数值。 但是，将 sync_binlog 设置为 N，对应的风险是：如果主机发生异常重启，会丢失最近 N 个事务的 binlog 日志。 2. redolog 写入机制事务在执行过程中，生成的 redo log 是要先写到 redo log buffer 的: redo log buffer 里面的内容，每次生成后不要要直接持久化到磁盘，如果事务执行期间 MySQL 发生异常重启，那这部分日志就丢了。由于事务并没有提交，所以这时日志丢了也不会有损失。 事务还没提交的时候，redo log buffer 中的部分日志可能被持久化到磁盘 2.1 redolog 的三种状态 redolog 有三种状态: 存在 redo log buffer 中，物理上是在 MySQL 进程内存中，就是图中的红色部分； 写到磁盘 (write)，但是没有持久化（fsync)，物理上是在文件系统的 page cache 里面，也就是图中的黄色部分 持久化到磁盘，对应的是 hard disk，也就是图中的绿色部分，持久化到磁盘相对于写入 buffer 和 page 要慢的多 innodb_flush_log_at_trx_commit 控制了 redo log 的写入策略: 设置为 0 的时候，表示每次事务提交时都只是把 redo log 留在 redo log buffer 中 ; 设置为 1 的时候，表示每次事务提交时都将 redo log 直接持久化到磁盘； 设置为 2 的时候，表示每次事务提交时都只是把 redo log 写到 page cache InnoDB 有一个后台线程，每隔 1 秒，就会把 redo log buffer 中的日志，调用 write 写到文件系统的 page cache，然后调用 fsync 持久化到磁盘。注意，事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些 redo log 也会被后台线程一起持久化到磁盘。也就是说，一个没有提交的事务的 redo log，也是可能已经持久化到磁盘的。 实际上，除了后台线程每秒一次的轮询操作外，还有两种场景会让一个没有提交的事务的 redo log 写入到磁盘中。 一种是，redo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动写盘。注意，由于这个事务并没有提交，所以这个写盘动作只是 write，而没有调用 fsync，也就是只留在了文件系统的 page cache。 另一种是，并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘。假设一个事务 A 执行到一半，已经写了一些 redo log 到 buffer 中，这时候有另外一个线程的事务 B 提交，如果 innodb_flush_log_at_trx_commit 设置的是 1，那么按照这个参数的逻辑，事务 B 要把 redo log buffer 里的日志全部持久化到磁盘。这时候，就会带上事务 A 在 redo log buffer 里的日志一起持久化到磁盘。 我们介绍两阶段提交的时候说过，时序上 redo log 先 prepare， 再写 binlog，最后再把 redo log commit。 如果把 innodb_flush_log_at_trx_commit 设置成 1，那么 redo log 在 prepare 阶段就要持久化一次，因为有一个崩溃恢复逻辑是要依赖于 prepare 的 redo log，再加上 binlog 来恢复的。 每秒一次后台轮询刷盘，再加上崩溃恢复这个逻辑，InnoDB 就认为 redo log 在 commit 的时候就不需要 fsync 了，只会 write 到文件系统的 page cache 中就够了。 通常我们说 MySQL 的“双 1”配置，指的就是 sync_binlog 和 innodb_flush_log_at_trx_commit 都设置成 1。也就是说，一个事务完整提交前，需要等待两次刷盘，一次是 redo log（prepare 阶段），一次是 binlog。 3. 组提交机制3.1 LSN日志逻辑序列号（log sequence number，LSN）是单调递增的，用来对应 redo log 的一个个写入点。每次写入长度为 length 的 redo log， LSN 的值就会加上 length。LSN 也会写到 InnoDB 的数据页中，来确保数据页不会被多次执行重复的 redo log。 如图 3 所示，是三个并发事务 (trx1, trx2, trx3) 在 prepare 阶段，都写完 redo log buffer，持久化到磁盘的过程，对应的 LSN 分别是 50、120 和 160。 从图中可以看到 trx1 是第一个到达的，会被选为这组的 leader； 等 trx1 要开始写盘的时候，这个组里面已经有了三个事务，这时候 LSN 也变成了 160； trx1 去写盘的时候，带的就是 LSN=160，因此等 trx1 返回时，所有 LSN 小于等于 160 的 redo log，都已经被持久化到磁盘； 这时候 trx2 和 trx3 就可以直接返回了。 以，一次组提交里面，组员越多，节约磁盘 IOPS 的效果越好。但如果只有单线程压测，那就只能老老实实地一个事务对应一次持久化操作了。在并发更新场景下，第一个事务写完 redo log buffer 以后，接下来这个 fsync 越晚调用，组员可能越多，节约 IOPS 的效果就越好。为了让一次 fsync 带的组员更多，MySQL 有一个很有趣的优化：拖时间。在介绍两阶段提交的时候，我曾经给你画了一个图，现在我把它截过来。 我把“写 binlog”当成一个动作。但实际上，写 binlog 是分成两步的： 先把 binlog 从 binlog cache 中写到磁盘上的 binlog 文件； 调用 fsync 持久化。 为了让组提交的效果更好，把 redo log 做 fsync 的时间拖到了步骤 1 之后。也就是说，上面的图变成了这样： 这样，binlog 也可以组提交了。在执行图 5 中第 4 步把 binlog fsync 到磁盘时，如果有多个事务的 binlog 已经写完了，也是一起持久化的，这样也可以减少 IOPS 的消耗。 不过通常情况下第 3 步执行得会很快，所以 binlog 的 write 和 fsync 间的间隔时间短，导致能集合到一起持久化的 binlog 比较少，因此 binlog 的组提交的效果通常不如 redo log 的效果那么好。 如果你想提升 binlog 组提交的效果，可以通过设置一下两个参数实现: binlog_group_commit_sync_delay 参数，表示延迟多少微秒后才调用 fsync; binlog_group_commit_sync_no_delay_count 参数，表示累积多少次以后才调用 fsync 这两个条件是或的关系，也就是说只要有一个满足条件就会调用 fsync。 现在你就能理解了，WAL 机制主要得益于两个方面： redo log 和 binlog 都是顺序写，磁盘的顺序写比随机写速度要快； 组提交机制，可以大幅度降低磁盘的 IOPS 消耗。 4. MySQL 的 IO 性能优化如果你的 MySQL 现在出现了性能瓶颈，而且瓶颈在 IO 上，可以通过哪些方法来提升性能呢？ 设置 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数，减少 binlog 的写盘次数。这个方法是基于“额外的故意等待”来实现的，因此可能会增加语句的响应时间，但没有丢失数据的风险。 将 sync_binlog 设置为大于 1 的值（比较常见是 100~1000）。这样做的风险是，主机掉电时会丢 binlog 日志。 将 innodb_flush_log_at_trx_commit 设置为 2。这样做的风险是，主机掉电的时候会丢数据。 不建议把 innodb_flush_log_at_trx_commit 设置成 0。因为把这个参数设置成 0，表示 redo log 只保存在内存中，这样的话 MySQL 本身异常重启也会丢数据，风险太大。而 redo log 写到文件系统的 page cache 的速度也是很快的，所以将这个参数设置成 2 跟设置成 0 其实性能差不多，但这样做 MySQL 异常重启时就不会丢数据了，相比之下风险会更小。 5. crash-safe 保证为什么 binlog cache 是每个线程自己维护的，而 redo log buffer 是全局共用的？MySQL 这么设计的主要原因是，binlog 是不能“被打断的”。一个事务的 binlog 必须连续写，因此要整个事务完成后，再一起写到文件里。 事务执行期间，还没到提交阶段，如果发生 crash 的话，redo log 肯定丢了，这会不会导致主备不一致呢？不会。因为这时候 binlog 也还在 binlog cache 里，没发给备库。crash 以后 redo log 和 binlog 都没有了，从业务角度看这个事务也没有提交，所以数据是一致的。binlog 在 binlog cache 不够时也只会写入临时文件中，而不会持久化 binlog file 中。 数据库的 crash-safe 保证的是： 如果客户端收到事务成功的消息，事务就一定持久化了； 如果客户端收到事务失败（比如主键冲突、回滚等）的消息，事务就一定失败了； 如果客户端收到“执行异常”的消息，应用需要重连后通过查询当前状态来继续后续的逻辑。此时数据库只需要保证内部（数据和日志之间，主库和备库之间）一致就可以了。 6. 双非 “1” 配置存在下列场景时，线上生产库会设置成“非双 1”: 业务高峰期。一般如果有预知的高峰期，DBA 会有预案，把主库设置成“非双 1” 备库延迟，为了让备库尽快赶上主库 用备份恢复主库的副本，应用 binlog 的过程，这个跟上一种场景类似 批量导入数据的时候 一般情况下，把生产库改成“非双 1”配置，是设置 innodb_flush_logs_at_trx_commit=2、sync_binlog=1000。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"1 MYSQL 基础架构","slug":"mysql/MySQL实战45讲/01_mysql基础架构","date":"2020-02-29T16:00:00.000Z","updated":"2020-05-24T15:49:15.516Z","comments":true,"path":"2020/03/01/mysql/MySQL实战45讲/01_mysql基础架构/","link":"","permalink":"http://yoursite.com/2020/03/01/mysql/MySQL实战45讲/01_mysql基础架构/","excerpt":"一条 SQL 查询语句是如何执行的，一条更新语句又是如何执行的","text":"一条 SQL 查询语句是如何执行的，一条更新语句又是如何执行的 1.查询语句的执行流程1.1 MySQL 基础架构MySQL 可以分为 Server 层和存储引擎层两部分: Server 包括: 连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能 所有的内置函数（如日期、时间、数学和加密函数等） 所有跨存储引擎的功能，比如存储过程、触发器、视图等都在这一层实现 存储引擎层: 负责数据的存储和提取 1.2 连接器连接器: 负责跟客户端建立连接、获取权限、维持和管理连接 权限获取: 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。 连接管理: 连接分为短连接，长连接，长时间空闲的链接连接称为空闲连接 空闲连接的保持时长由 wait_timeout 参数配置，默认为 8 小时，超时后连接就会自动断开 全部使用长连接后，有些时候 MySQL 占用内存涨得特别快，这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了，解决办法有如下两个: 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 MySQL&gt;=5.7，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 1.3 查询缓存查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。大多数情况下，建议按需启用查询缓存。QUERY_CACHE_TYPE: 是否启用查询缓存,可选值: OFF: 不启用，显示指定 SQL_CACHE 也不会缓存 ON: 启用，可以使用 SQL_NO_CACHE 显示指定不缓存查询结果 DEMAND: 按需启用，即可以使用 SQL_CACHE 显示指定缓存查询结果 MySQL 8.0 版本直接将查询缓存的整块功能删掉了，也就是说 8.0 开始彻底没有这个功能了。 1.4 分析器分析器，包括词法分析和语法分析，语法错误，会在此阶段爆出。 1.5 优化器优化器主要是优化SQL语句的执行: 在表里面有多个索引的时候，决定使用哪个索引 在一个语句有多表关联（join）的时候，决定各个表的连接顺序 1.6 执行器执行器负责执行语句 开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限 (在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限) 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。 慢查询日志中记录有一个 rows_examined 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟 rows_examined 并不是完全相同的。 2.更新语句的执行流程首先，可以确定的说，查询语句的那一套流程，更新语句也是同样会走一遍。与查询流程不一样的是，更新流程还涉及两个重要的日志模块：redo log（重做日志）和 binlog（归档日志）。 2.1 redo log为了提高磁盘的IO效率，避免大规模的随机IO，MySQL 采用一种叫做 WAL 技术(Write-Ahead Logging)它的关键点就是先写日志，再写磁盘: 具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log 里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做 InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示。 write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头 checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。 write pos 和 checkpoint 之间是可用部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，这时候不能再执行新的更新，得停下来将数据更新到数据文件，把 checkpoint 推进一下 有了 redo log，InnoDB 既保证了即使数据库发生异常重启，之前提交的记录都不会丢失(数据已记录到文件，这个能力称为 crash-safe)，也避免了大规模的随机IO带来的效率低下。 2.2 bin logredo log 是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，称为 binlog（归档日志）。binlog 的主要作用有两个: 备份和数据恢复 主从同步的数据复制 bin log 与 redo log redo log 是物理日志，记录的是“在某个数据页上做了什么修改” binlog 是逻辑日志，记录的是这个语句的原始逻辑，用于数据归档。 redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志 2.3 Innodb 更新语句的执行流程1mysql&gt; update T set c=c+1 where ID=2; 我们以上面更新语句为例，来看执行器和 InnoDB 引擎在执行这个简单的 update 语句时的内部流程。 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。 浅色框表示是在 InnoDB 内部执行的 深色框表示是在执行器中执行的。 两阶段提交redo log 的写入拆成了两个步骤：prepare 和 commit，这就是”两阶段提交”。要明白为什么 redo log 需要使用两阶段提交，我们需要明白下面几点: redo log 与 bin log 存在的目的不同: redo log: 为了兼顾crash-safe 和IO效率 bin log: 是为了备份和数据 mysql 要保证已经提交 commit 的事务数据不能丢失，这就包括事务提交后，mysql 服务奔溃数据不丢失，误操作进行数据恢复时或主从同步时数据不丢失，前者是 redo log 保证的，后者是 binlog 保证，因此需要保证redo log 与 bin log 之间数据一致 两阶段提交的目的是为了让两份日志之间的逻辑一致。 redo log/bin log 如何保证奔溃恢复在上面更新语句的执行流程图中，标明了两阶段提交的两个不同时刻，我们来看看在两阶段提交的不同时刻，MySQL 异常重启会出现什么现象。 时刻A: 也就是写入 redo log 处于 prepare 阶段之后、写 binlog 之前，发生了崩溃（crash），由于此时 binlog 还没写，redo log 也还没提交，所以崩溃恢复的时候，这个事务会回滚。这时候，binlog 还没写，所以也不会传到备库。 时刻 B: 也就是 binlog 写完，redo log 还没 commit 前发生 crash。我们先来看一下崩溃恢复时的判断规则: 如果 redo log 里面的事务是完整的，也就是已经有了 commit 标识，则直接提交； 如果 redo log 里面的事务只有完整的 prepare，则判断对应的事务 binlog 是否存在并完整： 如果是，则提交事务； 否则，回滚事务。时刻 B 发生 crash 对应的就是 2(a) 的情况，崩溃恢复过程中事务会被提交。为什么这么设计，这时候 binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。 2.4 一些常见问题bin log 是如何保证完整性的一个事务的 binlog 是有完整格式的： statement 格式的 binlog，最后会有 COMMIT； row 格式的 binlog，最后会有一个 XID event。 在 MySQL 5.6.2 版本以后，还引入了 binlog-checksum 参数，用来验证 binlog 内容的正确性。对于 binlog 日志由于磁盘原因，可能会在日志中间出错的情况，MySQL 可以通过校验 checksum 的结果来发现。 binlog redolog 是如何关联的它们有一个共同的数据字段，叫 XID。崩溃恢复的时候，会按顺序扫描 redo log： 如果碰到既有 prepare、又有 commit 的 redo log，就直接提交； 如果碰到只有 parepare、而没有 commit 的 redo log，就拿着 XID 去 binlog 找对应的事务。 binlog 为什么不能做奔溃恢复其中一个点是binlog 没有能力恢复“数据页”。假如事务提交了，也写入 binlog 了，但是数据在数据页级的丢失。此时，binlog 里面并没有记录数据页的更新细节，是补不回来的。redolog 是可以的，他记录了数据页的更新细节。 在崩溃恢复场景中，InnoDB 如果判断到一个数据页可能在崩溃恢复的时候丢失了更新，就会将它读到内存，然后让 redo log 更新内存内容。更新完成后，内存页变成脏页，就回到了第一种情况的状态。 redo log 设置多大几个 TB 的磁盘的话，直接将 redo log 设置为 4 个文件、每个文件 1GB 吧。 redo log buffer在一个事务的更新过程中，日志是要写多次的，比如多次 insert。插入数据的过程中，生成的日志都得先保存起来，但又不能在还没 commit 的时候就直接写到 redo log 文件里。 所以，redo log buffer 就是一块内存，用来先存 redo 日志的。也就是说，在执行第一个 insert 的时候，数据的内存被修改了，redo log buffer 也写入了日志。但是，真正把日志写到 redo log 文件（文件名是 ib_logfile+ 数字），是在执行 commit 语句的时候做的。 事务执行过程中不会“主动去刷盘”，以减少不必要的 IO 消耗。但是可能会出现“被动写入磁盘”，比如内存不够、其他事务提交等情况。 更新相同的值当 MySQL 去更新一行，但是要修改的值跟原来的值是相同的，这时候 MySQL 会真的去执行一次修改吗？还是看到值相同就直接返回呢？ MySQL 是否会更新，取决于MySQL是否读了数据，“读了数据，就会判断”。 update t set a=1 where id=1: 在这个语句里面，MySQL 认为读出来的值，只有一个确定的 (id=1), 而要写的是 (a=3)，只从这两个信息是看不出来“不需要修改” 因此InnoDB 认真执行了“把这个值修改成 (1,2)”这个操作，该加锁的加锁，该更新的更新。 update t set a=1 where id=1 and a=1: 在这个语句里面，MySQL 读出了 a 的值也做了判断 因此，Innodb 不会执行修改而是直接返回 面我们的验证结果都是在 binlog_format=statement 格式下进行的。如果是 binlog_format=row 并且 binlog_row_image=FULL 的时候，由于 MySQL 需要在 binlog 里面记录所有的字段，所以在读数据的时候就会把所有数据都读出来了。因此 update t set a=1 where id=1 中也会判断出 a=1，而不修改直接返回。 如果表中有 timestamp 字段而且设置了自动更新的话，那么更新“别的字段”的时候，MySQL 会读入所有涉及的字段，这样通过判断，就会发现不需要修改。 3. change buffer除了 redo log，bin log 为了提高IO效率，mysql 还有一个重要的组件 change buffer。要想理解 change buffer 的作用我们要回到在前面 Innodb 的更新流程中来。 当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，需要先从磁盘读入内存，在执行更新操作。但是将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。 在不影响数据一致性的前提下，InnoDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。 虽然名字叫作 change buffer，实际上它是可以持久化的数据。也就是说，change buffer 在内存中有拷贝，也会被写入到磁盘上。将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。 3.1 有 change buffer 的更新过程假设我们要执行下面的插入语句:1mysql&gt; insert into t(id,k) values(id1,k1),(id2,k2); 我们假设当前 k 索引树的状态，查找到位置后，k1 所在的数据页在内存 (InnoDB buffer pool) 中，k2 所在的数据页不在内存中。如图 2 所示是带 change buffer 的更新状态图。 更新语句涉及四个部分，内存、redo log（ib_log_fileX）、 数据表空间（t.ibd）、Innodb 全局表空间（ibdata1）。 t.ibd: 数据表空间存放着原始数据页 ibdata1: change buffer 在内存中有拷贝，也会被写入Innodb 的全局表空间中 如上图所示更新语句做了如下操作: Page 1 在内存中，直接更新内存； Page 2 没有在内存中，就在内存的 change buffer 区域，记录下“我要往 Page 2 插入一行”这个信息 将上述两个动作记入 redo log 中（图中 3 和 4），注意两个动作在 redo log 记录的不同，一个是 To Page，一个是 new change buffer item 做完上面这些，事务就可以完成了。所以，你会看到，执行这条更新语句的成本很低，就是写了两处内存，然后写了一处磁盘（两次操作合在一起写了一次磁盘），而且还是顺序写的。图中的两个虚线箭头，是后台操作，不影响更新的响应时间。 3.2 更新后的读请求我们现在要执行 select * from t where k in (k1, k2) 如果读语句发生在更新语句后不久，内存中的数据都还在，那么此时的这两个读操作就与系统表空间（ibdata1）和 redo log（ib_log_fileX）无关了。 读 Page 1 的时候，直接从内存返回 要读 Page 2 的时候，需要把 Page 2 从磁盘读入内存中，然后应用 change buffer 里面的操作日志，生成一个正确的版本并返回结果。 写 redo log 包含了数据的变更和 change buffer 的变更。 可以看到，直到需要读 Page 2 的时候，这个数据页才会被读入内存。所以，如果要简单地对比这两个机制在提升更新性能上的收益的话，redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。数据读入内存是需要占用 buffer pool 的，所以 changer buffer还能够避免占用内存，提高内存利用率。 3.3 merge 的执行流程上面我们介绍了单条记录 merge 的过程，但是 change buffer merge 的基本单位是磁盘页，merge 的执行流程是这样的： 从磁盘读入数据页到内存（老版本的数据页） 从 change buffer 里找出这个数据页的 change buffer 记录 (可能有多个），依次应用，得到新版数据页； 写 redo log。这个 redo log 包含了数据的变更和 change buffer 的变更。 到这里 merge 过程就结束了。这时候，数据页和内存中 change buffer 对应的磁盘位置都还没有修改，属于脏页，之后各自刷回自己的物理数据，就是另外一个过程了。 3.4 changer buffer 的丢失问题如果某次写入使用了 change buffer 机制，之后主机异常重启，是否会丢失 change buffer 和数据。这个问题的答案是不会丢失，虽然是只更新内存，但是在事务提交的时候，我们把 change buffer 的操作也记录到 redo log 里了，所以崩溃恢复的时候，change buffer 也能找回来。 3.5 changer buffer 适用场景因为 merge 的时候是真正进行数据更新的时刻，而 change buffer 的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做 merge 之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。 因此对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。 反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。所以，对于这种业务模式来说，change buffer 反而起到了副作用。对于此类不适用 change buffer 的场景，应该关闭 changer buffer。 4. 刷脏页通过上面的介绍，我们知道 InnoDB 在处理更新语句的时候，在更新内存写完 redo log 后，就返回给客户端，本次更新成功，数据并没有真正写入磁盘数据页。 当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。 mysql 终究是要把数据写入磁盘数据页，对应的就是把内存里的数据写入磁盘的过程，术语就是 flush。不难想象，平时执行很快的更新操作，其实就是在写内存和日志，而 MySQL 偶尔“抖”一下的那个瞬间，可能就是在刷脏页（flush）。 4.1 什么时候会触发 flush有四种情况会触发 flush 刷脏页: InnoDB 的 redo log 写满了，这时候系统会停止所有更新操作，把 checkpoint 往前推进，redo log 留出空间可以继续写。checkpoint 往前推进，就需要将对应的所有脏页都 flush 到磁盘上 系统内存不足。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。 MySQL 认为系统“空闲”的时候 MySQL 正常关闭的情况，MySQL 会把内存的脏页都 flush 到磁盘上，这样下次 MySQL 启动的时候，就可以直接从磁盘上读数据，启动速度会很快 我们一一来看这四种情况是如何触发 flush 刷脏页的 为什么淘汰内存时要 flush淘汰内存时必须刷脏页了，是因为如果刷脏页一定会写盘，就保证了每个数据页有两种状态： 一种是内存里存在，内存里就肯定是正确的结果，直接返回； 另一种是内存里没有数据，就可以肯定数据文件上是正确的结果，读入内存后返回。这样效率最高 4.2 flush 的性能影响redo log 写满这种情况是 InnoDB 要尽量避免的。因为出现这种情况的时候，整个系统就不能再接受更新了，所有的更新都必须堵住。如果你从监控上看，这时候更新数会跌为 0。 因此 如果 redo log 文件设置的过小，这时候系统不得不停止所有更新，去推进 checkpoint。就会出现磁盘压力很小，但是数据库出现间歇性的性能下跌。，在这种情况下，连 change buffer 的优化也失效了。因为 checkpoint 一直要往前推，这个操作就会触发 merge 操作，然后又进一步地触发刷脏页操作； 内存不够用这种情况其实是常态。InnoDB 用缓冲池（buffer pool）管理内存，缓冲池中的内存页有三种状态：第一种是，还没有使用的；第二种是，使用了并且是干净页；第三种是，使用了并且是脏页。InnoDB 的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。 刷脏页虽然是常态，但是出现以下这两种情况，都是会明显影响性能的： 一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长； 日志写满，更新全部堵住，写性能跌为 0，这种情况对敏感业务来说，是不能接受的。 所以，InnoDB 需要有控制脏页比例的机制，来尽量避免上面的这两种情况。 4.3 InnoDB 刷脏页的控制策略首先，你要正确地告诉 InnoDB 所在主机的 IO 能力，这样 InnoDB 才能知道需要全力刷脏页的时候，可以刷多快。 这就要用到 innodb_io_capacity 这个参数了，它会告诉 InnoDB 你的磁盘能力。这个值我建议你设置成磁盘的 IOPS。磁盘的 IOPS 可以通过 fio 这个工具来测试，下面的语句是我用来测试磁盘随机读写的命令： 1fio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest 因为没能正确地设置 innodb_io_capacity 参数，很可能会出现MySQL 的写入速度很慢，TPS 很低，但是数据库主机的 IO 压力并不大。如果这个是值设置的很低，InnoDB 认为这个系统的能力就这么差，所以刷脏页刷得特别慢，甚至比脏页生成的速度还慢，这样就造成了脏页累积，影响了查询和更新性能。 InnoDB 的刷盘速度就是要参考这两个因素：一个是脏页比例，一个是 redo log 写盘速度。InnoDB 会根据这两个因素先单独算出两个数字。 脏页比例innodb_max_dirty_pages_pct: 脏页比例上限，默认值是 75%。InnoDB 会根据当前的脏页比例（假设为 M），算出一个范围在 0 到 100 之间的数字，计算的伪代码如下: 1234567F1(M)&#123; if M&gt;=innodb_max_dirty_pages_pct then return 100; return 100*M/innodb_max_dirty_pages_pct;&#125; redo logInnoDB 每次写入的日志都有一个序号，当前写入的序号跟 checkpoint 对应的序号之间的差值，我们假设为 N。InnoDB 会根据这个 N 算出一个范围在 0 到 100 之间的数字，这个计算公式可以记为 F2(N) N 越大，算出来的值越大就好了。 刷盘速率根据上述算得的 F1(M) 和 F2(N) 两个值，取其中较大的值记为 R，之后引擎就可以按照 innodb_io_capacity 定义的能力乘以 R% 来控制刷脏页的速度。 4.3 脏页比例监控InnoDB 会在后台刷脏页，而刷脏页的过程是要将内存页写入磁盘。所以，无论是你的查询语句在需要内存的时候可能要求淘汰一个脏页，还是由于刷脏页的逻辑会占用 IO 资源并可能影响到了你的更新语句，都可能是造成你从业务端感知到 MySQL“抖”了一下的原因。要尽量避免这种情况，你就要合理地设置 innodb_io_capacity 的值，并且平时要多关注脏页比例，不要让它经常接近 75%。 脏页比例是通过 Innodb_buffer_pool_pages_dirty/Innodb_buffer_pool_pages_total 得到的，具体的命令参考下面的代码： 123456mysql&gt; select VARIABLE_VALUE into @a from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_dirty';mysql&gt; select VARIABLE_VALUE into @b from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_total';mysql&gt; select @a/@b; 4.4 连带刷页在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉；而且这个把“邻居”拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷。 innodb_flush_neighbors 参数就是用来控制这个行为的，值为 1 的时候会有上述的“连坐”机制，值为 0 时表示不找邻居，自己刷自己的。 找“邻居”这个优化在机械硬盘时代是很有意义的，可以减少很多随机 IO。机械硬盘的随机 IOPS 一般只有几百，相同的逻辑操作减少随机 IO 就意味着系统性能的大幅度提升。 SSD 这类 IOPS 比较高的设备的话，我就建议你把 innodb_flush_neighbors 的值设置成 0。在 MySQL 8.0 中，innodb_flush_neighbors 参数的默认值已经是 0 了 4.5 内存淘汰脏页，对应的 redo log 的操作日志相关参数redo log innodb_flush_log_at_trx_commit: 作用: 事务提交之后多久更新 redo log 建议: 设置为 1 表示每次事务的 redo log 都直接持久化到磁盘。建议设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失 bin log sync_binlog: 作用: 表示事务提交之后多久更新 bin log 建议: 设置为 1 表示每次事务的 binlog 都持久化到磁盘。建议设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失 change buffer innodb_change_buffer_max_size: 作用: change buffer 用的是 buffer pool 里的内存，此参数用于控制 changer buffer 能够占用 buffer pool 最大百分比 示例: =50 表示 change buffer 的大小最多只能占用 buffer pool 的 50%。 刷脏页 innodb_io_capacity: 设置磁盘的 IO 能力 innodb_max_dirty_pages_pct: 脏页比例的上线3.innodb_flush_neighbors: 刷脏页时是否刷新邻居脏页","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"}]},{"title":"1. k8s集群安装与配置","slug":"K8S/k8s初始化与k8s集群","date":"2019-11-14T16:00:00.000Z","updated":"2020-05-24T15:39:38.257Z","comments":true,"path":"2019/11/15/K8S/k8s初始化与k8s集群/","link":"","permalink":"http://yoursite.com/2019/11/15/K8S/k8s初始化与k8s集群/","excerpt":"k8s集群安装与配置","text":"k8s集群安装与配置 1. k8s 安装1.1 准备 yum 源12345678910111213141516cd /etc/yum.repo.d/# docker-ce 源wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# kubernetes 源vim kubernetes.repo[kuberneters]name=kuberneters repobaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/gpgcheck=1enabled=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg# 设置 docker kubelet 开机自启动systemctl enable docker kubelet 1.2 安装配置相关组件1234567891011121314151617181920# 安装相关组件yum install docker-ce kubectl kubelet kubeadm# 配置 docer 的 unit file 添加 https 代理，以便能下载相关被墙的镜像# 不过依旧不能用，此步骤省略# vim /usr/lib/systemd/system/docker.service # 添加# Environment=\"HTTPS_PROXY=http://www.ik8s.io:10080\"systemctl daemon-reloadsystemctl restart dockerdocker info # 看到 HTTPS_PROXY 行即可# 配置 kuberneters 不受 swap 分区的影响vim /etc/sysconfig/kubeletKUBELET_EXTRA_ARGS=\"--fail-swap-on=false\"# 系统参数初始化sysctl -w net.bridge.bridge-nf-call-ip6tables=1sysctl -w net.bridge.bridge-nf-call-iptables=1iptables -F 1.3 准备 kubeadm 所需镜像因为某种不可描述的原因，kubeadm 使用到的镜像无法访问，因此需要手动准备 kubeadm 所需的镜像文件。这里有片文章可以指导你去构建相应的 镜像 https://ieevee.com/tech/2017/04/07/k8s-mirror.html 12345678&gt; kubeadm config images listk8s.gcr.io/kube-apiserver:v1.12.2k8s.gcr.io/kube-controller-manager:v1.12.2k8s.gcr.io/kube-scheduler:v1.12.2k8s.gcr.io/kube-proxy:v1.12.2k8s.gcr.io/pause:3.1k8s.gcr.io/etcd:3.2.24k8s.gcr.io/coredns:1.2.2 我是自己去阿里云自建的镜像，使用下面的脚本对镜像进行重命名12345678910#!/bin/bashbase=k8s.gcr.ioaliyun=\"registry.cn-qingdao.aliyuncs.com/htttao\"images=(kube-apiserver:v1.12.2 kube-controller-manager:v1.12.2 kube-scheduler:v1.12.2 kube-proxy:v1.12.2 pause:3.1 etcd:3.2.24 coredns:1.2.2)for i in $&#123;images[@]&#125;do docker pull $aliyun/$i docker tag $aliyun/$i $base/$idone 1.4 初始化 Master 节点12345678910111213141516kubeadm init --kubernetes-version=v1.12.2 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swap# 运行完成之后，会提示将 Node 节点加入集群的命令kubeadm join 192.168.1.106:6443 --token z5fqxu.dn3awhi0u5n2i6eb --discovery-token-ca-cert-hash sha256:dc333a8af6ee0c7cd1e180b43251800685b90d6338929fa508e42f76579ce50c# 按照初始化后的提示，创建一个普通用户，并复制相应文件# user: kubernetesmkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# 测试kubectl get cskubectl get nodeskubectl get podskubectl get ns 1.5 部署网络组件初始化 Master 还有非常重要的一步，就是部署网络组件，否则各个 pod 等组件之间是无法通信的123kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.ymlkubectl get pods 1.6 k8s 集群重至如果配置过程中出现了错误，想重新配置集群， 可以使用 kubeadm reset 对整个集群进行重至，然后重新使用 kubeadm init 进行初始化创建。但是需要注意的时，kubeadm reset 不会重至 flannel 网络，想要完全重至可使用以下脚本 12345678910111213#!/bin/bashkubeadm resetsystemctl stop kubeletsystemctl stop dockerrm -rf /var/lib/cni/rm -rf /var/lib/kubelet/*rm -rf /etc/cni/ifconfig cni0 downifconfig flannel.1 downifconfig docker0 downip link delete cni0ip link delete flannel.1systemctl start docker 2. 安装脚本整个集群安装比较复杂，因此我将上述过程写成了两个脚本。因此按次序执行下面脚本然后进行 kubeadm init 进行集群初始化即可完成配置。 2.1 基础环境配置脚本1234567891011121314151617181920212223242526272829#!/bin/bash# 1. 设置系统参数mount /dev/cdrom /cdromiptables -F# 2. 准备 yum 源wget -P /etc/yum.repos.d/ https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repocat &lt;&lt; EOF &gt;&gt; /etc/yum.repos.d/kubernetes.repo[kuberneters]name=kuberneters repobaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/gpgcheck=1enabled=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF# 3. 配置 kuberneters 不受 swap 分区的影响yum install docker-ce kubelet kubeadm kubectl -yecho 'KUBELET_EXTRA_ARGS=\"--fail-swap-on=false\"' &gt; /etc/sysconfig/kubelet# 4. 启动相关服务systemctl start dockersystemctl enable docker kubeletcat &lt;&lt; EOF &gt; /etc/docker/daemon.json&#123; \"registry-mirrors\": [\"https://osafqkzd.mirror.aliyuncs.com\"]&#125;EOF 2.2 镜像下载脚本执行下面的下载脚本 /root/kubernetes.sh123456789101112131415161718#!/bin/bashsudo docker login --username=1556824234@qq.com registry.cn-qingdao.aliyuncs.comsysctl net.bridge.bridge-nf-call-ip6tables=1sysctl net.bridge.bridge-nf-call-iptables=1base=k8s.gcr.ioaliyun=\"registry.cn-qingdao.aliyuncs.com/htttao\"images=(kube-apiserver:v1.12.2 kube-controller-manager:v1.12.2 kube-scheduler:v1.12.2 kube-proxy:v1.12.2 pause:3.1 etcd:3.2.24 coredns:1.2.2)for i in $&#123;images[@]&#125;do docker pull $aliyun/$i docker tag $aliyun/$i $base/$idoneflannel=flannel:v0.10.0-amd64docker pull $aliyun/$flanneldocker tag $aliyun/$flannel quay.io/coreos/$flannel 2.3 集群初始化1234567891011kubeadm init --kubernetes-version=v1.12.2 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swapmkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# Node 节点的加入集群的命令kubeadm join 192.168.1.184:6443 --token w1b9i6.ryqstfgjmob2z8xp --discovery-token-ca-cert-hash sha256:0d3404f3919116e7efa56b2e0694c1397cd44915ed13f16d9e6e7600ada64c4c --ignore-preflight-errors=Swapkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml 3. Node 节点配置Node 节点的配置与 Master 过程类似，以此执行上述两个脚本即可，唯一的区别是在初始化时执行的是 kubeadm join. 1234# 1. 基础环境配置脚本# 2. 执行镜像下载脚本，准备好相关镜像# 3. 将节点加入集群, 需要注意节点的主机名不能与 Master 节点同名kubeadm join 192.168.1.184:6443 --token w1b9i6.ryqstfgjmob2z8xp --discovery-token-ca-cert-hash sha256:0d3404f3919116e7efa56b2e0694c1397cd44915ed13f16d9e6e7600ada64c4c --ignore-preflight-errors=Swap","categories":[{"name":"运维","slug":"运维","permalink":"http://yoursite.com/categories/运维/"}],"tags":[{"name":"K8S","slug":"K8S","permalink":"http://yoursite.com/tags/K8S/"}]},{"title":"附录 专业术语","slug":"db/db_100","date":"2019-10-29T16:00:00.000Z","updated":"2020-05-24T15:34:18.963Z","comments":true,"path":"2019/10/30/db/db_100/","link":"","permalink":"http://yoursite.com/2019/10/30/db/db_100/","excerpt":"数据系统中的专用术语","text":"数据系统中的专用术语 1. 第一章扇出扇出：从电子工程学中借用的术语，它描述了输入连接到另一个门输出的逻辑门数量。输出需要提供足够的电流来驱动所有连接的输入。 在事务处理系统中，我们使用它来描述为了服务一个传入请求而需要执行其他服务的请求数量。 SLO和SLA服务级别目标（SLO, service level objectives）和服务级别协议（SLA,service level agreements），即定义服务预期性能和可用性的合同。百分位点是通常的衡量指标，比如SLA可能会声明，如果服务响应时间的中位数小于200毫秒，且99.9百分位点低于1秒，则认为服务工作正常（如果响应时间更长，就认为服务不达标）。这些指标为客户设定了期望值，并允许客户在SLA未达标的情况下要求退款。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/categories/分布式/"}],"tags":[{"name":"数据密集型应用","slug":"数据密集型应用","permalink":"http://yoursite.com/tags/数据密集型应用/"}]},{"title":"7 Mariadb 读写分离","slug":"mysql/马哥_MySQL/7_读写分离","date":"2019-10-11T16:00:00.000Z","updated":"2020-05-24T15:48:33.186Z","comments":true,"path":"2019/10/12/mysql/马哥_MySQL/7_读写分离/","link":"","permalink":"http://yoursite.com/2019/10/12/mysql/马哥_MySQL/7_读写分离/","excerpt":"Mariadb 读写分离","text":"Mariadb 读写分离 1. 读写分离器简介 作用: 由于写操作只能发往主节点 通常读操作都发往从节点 可用软件: - 复制集群: 1. mysql-proxy (C++) --&gt; Qihoo (atlas) 2. amoeba (java) --&gt; Cobar 3. dbrelay 4. maxscale 5. ProxySQL 6. mycat 7. AliSQL 分区集群: 垂直切分-按表进行切分 水平切分-按表中的行进行的切分 cobar gizzard 双主模型: 是不需要读写分离的，只需要负载均衡 Mariadb Cluster PXC: Percona XtraDB Cluster 2. ProxySQL3. MHAMairadb 主从复制，需要读写分离，还需要 MHA 做高可用，配置起来较为麻烦。建议使用 PXC 或 AliSQL 做多主集群，这样只需要使用 Nginx 进行负载均衡即可。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"马哥 MySQL 运维","slug":"马哥-MySQL-运维","permalink":"http://yoursite.com/tags/马哥-MySQL-运维/"}]},{"title":"6 Mariadb 复制","slug":"mysql/马哥_MySQL/6_主从复制","date":"2019-10-10T16:00:00.000Z","updated":"2020-05-24T15:48:22.686Z","comments":true,"path":"2019/10/11/mysql/马哥_MySQL/6_主从复制/","link":"","permalink":"http://yoursite.com/2019/10/11/mysql/马哥_MySQL/6_主从复制/","excerpt":"Mariadb 主从复制","text":"Mariadb 主从复制 1.复制的基本原理复制简介: 复制功用： 负载均衡：读操作，适用于读密集型的应用 备份 高可用和故障切换 MySQL升级测试 主从复制系统架构： 从服务器： I/O线程：从master请求二进制日志信息，并保存至中继日志； SQL线程：从relay log中读取日志信息，在本地完成重放； 异步模式：async: 从服务器落后于主服务器，会出现主从数据不一致； 2.主从复制配置过程：主从复制的配置过程: 配置时间同步 复制的起始位置: 如果主服务器数据较小，且二进制日志完整，复制的起点可以从 0 位置开始 如果主服务器数据很多，或二进制日志不完成，应先以 xtrabackup 备份恢复的方式启动从节点，复制的起点为备份操作时主节点所处的日志文件及事件位置 master 启用二进制日志； 设置一个在当前集群中惟一的server-id； 创建一个有复制权限(REPLICATION SLAVE, REPLICATION CLIENT)账号； slave 启用中继日志； 设置一个在当前集群中惟一的server-id； 使用有复制权限用户账号连接至主服务器，并启动复制线程； 2.1 主从配置示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 关闭 iptables# 1. 主节点配置&gt; yum intall mariadb-server&gt; vim /etc/my.cnf.d/server.cnf [server] datadir = /data innodb_file_per_table = ON skip_name_resolve = ON server_id=1 log_bin = master-log slow_query_log = ON&gt; mkdir /data /backup&gt; chown -R mysql:mysql /data /backup&gt; chcon /data/ /backup/ -R --reference /var/lib/mysql/&gt; mysql_install_db --user=mysql --datadir=/data&gt; mysql_secure_installation&gt; systemctl start mariadb# 配置同步账号MariaDB [(none)]&gt; GRANT REPLICATION CLIENT,REPLICATION SLAVE ON *.* TO &quot;repluser&quot;@&quot;%&quot; IDENTIFIED BY &quot;replpass&quot;;Query OK, 0 rows affected (0.019 sec)MariaDB [(none)]&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.019 sec)# 2. 从节点配置&gt; yum intall mariadb-server&gt; vim /etc/my.cnf.d/server.cnf [server] datadir = /data innodb_file_per_table = ON skip_name_resolve = ON read_only = ON server_id = 2 log_bin = slave-log relay_log = relay-log slow_query_log = ON log_error = error-log&gt; mkdir /data /backup&gt; chown -R mysql:mysql /data /backup&gt; chcon /data/ /backup/ -R --reference /var/lib/mysql/&gt; mysql_install_db --user=mysql --datadir=/data&gt; mysql_secure_installation&gt; systemctl start mariadb&gt; mysql -uroot -p MariaDB [(none)]&gt; CHANGE MASTER TO MASTER_HOST=&quot;192.168.1.18&quot;, MASTER_USER=&quot;repluser&quot;, MASTER_PASSWORD=&quot;replpass&quot;,MASTER_PORT=3306,MASTER_LOG_FILE=&quot;master-log.000004&quot;,MASTER_LOG_POS=1222;Query OK, 0 rows affected (0.018 sec)MariaDB [(none)]&gt; SHOW SLAVE STATUS;MariaDB [(none)]&gt; START SLAVE;Query OK, 0 rows affected (0.004 sec) 2.2 CHANGE MASTER 命令使用CHANGE MASTER TO option [, option] … 作用: slave 连接至主服务器 option: MASTER_HOST = ‘host_name’ 主服务器地址 MASTER_USER = ‘user_name’ 有复制权限的用户名 MASTER_PASSWORD = ‘password’ 用户密码 MASTER_PORT = port_num 主服务器的端口 MASTER_CONNECT_RETRY = interval 连接重试时间间隔 MASTER_HEARTBEAT_PERIOD = interval 心跳检测时间间隔 MASTER_LOG_FILE = ‘master_log_name’ 主服务器二进制日志文件 MASTER_LOG_POS = master_log_pos 二进制日志文件中的位置 2.3 主从复制的注意事项 复制如何开始: 主节点运行很长时间，且已经有一定规模的数据，如何启动复制？ 在主节点做一个完全备份，并记录二进制日志文件及位置； 在从节点恢复此完全备份，并在启动复制时从记录的二进制日志文件和位置开始； 从服务器限制只读: 在从服务器启动read_only；但仅对非具有SUPER权限的用户有效； 阻止所有用户 ：MariaDB&gt; FLUSH TABLES WITH READ LOCK; 保证主从复制时的事务安全 在master节点启用参数： sync_binlog = on: 事务提交时，同步二进制日志 sync_master_info = 1 如果用到的为InnoDB存储引擎，应该启用以下参数： innodb_flush_logs_at_trx_commit: 事务提交时，同步事务日志到文件中 innodb_support_xa=on: 启用分布式事务 slave 节点启用参数： skip_slave_start: 从服务器意外终止时，尽量避免自动重启复制线程，以防止从服务器未执行完事务导致的数据不一致 sync_master_info = 1: 同步 master-info 日志文件 sync_relay_log_info = 1: 同步 relay-info 日志文件 sync_relay_log = 1 12345# 从节点 ll /data/*.info-rw-rw----. /data/master.info # 记录了主节点当前同步的位置-rw-rw----. /data/multi-master.info-rw-rw----. /data/relay-log.info # 记录了relay-log 与 master-log 二进制文件事件位置的对应关系 跟复制功能相关的文件： master.info：用于保存slave连接至master时的相关信息； relay-log.info：保存了当前slave节点上已经复制的当前二进制日志和本地relay log日志对应关系； 2.4 半同步复制1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# 1. 主节点：# 查看 Mairadb 所有插件MariaDB [(none)]&gt; SHOW PLUGINS;MariaDB [(none)]&gt; INSTALL PLUGIN rpl_semi_sync_master SONAME 'semisync_master.so';Query OK, 0 rows affected (0.05 sec) MariaDB [(none)]&gt; SHOW GLOBAL VARIABLES LIKE '%semi%';+------------------------------------+-------+| Variable_name | Value |+------------------------------------+-------+| rpl_semi_sync_master_enabled | OFF || rpl_semi_sync_master_timeout | 10000 | # 毫秒 10s，等待同步节点的超时时长| rpl_semi_sync_master_trace_level | 32 || rpl_semi_sync_master_wait_no_slave | ON | # 在没有同步的从节点时，是否等待 +------------------------------------+-------+4 rows in set (0.00 sec)MariaDB [(none)]&gt; SET GLOBAL rpl_semi_sync_master_enabled=1;Query OK, 0 rows affected (0.00 sec)MariaDB [(none)]&gt; SET GLOBAL rpl_semi_sync_master_timeout=2000;Query OK, 0 rows affected (0.00 sec) # 2. 从节点：MariaDB [(none)]&gt; INSTALL PLUGIN rpl_semi_sync_slave SONAME 'semisync_slave.so';Query OK, 0 rows affected (0.05 sec)MariaDB [(none)]&gt; SHOW GLOBAL VARIABLES LIKE '%semi%';+---------------------------------+-------+| Variable_name | Value |+---------------------------------+-------+| rpl_semi_sync_slave_enabled | OFF || rpl_semi_sync_slave_trace_level | 32 |+---------------------------------+-------+2 rows in set (0.00 sec)MariaDB [(none)]&gt; SET GLOBAL rpl_semi_sync_slave_enabled=1;Query OK, 0 rows affected (0.00 sec)# 重启复制线程，以使得半同步生效MariaDB [(none)]&gt; STOP SLAVE IO_THREAD;Query OK, 0 rows affected (0.013 sec)MariaDB [(none)]&gt; START SLAVE IO_THREAD;Query OK, 0 rows affected (0.002 sec)# 3.半同步状态查看 -- 主节点MariaDB [(none)]&gt; SHOW GLOBAL STATUS LIKE \"rpl%\";+--------------------------------------------+-------------+| Variable_name | Value |+--------------------------------------------+-------------+| Rpl_semi_sync_master_clients | 0 | # 作为同步复制的从节点个数| Rpl_semi_sync_master_get_ack | 0 || Rpl_semi_sync_master_net_avg_wait_time | 0 || Rpl_semi_sync_master_net_wait_time | 0 || Rpl_semi_sync_master_net_waits | 0 || Rpl_semi_sync_master_no_times | 0 || Rpl_semi_sync_master_no_tx | 0 || Rpl_semi_sync_master_request_ack | 0 || Rpl_semi_sync_master_status | OFF || Rpl_semi_sync_master_timefunc_failures | 0 || Rpl_semi_sync_master_tx_avg_wait_time | 0 || Rpl_semi_sync_master_tx_wait_time | 0 || Rpl_semi_sync_master_tx_waits | 0 || Rpl_semi_sync_master_wait_pos_backtraverse | 0 || Rpl_semi_sync_master_wait_sessions | 0 || Rpl_semi_sync_master_yes_tx | 0 || Rpl_semi_sync_slave_send_ack | 0 || Rpl_semi_sync_slave_status | OFF || Rpl_status | AUTH_MASTER || Rpl_transactions_multi_engine | 0 |+--------------------------------------------+-------------+ 3. 双主模型互为主从的潜在问题: 数据不一致； 自动增长id 定义一个节点使用奇数id auto_increment_offset=1 auto_increment_increment=2 定义另一个节点使用偶数id auto_increment_offset=2 auto_increment_increment=2 配置要点: 各自使用不同的server id 都启用binlog和relay log 定义自动增长的id字段的增长方式 都授权有复制权限的用户账号 各自把对方指定为主服务器 12345678910111213141516171819202122232425262728293031# 1. 主节点配置&gt; vim /etc/my.cnf.d/server.cnf [server] datadir = /data innodb_file_per_table = ON skip_name_resolve = ON server_id=1 log_bin = master-log relay_log = relay-log slow_query_log = ON log_error = error-log auto_increment_offset=1 auto_increment_increment=2# 2. 从节点配置&gt; vim /etc/my.cnf.d/server.cnf [server] datadir = /data innodb_file_per_table = ON skip_name_resolve = ON server_id = 2 log_bin = slave-log relay_log = relay-log slow_query_log = ON log_error = error-log auto_increment_offset=2 auto_increment_increment=2 4. 复制过滤器让slave仅复制有限的几个数据库，而非所有，有两种实现思路： 主服务器仅向二进制日志中记录有特定数据库相关的写操作； 问题：时间点还原将无法全面实现；不建议 配置: binlog_do_db=: 数据库白名单，只允许指定库的写操作记录到二进制日志中 binlog_ignore_db=: 数据库黑名单 从服务器的SQL_THREAD仅在中断日志中读取特定数据相关的语句并应用在本地； 问题：会造成网络带宽和磁盘IO的浪费； 配置: Replicate_Do_DB=: 数据库白名单 Replicate_Ignore_DB= Replicate_Do_Table=: 表级别白名单 Replicate_Ignore_Table= Replicate_Wild_Do_Table=: 通配符匹配表的白名单 Replicate_Wild_Ignore_Table=1234567891011121314151617181920212223242526272829MariaDB [(none)]&gt; STOP SLAVE;Query OK, 0 rows affected (0.014 sec)MariaDB [(none)]&gt; SET @@GLOBAL.replicate_do_db=mydb;Query OK, 0 rows affected (0.000 sec)MariaDB [(none)]&gt; START SLAVE;Query OK, 0 rows affected (0.002 sec)MariaDB [(none)]&gt; SHOW SLAVE STATUS \\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.1.18 Master_User: repluser Master_Port: 3306 Connect_Retry: 60 Master_Log_File: master-log.000006 Read_Master_Log_Pos: 472 Relay_Log_File: relay-log.000007 Relay_Log_Pos: 556 Relay_Master_Log_File: master-log.000006 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: mydb Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: 6. 复制的监控和维护6.1 二进制日志清理PURGE {BINARY|MASTER} LOGS TO &quot;log_name&quot; 作用: 清理二进制日志文件，会自动更新二进制的 .index 文件 1234567891011121314151617181920212223242526272829# 备份二进制文件&gt; cp /data/slave-log* /backup# 删除二进制文件MariaDB [(none)]&gt; SHOW MASTER LOGS;+------------------+-----------+| Log_name | File_size |+------------------+-----------+| slave-log.000001 | 351 || slave-log.000002 | 28636 || slave-log.000003 | 1705 || slave-log.000004 | 365 || slave-log.000005 | 342 |+------------------+-----------+5 rows in set (0.000 sec)MariaDB [(none)]&gt; PURGE MASTER LOGS TO \"slave-log.000005\";Query OK, 0 rows affected (0.010 sec)MariaDB [(none)]&gt; SHOW MASTER LOGS;+------------------+-----------+| Log_name | File_size |+------------------+-----------+| slave-log.000005 | 342 |+------------------+-----------+1 row in set (0.000 sec)&gt; sudo cat slave-log.index./slave-log.000005 6.2 复制监控 SHOW MASTER STATUS SHOW BINLOG EVENTS SHOW BINARY LOGS SHOW SLAVE STATUS: 判断slave是否落后于master: Seconds_Behind_Master: 0 6.3 主从节点数据是否一致 检查方法: 通过表自身的CHECKSUM检查 使用percona-tools中pt-table-checksum 数据不一致的修复方法：重新复制","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"马哥 MySQL 运维","slug":"马哥-MySQL-运维","permalink":"http://yoursite.com/tags/马哥-MySQL-运维/"}]},{"title":"5 MYSQL 备份与恢复","slug":"mysql/马哥_MySQL/5_备份与恢复","date":"2019-10-09T16:00:00.000Z","updated":"2020-05-24T15:48:15.872Z","comments":true,"path":"2019/10/10/mysql/马哥_MySQL/5_备份与恢复/","link":"","permalink":"http://yoursite.com/2019/10/10/mysql/马哥_MySQL/5_备份与恢复/","excerpt":"Mariadb 备份与恢复","text":"Mariadb 备份与恢复 1. 备份的简介 备份类型: 热备份、温备份和冷备份： 热备份：在线备份，读写操作不受影响； 温备份：在线备份，读操作可继续进行，但写操作不允许； 冷备份：离线备份，数据库服务器离线，备份期间不能为业务提供读写服务； 物理备份和逻辑备份： 物理备份：直接复制数据文件进行的备份； 逻辑备份：从数据库中“导出”数据另存而进行的备份，与存储引擎无关 规则备份时需要考虑的因素： 持锁的时长 备份过程时长 备份负载 恢复过程时长 备份什么: 数据、额外的数据（二进制日志和InnoDB的事务日志） 代码（存储过程和存储函数、触发器、事件调度器等）、服务器配置文件 设计备份方案: 全量备份+增量备份，binlog 全量备份+差异备份，binlog 2.备份工具备份工具: mysqldump: 逻辑备份工具，适用于所有存储引擎，温备；完全备份，部分备份，不支持差异和增量备份 对InnoDB存储引擎支持热备 MyISAM 温备 cp, tar等文件系统工具: 物理备份工具，适用于所有存储引擎；冷备；完全备份，部分备份； lvm2的快照：请求一个全局锁，之后立即释放，几乎热备 mysqlhotcopy: 几乎冷备；仅适用于MyISAM存储引擎； xtrabackup: Innodb 热备的物理备份工具，支持全量，增量和差异备份 备份方案工具选择: mysqldump+binlog: mysqldump 完全备份，通过备份二进制日志实现增量备份； lvm2快照+binlog：几乎热备，物理备份 xtrabackup + binlog: 对InnoDB：热备，支持完全备份和增量备份 对MyISAM引擎：温备，只支持完全备份 2.1 mysqldumpmysqldump: 格式: mysqldump [options] database [tables]: 单库，多表备份 mysqldump --databases [options] DB1 [DB2....]: 单库，多库备份 mysqldump --all-databases [options]: 备份所有库 作用: mysql 客户端，通过mysql协议连接至mysqld，支持逻辑，完全，部分备份 生成: Schema和数据存储一起保存为巨大的SQL语句、单个巨大的备份文件 二次封装工具: mydumper, phpMyAdmin 参数: 温备: 支持MyISAM INNODB，MyISAM 必须显示指定 -x, –lock-all-tables：锁定所有表 -l, –lock-tables：锁定备份的表 热备: 支持 INNODB –single-transaction：启动一个大的单一事务实现备份 选库: -A, –all-databases -B, –databases db_name1 db_name2 …：备份指定的数据库 -C, –compress：压缩传输； 其他: -E, –events：备份指定库的事件调度器； -R, –routines：备份存储过程和存储函数； –triggers：备份触发器 --master-data[=#]: 记录备份开始时刻，二进制文件所处的文件和位置，可选值为 =1：记录CHANGE MASTER TO语句，此语句未被注释； =2：记录CHANGE MASTER TO语句，为注释语句，CHANGE MASTER TO 只对从服务有效，通常应该注释掉 –flush-logs, -F：锁定表之后执行flush logs命令，这样二进制日志就会滚动到新的文件，在利用二进制日志进行回滚时就不用进行日志截取了 1234567891011121314# mysqsldump + binlog 做备份的示例# 1. mysqldump 全量备份mysqldump -uroot -p --single-transaction -R -E --triggers --master-data=2 --flush-logs --databases tsong &gt; /home/tao/tsong-fullback-$(date +%F).sql# 2. 将 binlog 生成 sql 语句, 对应的二进制文件已经记录在 mysqldump 内的CHANGE MASTER TO 语句内sudo mysqlbinlog /data/master-log.000004 &gt; binlog.sql# 3. 启动新的 mairadb 服务器，执行上述两个 sql 脚本mysql&gt; SET SESSION sql_log_bin=0; # 避免重放的 sql 语句记录到新的二进制文件中mysql&gt; SOURCE /path/from/somefile.sql; # 重放上述两个 sql 脚本mysql&gt; SET SESSION sql_log_bin=1;# 4. 对恢复的数据库重新做一次全量备份mysqldump -uroot -p --single-transaction -R -E --triggers --master-data=2 --flush-logs --databases tsong &gt; /home/tao/tsong-fullback-$(date +%F).sql 2.2 Xtrabackup在MariaDB10.3.x及以上版本的redo日志格式发生了更改，因此已经无法使用 Xtrabackup，需要使用 Mariadb 提供的 mariabackup，两个命令使用的方式类似，我们将主要介绍 mariabackup 的使用12345&gt; yum install MariaDB-backup&gt; rpm -ql MariaDB-backup/usr/bin/mariabackup # 等同于 xtrabackup /usr/bin/mariadb-backup/usr/bin/mbstream Mariabackup 恢复过程1完全备份 --&gt; 增量备份1 ---&gt; 增量备份2 如上所示，在第一次完整备份之后，mariabackup 增加了两次增量备份。在进行恢复时，mariabackup 并不是将完全备份，增量备份1，增量备份2 依次拿到 mairadb 上进行重放，而是先将增量备份1 合并到完全备份，再将增量备份2 合并到完全备份，最后使用完成完整备份进行数据恢复。因此 mariabackup 有以下几个与阶段参数: –prepare: 将增量备份合并到完全备份，合并前完全备份也要执行 –prepare –aply-log-only: 对Innodb 事务日志中，已提交日志(redo log)进行合并 1234567891011# 第一步：准备全备数据mariabackup --prepare --target-dir /backup/fullbackup/ --user root --password centos --apply-log-only# 第二步：将增量备份与全备合并mariabackup --prepare --target-dir /backup/fullbackup/ \\ --user root --password centos \\ --incremental-dir /backup/inc1 --apply-log-only第三步：还原(保证data目录为空)mariabackup --copy-back --target-dir /backup/fullbackup/ \\ --user root --password centos 2.3 备份注意事项:备份注意事项 将数据和二进制文件放置于不同的设备；二进制日志也应该周期性地备份； 将数据和备份分开存放，建议不在同一设备、同一主机、同一机房、同一地域； 每次灾难恢复后都应该立即做一次完全备份； 备份后的数据应该周期性地做还原测试； 从备份中恢复应该遵循的步骤： 停止MySQL服务器； 记录服务器配置和文件权限； 将备份恢复到MySQL数据目录；此步骤依赖具体的备份工具； 改变配置和文件权限； 以限制方式启动MySQL服务器：比如通过网络访问；skip-networking socket=/tmp/mysql-recovery.sock 载入额外的逻辑备份；而检查和重放二进制日志； 检查已经还原的数据； 以完全访问模式重启服务器；","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"马哥 MySQL 运维","slug":"马哥-MySQL-运维","permalink":"http://yoursite.com/tags/马哥-MySQL-运维/"}]},{"title":"4 MYSQL 日志","slug":"mysql/马哥_MySQL/4_日志","date":"2019-10-08T16:00:00.000Z","updated":"2020-05-24T15:48:06.762Z","comments":true,"path":"2019/10/09/mysql/马哥_MySQL/4_日志/","link":"","permalink":"http://yoursite.com/2019/10/09/mysql/马哥_MySQL/4_日志/","excerpt":"Mariadb 日志","text":"Mariadb 日志 1. Mariadb 日志Mariadb 日志类别: 查询日志： 慢查询日志：查询执行时长超过指定时长的查询操作所记录日志 错误日志： Mariadb 错误信息 主从复制，从服务复制线程的启动和关闭信息 事件调度器被调度执行的信息 二进制日志：binlog，记录能改变或能潜在改变 Mariadb 数据的 SQL 语句 中继日志：relay_log，从服务器从主服务器通过过来的二进制日志 事务日志：Innodb 事务日志包括 REDO LOG, UNDO LOG 1.1 查询日志 作用: 记录查询语句，一般不用启用 配置: general_log = {ON|OFF}: 是否启用查询日志 log_output = {TABLE|FILE|NONE}: 查询语句的输出位置 TABLE: 输出到表 mysql.general_log 中 FILE: 输出到 mariadb 存储目录下的文件中，文件名由 general_log_file 参数指定 None: 不输出，即不开启查询日志 general_log_file = HOSTNAME.log: 当log_output=FILE类型时，日志信息的记录位置，文件名默认为主机名 1.2 慢查询日志： 作用: 查询执行时长超过指定时长的查询操作所记录日志 位置: 受 log_output 参数的影响 =FILE: 保存位置由 slow_query_log_file 参数指定 =TABLE: 保存在表 mysql.slow_log 中 配置: long_query_time: 慢查询判定的时间界限 slow_query_log = {ON|OFF}：是否启用慢查询日志 slow_query_log_file: 慢查询日志文件路径 log_slow_filter: 设置慢查询记录的语句类型，可选值如下，多个值由逗号隔开 admin,filesort,filesort_on_disk full_join,full_scan query_cache,query_cache_miss tmp_table,tmp_table_on_disk log_slow_rate_limit =: 慢日志记录的速率 log_slow_verbosity =: 123456789101112131415161718192021222324252627SELECT @@GLOBAL.long_query_time;+--------------------------+| @@GLOBAL.long_query_time |+--------------------------+| 10.000000 |+--------------------------+show global variables like \"slow%\";+---------------------+--------------------+| Variable_name | Value |+---------------------+--------------------+| slow_launch_time | 2 || slow_query_log | OFF || slow_query_log_file | localhost-slow.log |+---------------------+--------------------+ show global variables like \"log_slow_%\";+------------------------------+--------------------------------------------------------------------------------------------------------------------------------------+| Variable_name | Value |+------------------------------+--------------------------------------------------------------------------------------------------------------------------------------+| log_slow_admin_statements | ON || log_slow_disabled_statements | sp || log_slow_filter | admin,filesort,filesort_on_disk,filesort_priority_queue,full_join,full_scan,query_cache,query_cache_miss,tmp_table,tmp_table_on_disk || log_slow_rate_limit | 1 || log_slow_slave_statements | ON || log_slow_verbosity | |+------------------------------+--------------------------------------------------------------------------------------------------------------------------------------+ 1.3 错误日志 内容: mysqld启动和关闭过程中输出的信息； mysqld运行中产生的错误信息； event scheduler运行一个event时产生的日志信息； 在主从复制架构中的从服务器上启动从服务器线程时产生的日志信息； 配置: log_error: OFF|/path/to/somefile，不启用或者记录到指定文件中 log_warnings = {ON|OFF}：是否记录警告信息于错误日志中； 2. 二进制日志2.1 二进制日志配置 作用: 记录能改变或能潜在改变 Mariadb 数据的 SQL 语句 文件的构成： 日志文件：文件名前缀.XXXXX 索引文件：文件名前缀.index，记录当前所有的二进制日志文件名 EVENT: 二进制文件里每个 SQL 语句的记录都称为一个事件 EVENT 配置: binlog_format: STATEMENT: 基于“语句”记录； ROW: 基于“行”记录 MIXED: statement 与 row 的混合模式，由 mariadb 决定采用何种格式 log_bin: OFF|log_path 不启用或者二进制日志的保存路径，只能在服务启动前配置 sql_log_bin = ON: 当前会话产生的修改操作是否记录到二进制日志，ON-记录，OFF-不记录 max_binlog_size: 二进制日志文件的单文件大小上限； max_binlog_cache_size: max_binlog_stmt_cache_size: sync_binlog = [0|+N]: 设定多久个事务提交之后，同步一次二进制日志文件 0: 0表示不同步 +N: 任何正值表示记录 N 个事务提交之后同步一次 123456789101112131415161718192021222324252627# 查看主服务器端处于由mysqld维护状态中的二进制日志文件；SHOW &#123;BINARY | MASTER&#125; LOGS# 查看正在使用的二进制日志的状态SHOW [MASTER|BINARY] STATUS # 显示指定的二进制日志文件中的相关事件SHOW BINLOG EVENTS [IN 'log_name'] [FROM pos] [LIMIT [offset,] row_count]# 滚动二进制日志，每次重启 Mairadb 时，也会滚动二进制日志FLUSH LOGSshow global variables like \"log_bin\";+---------------+-------+| Variable_name | Value |+---------------+-------+| log_bin | OFF |+---------------+-------+show global variables like \"max_bin%\";+----------------------------+----------------------+| Variable_name | Value |+----------------------------+----------------------+| max_binlog_cache_size | 18446744073709547520 || max_binlog_size | 1073741824 || max_binlog_stmt_cache_size | 18446744073709547520 |+----------------------------+----------------------+ 2.2 日志记录格式：mysqlbinlog [options] log_file 作用: 查看 mysql 的二进制日志 参数: -j, --start-position=#：从指定的事件位置查看 --stop-position=#：只显示到指定的事件位置 --start-datetime=datetime: --stop-datetime=datetime: datetiem format - YYYY-MM-DD hh:mm:ss 二进制日志的格式 事件的起始位置(at) 事件发生的日期和时间；(140829 15:50:07) 事件发生在服务器的标识（server id） 事件的结束位置：（end_log_pos 19486） 事件的类型：(Query) 事件发生时所在的服务器执行此事件的线程的ID：（thread_id=13） 语句的时间戳与将其写入二进制文件中的时间差：（exec_time=0） 错误代码：（error_code=0) 事件内容：（SET TIMESTAMP=1409298607/!/;GRANT SELECT ON tdb.* TO tuser@localhost） GTID事件专属：事件所属的全局事务的GTID：（GTID 0-1-2） 123456# at 19364#140829 15:50:07 server id 1 end_log_pos 19486 Query thread_id=13 exec_time=0 error_code=0SET TIMESTAMP=1409298607/*!*/;GRANT SELECT ON tdb.* TO tuser@localhost/*!*/;# at 19486 3. 事务日志（innodb存储引擎） 作用: Innodb 用于实现事务的日志 位置: ib_logfileN, 位于数据存储目录下，以组的方式出现 配置: innodb_log_group_home_dir: 事务日志目录 innodb_log_files_in_group: 事务日志组中包含的文件数 1234567891011121314151617181920212223242526272829303132show variables like \"innodb_log%\";+-----------------------------+----------+| Variable_name | Value |+-----------------------------+----------+| innodb_log_buffer_size | 16777216 || innodb_log_checksums | ON || innodb_log_compressed_pages | ON || innodb_log_file_size | 50331648 || innodb_log_files_in_group | 2 || innodb_log_group_home_dir | ./ || innodb_log_optimize_ddl | ON || innodb_log_write_ahead_size | 8192 |+-----------------------------+----------+ll /data/总用量 122936-rw-rw----. 1 mysql mysql 24576 3月 10 16:58 aria_log.00000001 # MyISAM 日志-rw-rw----. 1 mysql mysql 52 3月 10 16:58 aria_log_control-rw-rw----. 1 mysql mysql 1004 3月 10 16:58 ib_buffer_pool-rw-rw----. 1 mysql mysql 12582912 3月 10 16:58 ibdata1 # Innodb 全局表空间-rw-rw----. 1 mysql mysql 50331648 3月 10 16:58 ib_logfile0 # Innodb 事务日志-rw-rw----. 1 mysql mysql 50331648 3月 10 00:14 ib_logfile1-rw-rw----. 1 mysql mysql 12582912 3月 10 16:58 ibtmp1-rw-rw----. 1 mysql mysql 142 3月 10 16:58 localhost.log # 查询日志-rw-rw----. 1 mysql mysql 6 3月 10 16:58 localhost.pid -rw-rw----. 1 mysql mysql 142 3月 10 16:58 localhost-slow.log # 慢查询日志-rw-rw----. 1 mysql mysql 329 3月 10 16:58 master-log.000001 # 二进制日志-rw-rw----. 1 mysql mysql 20 3月 10 16:58 master-log.index-rw-rw----. 1 mysql mysql 0 3月 10 00:27 multi-master.infodrwx------. 2 mysql mysql 4096 3月 10 00:15 mysqldrwx------. 2 mysql mysql 20 3月 10 00:15 performance_schemadrwx------. 2 mysql mysql 94 3月 10 01:35 test 4. 日志相关的服务器参数详解：通用日志选项: log_output={TABLE|FILE|NONE}: 定义一般查询日志和慢查询日志的保存方式，可以是TABLE、FILE、NONE，也可以是TABLE及FILE的组合(用逗号隔开)，默认为TABLE。如果组合中出现了NONE，那么其它设定都将失效，同时，无论是否启用日志功能，也不会记录任何相关的日志信息。作用范围为全局级别，可用于配置文件，属动态变量 慢查询日志: log_slow_queries={YES|NO}: 是否记录慢查询日志。慢查询是指查询的执行时间超出long_query_time参数所设定时长的事件。MySQL 5.6将此参数修改为了slow_query_log。作用范围为全局级别，可用于配置文件，属动态变量 slow_query_log={ON|OFF}: 设定是否启用慢查询日志。0或OFF表示禁用，1或ON表示启用。日志信息的输出位置取决于log_output变量的定义，如果其值为NONE，则即便slow_query_log为ON，也不会记录任何慢查询信息。作用范围为全局级别，可用于选项文件，属动态变量。 slow_query_log_file=/PATH/TO/SOMEFILE: 设定慢查询日志文件的名称。默认为hostname-slow.log，但可以通过–slow_query_log_file选项修改。作用范围为全局级别，可用于选项文件，属动态变量。 sql_log_bin={ON|OFF}: 用于控制二进制日志信息是否记录进日志文件。默认为ON，表示启用记录功能。用户可以在会话级别修改此变量的值，但其必须具有SUPER权限。作用范围为全局和会话级别，属动态变量。 sql_log_off={ON|OFF}: 用于控制是否禁止将一般查询日志类信息记录进查询日志文件。默认为OFF，表示不禁止记录功能。用户可以在会话级别修改此变量的值，但其必须具有SUPER权限。作用范围为全局和会话级别，属动态变量。 sync_binlog=#: 设定多久同步一次二进制日志至磁盘文件中，0表示不同步，任何正数值都表示对二进制每多少次写操作之后同步一次。当autocommit的值为1时，每条语句的执行都会引起二进制日志同步，否则，每个事务的提交会引起二进制日志同步。 二进制日志: log-bin={YES|NO}: 是否启用二进制日志，如果为mysqld设定了–log-bin选项，则其值为ON，否则则为OFF。其仅用于显示是否启用了二进制日志，并不反应log-bin的设定值。作用范围为全局级别，属非动态变量 binlog-format={ROW|STATEMENT|MIXED}: 指定二进制日志的类型，默认为STATEMENT。如果设定了二进制日志的格式，却没有启用二进制日志，则MySQL启动时会产生警告日志信息并记录于错误日志中。作用范围为全局或会话，可用于配置文件，且属于动态变量 expire_logs_days={0..99}: 设定二进制日志的过期天数，超出此天数的二进制日志文件将被自动删除。默认为0，表示不启用过期自动删除功能。如果启用此功能，自动删除工作通常发生在MySQL启动时或FLUSH日志时。作用范围为全局，可用于配置文件，属动态变量 log_query_not_using_indexes={ON|OFF}: 设定是否将没有使用索引的查询操作记录到慢查询日志。作用范围为全局级别，可用于配置文件，属动态变量。 log_bin_trust_function_creators={TRUE|FALSE}: 此参数仅在启用二进制日志时有效，用于控制创建存储函数时如果会导致不安全的事件记录二进制日志条件下是否禁止创建存储函数。默认值为0，表示除非用户除了CREATE ROUTING或ALTER ROUTINE权限外还有SUPER权限，否则将禁止创建或修改存储函数，同时，还要求在创建函数时必需为之使用DETERMINISTIC属性，再不然就是附带READS SQL DATA或NO SQL属性。设置其值为1时则不启用这些限制。作用范围为全局级别，可用于配置文件，属动态变量 long_query_time=#: 设定区别慢查询与一般查询的语句执行时间长度。这里的语句执行时长为实际的执行时间，而非在CPU上的执行时长，因此，负载较重的服务器上更容易产生慢查询。其最小值为0，默认值为10，单位是秒钟。它也支持毫秒级的解析度。作用范围为全局或会话级别，可用于配置文件，属动态变量。 max_binlog_cache_size: {4096 .. 18446744073709547520} 二进定日志缓存空间大小，5.5.9及以后的版本仅应用于事务缓存，其上限由max_binlog_stmt_cache_size决定。作用范围为全局级别，可用于配置文件，属动态变量。 max_binlog_size={4096 .. 1073741824}: 设定二进制日志文件上限，单位为字节，最小值为4K，最大值为1G，默认为1G。某事务所产生的日志信息只能写入一个二进制日志文件，因此，实际上的二进制日志文件可能大于这个指定的上限。作用范围为全局级别，可用于配置文件，属动态变量。 查询日志: general_log={ON|OFF}: 设定是否启用查询日志，默认值为取决于在启动mysqld时是否使用了–general_log选项。如若启用此项，其输出位置则由–log_output选项进行定义，如果log_output的值设定为NONE，即使用启用查询日志，其也不会记录任何日志信息。作用范围为全局，可用于配置文件，属动态变量| general_log_file=FILE_NAME: 查询日志的日志文件名称，默认为“hostname.log”。作用范围为全局，可用于配置文件，属动态变量| log={YES|NO}: 是否启用记录所有语句的日志信息于一般查询日志(general query log)中，默认通常为OFF。MySQL 5.6已经弃用此选项 错误日志: log_error=/PATH/TO/ERROR_LOG_FILENAME: 定义错误日志文件。作用范围为全局或会话级别，可用于配置文件，属非动态变量 log_warnings=#: 设定是否将警告信息记录进错误日志。默认设定为1，表示启用；可以将其设置为0以禁用；而其值为大于1的数值时表示将新发起连接时产生的“失败的连接”和“拒绝访问”类的错误信息也记录进错误日志 中继日志: log_slave_updates: 用于设定复制场景中的从服务器是否将从主服务器收到的更新操作记录进本机的二进制日志中。本参数设定的生效需要在从服务器上启用二进制日志功能。 max_relay_log_size={4096..1073741824}: 设定从服务器上中继日志的体积上限，到达此限度时其会自动进行中继日志滚动。此参数值为0时，mysqld将使用max_binlog_size参数同时为二进制日志和中继日志设定日志文件体积上限。作用范围为全局级别，可用于配置文件，属动态变量。 relay_log=file_name: 设定中继日志的文件名称，默认为host_name-relay-bin。也可以使用绝对路径，以指定非数据目录来存储中继日志。作用范围为全局级别，可用于选项文件，属非动态变量。 relay_log_index=file_name: 设定中继日志的索引文件名，默认为为数据目录中的host_name-relay-bin.index。作用范围为全局级别，可用于选项文件，属非动态变量。 relay-log-info-file=file_name: 设定中继服务用于记录中继信息的文件，默认为数据目录中的relay-log.info。作用范围为全局级别，可用于选项文件，属非动态变量。 relay_log_purge={ON|OFF}: 设定对不再需要的中继日志是否自动进行清理。默认值为ON。作用范围为全局级别，可用于选项文件，属动态变量。 relay_log_space_limit=#: 设定用于存储所有中继日志文件的可用空间大小。默认为0，表示不限定。最大值取决于系统平台位数。作用范围为全局级别，可用于选项文件，属非动态变量。 Innodb事务日志: innodb_log_buffer_size: ={262144 .. 4294967295}设定InnoDB用于辅助完成日志文件写操作的日志缓冲区大小，单位是字节，默认为8MB。较大的事务可以借助于更大的日志缓冲区来避免在事务完成之前将日志缓冲区的数据写入日志文件，以减少I/O操作进而提升系统性能。因此，在有着较大事务的应用场景中，建议为此变量设定一个更大的值。作用范围为全局级别，可用于选项文件，属非动态变量。 innodb_log_file_size:={108576 .. 4294967295}设定日志组中每个日志文件的大小，单位是字节，默认值是5MB。较为明智的取值范围是从1MB到缓存池体积的1/n，其中n表示日志组中日志文件的个数。日志文件越大，在缓存池中需要执行的检查点刷写操作就越少，这意味着所需的I/O操作也就越少，然而这也会导致较慢的故障恢复速度。作用范围为全局级别，可用于选项文件，属非动态变量。 innodb_log_files_in_group={2 .. 100}: 设定日志组中日志文件的个数。InnoDB以循环的方式使用这些日志文件。默认值为2。作用范围为全局级别，可用于选项文件，属非动态变量。 innodb_log_group_home_dir=/PATH/TO/DIR 设定InnoDB重做日志文件的存储目录。在缺省使用InnoDB日志相关的所有变量时，其默认会在数据目录中创建两个大小为5MB的名为ib_logfile0和ib_logfile1的日志文件。作用范围为全局级别，可用于选项文件，属非动态变量。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"马哥 MySQL 运维","slug":"马哥-MySQL-运维","permalink":"http://yoursite.com/tags/马哥-MySQL-运维/"}]},{"title":"3 MARIADB 基础使用","slug":"mysql/马哥_MySQL/3_mysql基础使用","date":"2019-10-07T16:00:00.000Z","updated":"2020-05-24T15:48:00.451Z","comments":true,"path":"2019/10/08/mysql/马哥_MySQL/3_mysql基础使用/","link":"","permalink":"http://yoursite.com/2019/10/08/mysql/马哥_MySQL/3_mysql基础使用/","excerpt":"mariadb 基础使用","text":"mariadb 基础使用 1.Mariadb 架构图2. Mariadb 存储引擎123456789101112# 查看支持的所有存储引擎&gt; show engines# 显示表状态信息&gt; use mysql&gt; show table status where engine=\"Aria\" # 显示 Innodb 引擎的状态信息&gt; show engine innodb status \\G# 显示 myisam 引擎的状态信息&gt; show engine aria status; 2.1 Innodb 存储引擎特性: 适用于处理大量短事务，但不适用于处理长事务 基于 MVCC 并发访问控制，支持四种隔离级别 使用聚集索引，支持自适应 hash 索引，左前缀索引 锁粒度: 行锁，间隙锁 支持热备份 Innodb 表空间(tablespace)1234567891011121314151617181920ll /data/总用量 122920-rw-rw----. 1 mysql mysql 24576 3月 10 00:31 aria_log.00000001-rw-rw----. 1 mysql mysql 52 3月 10 00:17 aria_log_control-rw-rw----. 1 mysql mysql 888 3月 10 00:17 ib_buffer_pool-rw-rw----. 1 mysql mysql 12582912 3月 10 00:17 ibdata1-rw-rw----. 1 mysql mysql 50331648 3月 10 00:27 ib_logfile0 # 事务日志，通常以组的形式出现-rw-rw----. 1 mysql mysql 50331648 3月 10 00:14 ib_logfile1 # 事务日志-rw-rw----. 1 mysql mysql 12582912 3月 10 00:27 ibtmp1-rw-rw----. 1 mysql mysql 6 3月 10 00:27 localhost.pid-rw-rw----. 1 mysql mysql 0 3月 10 00:27 multi-master.infodrwx------. 2 mysql mysql 4096 3月 10 00:15 mysqldrwx------. 2 mysql mysql 20 3月 10 00:15 performance_schemadrwx------. 2 mysql mysql 20 3月 10 00:15 test ll /data/test/总用量 120-rw-rw----. 1 mysql mysql 65 3月 10 00:15 db.opt-rw-rw----. 1 mysql mysql 1822 3月 10 00:54 test.frm-rw-rw----. 1 mysql mysql 114688 3月 10 00:54 test.ibd Innodb 的数据存储在表空间文件中，依据 innodb_file_per_table 是否启用分为两种不同的保存方式。 innodb_file_per_table=off 位置: 所有数据库的所有存储引擎为Innodb的表使用同一个表空间文件 datadir/ibdata[N]: 共用的表空间文件，用于保存所有Innodb表的数据和索引 数据库目录/db_name.frm: 表结构定义保存在各个数据库目录下 特性: 不支持单表导入等高级特性 innodb_file_per_table=on 位置: 每表使用单独的表空间文件，位于各个数据库目录下 db_name.ibd: 表单独的表空间文件，用于存储单独表的数据和索引 db_name.frm: 用于存储表结构定义 2.2 Aria(MyISAM)123456ll总用量 140-rw-rw----. 1 mysql mysql 65 3月 10 00:15 db.opt-rw-rw----. 1 mysql mysql 1820 3月 10 01:35 t1.frm-rw-rw----. 1 mysql mysql 8192 3月 10 01:35 t1.MAD-rw-rw----. 1 mysql mysql 8192 3月 10 01:35 t1.MAI 存储: 每表有三个文件，保存在对应的数据库目录中 db_name.frm: 表结构定义文件 db_name.MAD: 数据文件 db_name.MAI: 索引文件 3.Mariadb 锁策略Mariadb 的锁分为 Sever 级别锁: 又称为显示锁，可在 SQL 语句中自行决定是否加锁 存储引擎级别锁: 存储引擎为了实现并发访问控制而自行施加的锁 3.1 显示锁的使用1234567891011# 锁定单表LOCK TABLE tb_name [READ|WRITER]UNLOCK TABLES # 锁定所有数据库所有表FLUSH TABLES WITH [READ|WRITER] LOCKUNLOCK TABLES# 行锁SELECT CLUASE FOR UPDATE SELECT CLUASE LOCK IN SHARE MODE 4.用户与权限4.1 权限类别Mariadb 的权限分为以下几种类别 库，表级别的权限 ALTER, CREATE, DROP CREATE VIEW, SHOW VIEW INDEX GRANT OPTION：能够把自己获得的权限赠经其他用户一个副本； 字段级别: [SELECT, INSERT, UPDATE](col1,col2,…)DELETE 管理类: CREATE TEMPORARY TABLES CREATE USER FILE SUPER SHOW DATABASES RELOAD SHUTDOWN REPLICATION SLAVE REPLICATION CLIENT LOCK TABLES PROCESS 程序类: [CREATE, ALTER, DROP, EXCUTE] * [FUNCTION, PROCEDURE, TRIGGER] 所有权限: ALL PRIVILEGES, ALL 4.2 权限保存位置所有的授权都保存在 mariadb 的元数据数据库 mysql 的如下表中: db, user columns_priv tables_priv procs_priv proxies_priv global_priv 4.3 账号管理123456789101112131415# 创建用户：CREATE USERCREATE USER 'USERNAME'@'HOST' [IDENTIFIED BY 'password']；# 查看用户获得的授权：SHOW GRANTS FORSHOW GRANTS FOR 'USERNAME'@'HOST'# 用户重命名：RENAME USERRENAME USER old_user_name TO new_user_name# 删除用户DROP USER 'USERNAME'@'HOST'# 修改密码：SET PASSWORD FOR 'bob'@'%.loc.gov' = PASSWORD('newpass');UPDATE mysql.user SET password=PASSWORD('your_password') WHERE clause; 修改密码还可以使用 mysqladmin命令: mysqladmin password &quot;new_password&quot; -uroot -h -p 4.4 忘记管理员密码1234567891011systemctl stop mariadb# 1. 为 mariadb 添加 --skip-grant-tables --skip-networking 参数vim /usr/lib/systemd/system/mariadb.service # 在 ExecStart 后添加systemctl start mariadb# 2. 登录mariadb 并使用 UPDATE 命令修改管理员密码update mysql.user set password=PASSWORD(\"1234\") where user=\"root\";# 3. 关闭mysqld进程，移除上述两个选项，重启mysqld; 4.5 授权授权: GRANT priv_type[,...] ON [{table|function|procedure}] db.{table|routine} TO &#39;USERNAME&#39;@&#39;HOST&#39; [IDENTIFIED BY &#39;password&#39;] [REQUIRE SSL] [WITH with_option] 查看授权:SHOW GRANTS [FOR &quot;user&quot;@&quot;host&quot;] 收回授权: REVOKE priv_type[,...] ON [{table|function|procedure}] db.{table|routine} FROM user [, user] 7.查询缓存查询缓存： 如何判断是否命中：通过查询语句的哈希值判断：哈希值考虑的因素包括查询本身、要查询的数据库、客户端使用协议版本。查询语句任何字符上的不同，都会导致缓存不能命中； 哪此查询可能不会被缓存: 查询中包含UDF、存储函数、用户自定义变量、临时表、mysql库中系统表、或者包含列级权限的表、有着不确定值的函数(Now()); 7.1 查询缓存相关的服务器变量 query_cache_min_res_unit: 查询缓存中内存块的最小分配单位； 较小值会减少浪费，但会导致更频繁的内存分配操作； 较大值会带来浪费，会导致碎片过多； query_cache_limit：能够缓存的最大查询结果，对于有着较大结果的查询语句，建议在SELECT中使用SQL_NO_CACHE query_cache_size：查询缓存总共可用的内存空间；单位是字节，必须是1024的整数倍； query_cache_type: 可选值: OFF: 不启用，显示指定 SQL_CACHE 也不会缓存 ON: 启用，可以使用 SQL_NO_CACHE 显示指定不缓存查询结果 DEMAND: 按需启用，即可以使用 SQL_CACHE 显示指定缓存查询结果 query_cache_wlock_invalidate：如果某表被其它的连接锁定，是否仍然可以从查询缓存中返回结果；默认值为OFF，表示可以在表被其它连接锁定的场景中继续从缓存返回数据；ON则表示不允许； 7.2 查询相关的状态变量缓存命中率的评估：Qcache_hits/Com_select 1234567891011121314151617181920&gt; SHOW GLOBAL STATUS LIKE 'Qcache%';+-------------------------+----------+| Variable_name | Value |+-------------------------+----------+| Qcache_free_blocks | 1 || Qcache_free_memory | 16759688 | # 分配未使用的缓存块| Qcache_hits | 0 | # 缓存命中次数| Qcache_inserts | 0 | # 已经插入的缓存条目| Qcache_lowmem_prunes | 0 | # 因为内存太小，而被动失效缓存| Qcache_not_cached | 0 || Qcache_queries_in_cache | 0 | # 缓存的查询语句数| Qcache_total_blocks | 1 | # 已经分配的缓存块+-------------------------+----------+&gt; show global status like \"Com_select\";+---------------+-------+| Variable_name | Value |+---------------+-------+| Com_select | 26 | # 查询次数+---------------+-------+ 8.索引索引优点： - 索引可以降低服务需要扫描的数据量，减少了IO次数； - 索引可以帮助服务器避免排序和使用临时表； - 索引可以帮助将随机I/O转为顺序I/O； 高性能索引策略： - 独立使用列，尽量避免其参与运算； - 左前缀索引：索引构建于字段的左侧的多少个字符，要通过索引选择性来评估 - 索引选择性：不重复的索引值和数据表的记录总数的比值； - 多列索引：AND操作时更适合使用多列索引； - 选择合适的索引列次序：将选择性最高放左侧； 12# 查看表的索引&gt; SHOW INDEX FROM tb_name 8.1 B+ TreeB+ Tree索引： 特点: 顺序存储，每一个叶子节点到根结点的距离是相同的；左前缀索引，适合查询范围类的数据； 适用: 可以使用B-Tree索引的查询类型：全键值、键值范围或键前缀查找； 全值匹配：精确某个值, “Jinjiao King”； 匹配最左前缀：只精确匹配起头部分，”Jin%” 匹配范围值： 精确匹配某一列并范围匹配另一列： 只访问索引的查询 不适用: 如果不从最左列开始，索引无效； (Age,Name) 不能跳过索引中的列；(StuID,Name,Age) 如果查询中某个列是为范围查询，那么其右侧的列都无法再使用索引优化查询；(StuID,Name) 8.2 Hash 索引Hash索引： 特点: 基于哈希表实现，特别适用于精确匹配索引中的所有列； 注意：只有Memory存储引擎支持显式hash索引； 适用： 只支持等值比较查询，包括=, IN(), &lt;=&gt;; 不适合: 存储的非为值的顺序，因此，不适用于顺序查询； 不支持模糊匹配； 8.3 EXPLAINEXPLAIN SELECT clause 作用: 获取查询执行计划信息，用来查看查询优化器如何执行查询； 输出： id: 当前查询语句中，每个SELECT语句的编号； select_type： 简单查询为SIMPLE 复杂查询： SUBQUERY: 简单子查询； DERIVED: 用于FROM中的子查询； PRIMARY: 联合查询中的第一个查询 UNION: 联合查询中的第一个查询之后的其他查询 UNION RESULT: 联合查询生成的临时表 注意：UNION查询的分析结果会出现一外额外匿名临时表； table：SELECT语句关联到的表； type：关联类型，或访问类型，即MySQL决定的如何去查询表中的行的方式； ALL: 全表扫描； index：根据索引的次序进行全表扫描；如果在Extra列出现“Using index”表示了使用覆盖索引，而非全表扫描； range：有范围限制的根据索引实现范围扫描；扫描位置始于索引中的某一点，结束于另一点； ref: 根据索引返回表中匹配某单个值的所有行； eq_ref: 根据索引返回表中匹配某单个值的单一行，仅返回一行； const, system: 与常量比较，直接返回单个行； possible_keys：查询可能会用到的索引； key: 查询中使用了的索引； key_len: 在索引使用的字节数； ref: 在利用key字段所表示的索引完成查询时所有的列或某常量值； rows：MySQL估计为找所有的目标行而需要读取的行数； Extra：额外信息 Using index condition：使用索引进行条件过滤 Using index：MySQL将会使用覆盖索引，以避免访问表； Using where：MySQL服务器将在存储引擎检索后，再进行一次过滤； Using temporary：MySQL对结果排序时会使用临时表； Using filesort：对结果使用一个外部索引排序；","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"马哥 MySQL 运维","slug":"马哥-MySQL-运维","permalink":"http://yoursite.com/tags/马哥-MySQL-运维/"}]},{"title":"2 MYSQL 常用服务端参数","slug":"mysql/马哥_MySQL/2_常用服务端参数","date":"2019-10-06T16:00:00.000Z","updated":"2020-05-24T15:46:05.693Z","comments":true,"path":"2019/10/07/mysql/马哥_MySQL/2_常用服务端参数/","link":"","permalink":"http://yoursite.com/2019/10/07/mysql/马哥_MySQL/2_常用服务端参数/","excerpt":"mariadb 常用服务端参数","text":"mariadb 常用服务端参数 1. mariadb 特性参数1.1 SQL_MODESQL_MODE 作用: 设置 sql 模式，sql 模式会影响值溢出等行为的处理方式 1.2 QUERY_CACHE_TYPEQUERY_CACHE_TYPE 作用: 是否启用查询缓存 可选值: OFF: 不启用，显示指定 SQL_CACHE 也不会缓存 ON: 启用，可以使用 SQL_NO_CACHE 显示指定不缓存查询结果 DEMAND: 按需启用，即可以使用 SQL_CACHE 显示指定缓存查询结果 1.3 事务隔离级别设置TX_ISOLATION 作用: 设置事务的隔离级别 可选值: REPEATABLE-READ: 可重复度 READ-UNCOMMITTED: 读未提交 READ-COMMITTED: 读提交 SERIALIZABLE: 串行化 事务日志配置: innodb_log_files_in_group: 一个事务日志组中包含几个文件 innodb_log_group_home_dir：事务日志所在的目录 innodb_log_file_size：事务日志的大小 12345678910111213141516171819202122232425select @@session.tx_isolation;show global variables like \"innodb%log%\";+----------------------------------+-----------+| Variable_name | Value |+----------------------------------+-----------+| innodb_encrypt_log | OFF || innodb_flush_log_at_timeout | 1 || innodb_flush_log_at_trx_commit | 1 || innodb_locks_unsafe_for_binlog | OFF || innodb_log_buffer_size | 16777216 || innodb_log_checksums | ON || innodb_log_compressed_pages | ON || innodb_log_file_size | 50331648 || innodb_log_files_in_group | 2 || innodb_log_group_home_dir | ./ || innodb_log_optimize_ddl | ON || innodb_log_write_ahead_size | 8192 || innodb_max_undo_log_size | 10485760 || innodb_online_alter_log_max_size | 134217728 || innodb_scrub_log | OFF || innodb_scrub_log_speed | 256 || innodb_undo_log_truncate | OFF || innodb_undo_logs | 128 |+----------------------------------+-----------+","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"马哥 MySQL 运维","slug":"马哥-MySQL-运维","permalink":"http://yoursite.com/tags/马哥-MySQL-运维/"}]},{"title":"1 MYSQL 安装配置","slug":"mysql/马哥_MySQL/1_mysql安装配置","date":"2019-10-05T16:00:00.000Z","updated":"2020-05-24T15:45:18.164Z","comments":true,"path":"2019/10/06/mysql/马哥_MySQL/1_mysql安装配置/","link":"","permalink":"http://yoursite.com/2019/10/06/mysql/马哥_MySQL/1_mysql安装配置/","excerpt":"mariadb 安装配置","text":"mariadb 安装配置 1. mariadb 简介自从 mysql 被 Oracle 收购之后，由于担心版权问题，mysql 的创始人就新建了另一开源分支 mariadb，在 Centos6 中默认安装的是 mysql，而在 Centos7 中默认安装的已经是 mariadb。mariadb 跟 mysql 底层的基础特性是类似的，但是高级特性有很大不同，彼此支持的高级功能也不相同。除了 mariadb，mysql还有很多二次发行版本，比如Percona，AllSQL(阿里的mysql 发行版)以及，TIDB mysql 与 mariadb 的官网分别是： www.mysql.com MariaDB: www.mariadb.org 1.1 mariadb 特性mariadb 有两个比较重要的特性，一个是它是一个单进程多线程程序，另一个是支持插件式存储引擎，即存储管理器有多种实现版本，彼此间的功能和特性可能略有区别；用户可根据需要灵活选择。存储引擎也称为“表类型”。常见的存储引擎就是 MyISAM:不支持事务和表级锁，奔溃后不保证安全恢复； InnoDB: 支持事务，行级锁，外键和热备份； MyISAM 在 mariadb 中被扩展为 Aria，支持安全恢复, InnoDB 在 Mariadb 中的开源实现为 XtraDB。在 mysql 的客户端中输入 show engines 即可查看 mariadb 支持的所有存储引擎。 123456789101112131415MariaDB [(none)]&gt; show engines;+--------------------+---------+----------------------------------------------------------------------------+--------------+------+------------+| Engine | Support | Comment | Transactions | XA | Savepoints |+--------------------+---------+----------------------------------------------------------------------------+--------------+------+------------+| CSV | YES | CSV storage engine | NO | NO | NO || MRG_MYISAM | YES | Collection of identical MyISAM tables | NO | NO | NO || MEMORY | YES | Hash based, stored in memory, useful for temporary tables | NO | NO | NO || BLACKHOLE | YES | /dev/null storage engine (anything you write to it disappears) | NO | NO | NO || MyISAM | YES | MyISAM storage engine | NO | NO | NO || InnoDB | DEFAULT | Percona-XtraDB, Supports transactions, row-level locking, and foreign keys | YES | YES | YES || ARCHIVE | YES | Archive storage engine | NO | NO | NO || FEDERATED | YES | FederatedX pluggable storage engine | YES | NO | YES || PERFORMANCE_SCHEMA | YES | Performance Schema | NO | NO | NO || Aria | YES | Crash-safe tables with MyISAM heritage | NO | NO | NO |+--------------------+---------+----------------------------------------------------------------------------+--------------+------+------------+ 1.2 MariaDB程序的组成mariadb 是 C/S 架构的服务，其命令分为服务器端和客户端两个部分 C：Client mysql：CLI交互式客户端程序； mysqldump：备份工具； mysqladmin：管理工具； mysqlbinlog： … S：Server mysqld：默认的 mariadb 启动的守护进程 mysqld_safe：mariadb 线程安全版本，通常在线上环境安装的是此服务而不是 mysqld； mysqld_multi：用于在单主机上运行多 mariadb 实例的服务 msyql 服务器可监听在两种套接字上 IPV4/6 的 tcp 的 3306 端口上，支持远程通信 Unix Sock，监听在 socket 文件上，仅支持本地通信，套接子文件通常位于 /var/lib/mysql/mysql.sock或 /tmp/mysql.sock 由配置文件指定。 12ll /var/lib/mysql/mysql.socksrwxrwxrwx. 1 mysql mysql 0 8月 21 11:10 /var/lib/mysql/mysql.sock 2. MariaDB 安装与其他软件一样，Mariadb 常见的安装方式有如下三种: rpm包；由OS的发行商提供，或从程序官方直接下载 源码包编译安装: 编译安装，除非需要定制功能，否则一般不推荐编译安装 通用二进制格式的程序包: 展开至特定路径，并经过简单配置后即可使用，这种方式便于部署，无需解决环境依赖 通常情况下，为了便于自动化安装配置，我们都会以 rpm 包的方式进行安装，Centos7 中安装 mariadb 的命令如下： 1234yum install mariadb-server# 如果修改了mysql 默认保存数据的存储目录 datadir，需要重新执行 mysql_install_dbmysql_install_db --user=mysql --datadir=/data 2.1 初始化配置mysql的用户账号由两部分组成：&#39;USERNAME&#39;@&#39;HOST&#39;; HOST: 用于限制此用户可通过哪些远程主机连接当前的mysql服务.HOST的表示方式，支持使用通配符： %：匹配任意长度的任意字符； 172.16.%.% == 172.16.0.0/16 _：匹配任意单个字符； 默认情况下 mysql 登陆时会对客户端的 IP 地址进行反解，这种反解一是浪费时间可能导致阻塞，二是如果反解成功而 mysql 在授权时只授权了 IP 地址而没有授权主机名，依旧无法登陆，所以在配置 mysql 时都要关闭名称反解功能。 1234vim /etc/my.cnf # 添加三个选项：datadir = /mydata/datainnodb_file_per_table = ONskip_name_resolve = ON 2.2 mysql 安全初始化默认安装的情况下 mysql root 帐户是没有密码的，可通过 mysql 提供的安全初始化脚本，快速进行安全初始化。1234567# 查看mysql用户及其密码mysql&gt; use mysql;&gt; select user,host,password from user;# 运行脚本安全初始化脚本/user/local/mysql/bin/mysql_secure_installation 3. Mariadb 配置3.1 配置文件格式mysql 的配置文件是 ini 风格的配置文件；客户端和服务器端的多个程序可通过一个配置文件进行配置，使用 [program_name] 标识配置的程序即可： mysqld：配置 mysqld 服务 mysqld_safe：配置 mysqld_safe 服务 server：适用于所有服务 mysql：mysql 命令行客户端配置 mysqldump：mysqldump 配置 client：适用于所有客户端 1234567891011vim /etc/my.cnf[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sock[mysqld_safe]log-error=/var/log/mariadb/mariadb.logpid-file=/var/run/mariadb/mariadb.pid# include all files from the config directory!includedir /etc/my.cnf.d 3.2 配置文件读取次序mysql 的各类程序启动时都读取不止一个配置文件，配置文件将按照特定的顺序读取，最后读取的为最终生效的配置。可以使用 my_print_defaults 查看默认的配置文件查找次序。 123$ my_print_defaultsDefault options are read from the following files in the given order:/etc/mysql/my.cnf /etc/my.cnf ~/.my.cnf 除了默认配置文件，mariadb 还可以通过命令行参数传入配置文件的位置： --default-extra-file=/PATH/TO/CONF_FILE: 默认的配置文件之外在加一个配置文件 --default-file : 修改默认读取的配置文件 配置文件查找次序默认情况下 OS Vendor提供mariadb rpm包安装的服务的配置文件查找次序： /etc/mysql/my.cnf /etc/my.cnf /etc/my.cnf.d/ --default-extra-file=/PATH/TO/CONF_FILE: 通过命令行指定的配置文件 ~/.my.cnf: 家目录下的配置文件 通用二进制格式安装的服务程序其配置文件查找次序 /etc/my.cnf /etc/my.cnf.d/ /etc/mysql/my.cnf --default-extra-file=/PATH/TO/CONF_FILE: 通过命令行指定的配置文件 ~/.my.cnf: 家目录下的配置文件 12345678910# os rpm 包安装的 mariadb 配置文件ll -d /etc/my*-rw-r--r--. 1 root root 570 6月 8 2017 /etc/my.cnfdrwxr-xr-x. 2 root root 67 2月 27 09:57 /etc/my.cnf.dll /etc/my.cnf.d总用量 12-rw-r--r--. 1 root root 295 4月 30 2017 client.cnf-rw-r--r--. 1 root root 232 4月 30 2017 mysql-clients.cnf-rw-r--r--. 1 root root 744 4月 30 2017 server.cnf 3.3 运行时参数修改123MariaDB [(none)]&gt; help 'show variables'MariaDB [(none)]&gt; show global variables like 'skip_name_resolve'MariaDB [(none)]&gt; show variables where variable_name=\"innodb_version\"; 运行时参数查看： SHOW GLOBAL VARIABLES [LIKE &#39;pattern&#39; | WHERE expr]: 查看全局默认参数 SHOW SESSION VARIABLES [LIKE &#39;pattern&#39; | WHERE expr]: 查看当前会话的参数 运行时参数修改: SET GLOBAL VARIABLES= 或者 SET @@GLOBAL.VARIABLES=: 修改全局默认参数，仅对修改后新建的会话有效 SET SESSION VARIABLES= 或者 SET @@SESSION.VARIABLES=: 修改当前会话参数 3.4 查看 mariadb 状态变量SHOW [GLOBAL | SESSION] STATUS [LIKE &#39;pattern&#39; | WHERE expr] 4 mysql 客户端启动命令mysql [OPTIONS] [database] 常用选项： -u, --user=name：用户名，默认为root； -h, --host=name：远程主机（即mysql服务器）地址，默认为localhost; -p, --password：USERNAME所表示的用户的密码； 默认为空； -P, --port: 指定 mysql 服务监听的端口，默认为 3306 -D, --database：连接到服务器端之后，设定其处指明的数据库为默认数据库； -e, --execute=&#39;SQL COMMAND;&#39;：连接至服务器并让其执行此命令后直接返回； -S, --socket: 指定本地通信的套接字路经 mysql 客户端内可输入的命令分为两类: 客户段命令: 只在客户端运行的命令，使用 help 可获取此类命令的帮助 服务段命令: 通过 mysql 的协议送到服务段运行的命令，所以必须要有命令结束符,默认为 ;；使用 help contents 获取服务器端命令使用帮助。 4.1 查看本地命令mysql&gt; help \\u db_name：设定哪个库为默认数据库 \\q：退出 \\d CHAR：设定新的语句结束符，默认为 ; \\g：语句结束标记，默认就相当于 ; 作用 \\G：语句结束标记，结果竖排方式显式 \\! COMMAND: 在客户端内运行 shell 命令 \\. PATH: 在客户端内执行 sql 脚本(包含 sql 的文本) 12345678910111213141516171819202122232425262728293031323334353637$ mysql -uroot -p1234MariaDB [(none)]&gt; help # help 查看 mysql 的所有命令List of all MySQL commands:Note that all text commands must be first on line and end with &apos;;&apos;? (\\?) Synonym for `help&apos;.clear (\\c) Clear the current input statement.connect (\\r) Reconnect to the server. Optional arguments are db and host.delimiter (\\d) Set statement delimiter.edit (\\e) Edit command with $EDITOR.ego (\\G) Send command to mysql server, display result vertically.exit (\\q) Exit mysql. Same as quit.go (\\g) Send command to mysql server.help (\\h) Display this help.nopager (\\n) Disable pager, print to stdout.notee (\\t) Don&apos;t write into outfile.pager (\\P) Set PAGER [to_pager]. Print the query results via PAGER.print (\\p) Print current command.prompt (\\R) Change your mysql prompt.quit (\\q) Quit mysql.rehash (\\#) Rebuild completion hash.source (\\.) Execute an SQL script file. Takes a file name as an argument.status (\\s) Get status information from the server.system (\\!) Execute a system shell command.tee (\\T) Set outfile [to_outfile]. Append everything into given outfile.use (\\u) Use another database. Takes database name as argument.charset (\\C) Switch to another charset. Might be needed for processing binlog with multi-byte charsets.warnings (\\W) Show warnings after every statement.nowarning (\\w) Don&apos;t show warnings after every statement.For server side help, type &apos;help contents&apos;# 执行 shell 命令MariaDB [(none)]&gt; \\! ls /varaccount cache db games iso lib lock mail nis preserve spool tmp ypadm crash empty gopher kerberos local log named opt run target www 4.2 查看服务端命令12345678910111213141516171819202122232425262728293031323334MariaDB [(none)]&gt; help contents # 查看 mysql 命令的组成部分For more information, type 'help &lt;item&gt;', where &lt;item&gt; is one of the followingcategories: Account Management Administration Compound Statements Data Definition Data Manipulation.........MariaDB [(none)]&gt; help 'Account Management' # 查看特定命令组内的命令topics: CREATE USER DROP USER GRANT RENAME USER REVOKE SET PASSWORDMariaDB [(none)]&gt; help 'CREATE USER' # 查看特定命令使用帮助Name: 'CREATE USER'Description:Syntax:CREATE USER user_specification [, user_specification] ...user_specification: user [ IDENTIFIED BY [PASSWORD] 'password' | IDENTIFIED WITH auth_plugin [AS 'auth_string'] ]...............","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"马哥 MySQL 运维","slug":"马哥-MySQL-运维","permalink":"http://yoursite.com/tags/马哥-MySQL-运维/"}]},{"title":"5. 索引使用策略","slug":"mysql/高性能的mysql/05_索引使用策略","date":"2019-10-04T16:00:00.000Z","updated":"2020-05-24T15:44:41.722Z","comments":true,"path":"2019/10/05/mysql/高性能的mysql/05_索引使用策略/","link":"","permalink":"http://yoursite.com/2019/10/05/mysql/高性能的mysql/05_索引使用策略/","excerpt":"如何使用 mysql 的索引","text":"如何使用 mysql 的索引 1. 前言这是与索引相关的第二篇文章，上一篇我们讲解了索引的基本原理，介绍了各种不同类型的索引的适用情景。本文我们将介绍高效适用索引的策略。下面介绍的索引适用策略大多数于 Innodb 的 B-Tree 索引相关。我们会介绍: mysql 通用的索引适用技巧 B-Tree 索引所使用的聚簇索引原理 B-Tree 索引的使用技巧(或者叫优化策略) 2. mysql 通用索引技巧2.1 独立的列“独立的列”是指索引列不能是表达式的一部分，也不能是函数的参数。在 mysql 中，如果查询中的列不是独立的，则MySQL就不会使用索引。例如:12# 1. mysql 不会使用 actor_id 上的索引mysql&gt; SELECT actor_id FROM sakila.actor WHERE actor_id + 1 = 5; 我们应该养成简化WHERE条件的习惯，始终将索引列单独放在比较符号的一侧。 2.2 索引选择性对于字符串索引，mysql 支持只索引开始的部分字符，称为前缀索引。像下面这样: 12# 1. 添加前缀索引。mysql&gt; ALTER TABLE sakila.city_demo ADD KEY (city(7)); 前缀索引能使索引更小、大大节约索引空间，从而提高索引效率。但是其也有缺点: 降低索引的选择性，并且MySQL无法使用前缀索引做ORDER BY和GROUP BY，也无法使用前缀索引做覆盖扫描。对于BLOB、TEXT或者很长的VARCHAR类型的列，必须使用前缀索引，因为MySQL不允许索引这些列的完整长度。 而如何选择前缀索引的长度以及在哪些列上创建索引，要看索引的选择性。索引的选择性是指，不重复的索引值（也称为基数，cardinality）和数据表的记录总数（#T）的比值。对于前缀索引要选择足够长的前缀以保证较高的选择性，同时又不能太长（以便节约空间）。前缀应该足够长，以使得前缀索引的选择性接近于索引整个列。 通常我们要按照如下的步骤取选择前缀的长度: 首先我们要计算不同前缀长度的索引选择性，选择长度合适且与整列选择性接近索引长度 只看平均选择性是不够的，如果数据分布很不均匀，可能就会有陷阱，需要考虑最坏情况下的选择性。即出现次数最多头部数据的选择性。 最后，可能还需要根据那些运行频率最高的查询来调整索引列的顺序， 下面是一个对城市名字段创建前缀索引的探索过程:123456789101112131415# 1. 计算不同索引长度的选择性select COUNT(distinct LEFT(city, 3)) / COUNT(*) AS sel3, COUNT(distinct LEFT(city, 4)) / COUNT(*) AS sel3, COUNT(distinct LEFT(city, 5)) / COUNT(*) AS sel3, COUNT(distinct LEFT(city, 6)) / COUNT(*) AS sel3, COUNT(distinct LEFT(city, 7)) / COUNT(*) AS sel3, COUNT(distinct city) / COUNT(*) AS selfrom city_demo# 2. 观察使用前缀为 4，和使用整列的头部数据出现次数select COUNT(*) as cnt, LEFT(city, 4) as preffrom city_demo group by pref order by cnt desc limit 5;select COUNT(*) as cnt, cityfrom city_demo group by city order by cnt desc limit 5; MySQL原生并不支持反向索引，但是可以把字符串反转后存储，并基于此建立前缀索引。可以通过触发器来维护这种索引。索引的内容的选择并不只有前缀后缀两种，需要我们根据数据特征去截取选择性最高的数据部分。比如一段数据的前后都是重复的，而中间部分选择性足够高可以作为索引，我们就需要节取中间部分数据用作索引，这其实也是模拟哈希索引的一种，只不过这里的哈希函数只针对特定数据。 2.3 多列索引我们先来说一说，多列索引的一个误区: 为每个列创建独立的索引。在多个列上建立独立的单列索引大部分情况下并不能提高MySQL的查询性能。MySQL 5.0和更新版本引入了“索引合并”(index merge)策略来优化多列索引的使用。 1234567891011121314151617181920212223mysql&gt; SELECT film_id, actor_id FROM sakila.film_actor -&gt; WHERE actor_id = 1 OR film_id = 1;# 1. 在老的MySQL版本中，MySQL对这个查询会使用全表扫描。除非改写成如下的两个查询UNION的方式mysql&gt; SELECT film_id, actor_id FROM sakila.film_actor WHERE actor_id = 1 -&gt; UNION ALL -&gt; SELECT film_id, actor_id FROM sakila.film_actor WHERE actor_id &lt;&gt; 1 AND film_id = 1# 2. mysql5.1及更高版本引入了 索引合并，查询能够同时使用这两个单列索引进行扫描，并将结果进行合并mysql&gt; EXPLAIN SELECT film_id, actor_id FROM sakila.film_actor -&gt; WHERE actor_id = 1 OR film_id = 1\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: film_actor type: index_mergepossible_keys: PRIMARY,idx_fk_film_id key: PRIMARY,idx_fk_film_id key_len: 2,2 ref: NULL rows: 29 Extra: Using union(PRIMARY,idx_fk_film_id); Using where 索引合并有三个变种： OR条件的联合（union） AND条件的相交（intersection） 组合前两种情况的联合及相交 索引合并策略有时候是一种优化的结果，但实际上更多时候说明了表上的索引建得很糟糕： 当出现服务器对多个索引做相交操作时（通常有多个AND条件），通常意味着需要一个包含所有相关列的多列索引，而不是多个独立的单列索引 当服务器需要对多个索引做联合操作时（通常有多个OR条件），通常需要耗费大量CPU和内存资源在算法的缓存、排序和合并操作上。特别是当其中有些索引的选择性不高，需要合并扫描返回的大量数据的时候 更重要的是，优化器不会把这些计算到“查询成本”（cost）中，优化器只关心随机页面读取。这会使得查询的成本被“低估”，导致该执行计划还不如直接走全表扫描。这样做不但会消耗更多的CPU和内存资源，还可能会影响查询的并发性，但如果是单独运行这样的查询则往往会忽略对并发性的影响。通常来说，还不如像在MySQL 4.1或者更早的时代一样，将查询改写成UNION的方式往往更好 如果在EXPLAIN中看到有索引合并，应该好好检查一下查询和表的结构，看是不是已经是最优的。也可以通过参数optimizer_switch来关闭索引合并功能。也可以使用IGNORE INDEX提示让优化器忽略掉某些索引。 3. Innodb 聚簇索引3.1 聚簇索引的原理聚簇索引指的是一种数据存储方式。术语“聚簇”表示数据行和相邻的键值紧凑地存储在一起。即数据行实际上存放在索引的叶子页（leaf page）中。InnoDB通过主键聚集数据，主键索引中同时保存了B-Tree索引和数据行。 如果没有定义主键，InnoDB会选择一个唯一的非空索引代替。如果没有这样的索引，InnoDB会隐式定义一个主键来作为聚簇索引。InnoDB只聚集在同一个页面中的记录。包含相邻键值的页面可能会相距甚远。 因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。我们就以Innode 和 MyISAM为例来讲解聚簇和非聚簇索引的区别，他们都是用了B-Tree索引。 1234567CREATE TABLE layout_test ( col1 int NOT NULL, col2 int NOT NULL, PRIMARY KEY(col1), KEY(col2)); 假设该表的主键取值为1～10000，按照随机顺序插入并使用OPTIMIZE TABLE命令做了优化。换句话说，数据在磁盘上的存储方式已经最优，但行的顺序是随机的。列col2的值是从1～100之间随机赋值，所以有很多重复的值。 MyISAMMyISAM按照数据插入的顺序存储在磁盘上，如下图最左边的行号就是物理页的指针。 MyISAM—SAVE 当创建索引时，如下图所示，索引包含了”行号”。MyISAM中主键索引和其他索引在结构上没有什么不同。主键索引就是一个名为PRIMARY的唯一非空索引。 MyISAM—SAVE InnodeInnode 的聚簇索引按如下图的方式存储数据: Innodb—SAVE 主键索引上保存了整个表的数据，聚簇索引的每一个叶子节点都包含了主键值、事务ID、用于事务和MVCC的回滚指针以及所有的剩余列。在InnoDB中，聚簇索引“就是”表，不像MyISAM那样需要独立的行存储。因为数据行是按照主键顺序存储的，在插入新行，或者主键被更新导致需要移动行的时候，可能面临“页分裂（page split）”的问题。当行的主键值要求必须将这一行插入到某个已满的页中时，存储引擎会将该页分裂成两个页面来容纳该行，这就是一次页分裂操作。页分裂会导致表占用更多的磁盘空间。 对于二级索引，索引的叶子节点中存储的不是“行指针”，而是主键值。如下图所示: Innodb—INDEX 这样的策略减少了当出现行移动或者数据页分裂时二级索引的维护工作。使用主键值当作指针会让二级索引占用更多的空间，换来的好处是，InnoDB在移动行时无须更新二级索引中的这个“指针” 对比下面是聚簇和非聚簇索引的对比示意图:myisam_innodb_compare 聚簇索引特性聚簇索引由于存储上的特点，使得它具有如下特性: 优点: 可以把相关数据保存在一起，数据访问更快 使用覆盖索引扫描的查询可以直接使用页节点中的主键值 最大限度地提高了I/O密集型应用的性能，但如果数据全部都放在内存中，聚簇索引也就没什么优势了 缺点: 插入和更新时的叶分裂问题，因此插入速度严重依赖于插入顺序。按照主键的顺序插入是加载数据到InnoDB表中速度最快的方式。但如果不是按照主键顺序加载数据，那么在加载完成后最好使用OPTIMIZE TABLE命令重新组织一下表 更新聚簇索引列的代价很高，因为会强制InnoDB将每个被更新的行移动到新的位置。 簇索引可能导致全表扫描变慢，尤其是行比较稀疏，或者由于页分裂导致数据存储不连续的时候。 二级索引（非聚簇索引）可能比想象的要更大，因为在二级索引的叶子节点包含了引用行的主键列。 二级索引访问需要两次索引查找，而不是一次，因为i需要根据主键在查找一次主键的B-Tree索引，又称为回表 3.2 Innodb 的主键管理如果正在使用InnoDB表并且没有什么数据需要聚集，那么可以定义一个代理键（surrogate key）作为主键。最简单的方法是使用AUTO_INCREMENT自增列。这样可以保证数据行是按顺序写入，对于根据主键做关联操作的性能也会更好。 最好避免随机的（不连续且值的分布范围非常大）聚簇索引。例如:使用UUID来作为聚簇索引则会很糟糕：它使得聚簇索引的插入变得完全随机，这是最坏的情况，使得数据没有任何聚集特性。因为新行的主键值不一定比之前插入的大，所以InnoDB无法简单地总是把新行插入到索引的最后，而是需要为新的行寻找合适的位置——通常是已有数据的中间位置——并且分配空间。这会增加很多的额外工作，并导致数据分布不够优化。因为写入是乱序的，InnoDB不得不频繁地做行数据移动和页分裂操作，由于频繁的页分裂，页会变得稀疏并被不规则地填充，所以最终数据会有碎片。 3.3 顺序主键的问题对于高并发工作负载，在InnoDB中按主键顺序插入可能会造成明显的争用。主键的上界会成为“热点”。因为所有的插入都发生在这里，所以并发插入可能导致间隙锁竞争。另一个热点可能是AUTO_INCREMENT锁机制；如果遇到这个问题，则可能需要考虑重新设计表或者应用，或者更改innodb_autoinc_lock_mode配置。如果你的服务器版本还不支持innodb_autoinc_lock_mode参数，可以升级到新版本的InnoDB，可能对这种场景会工作得更好。 4.B-Tree 索引的使用技巧4.1 索引列顺序正确的顺序依赖于使用该索引的查询，并且同时需要考虑如何更好地满足排序和分组的需要。 对于如何选择索引的列顺序有一个经验法则：将选择性最高的列放到索引最前列。这在某些场景可能有帮助，但通常不如避免随机IO和排序那么重要。 当不需要考虑排序和分组时，将选择性最高的列放在前面通常是很好的，然而，性能不只是依赖于所有索引列的选择性（整体基数），也和查询条件的具体值有关，也就是和值的分布有关。可能需要根据那些运行频率最高的查询来调整索引列的顺序。对于索引列顺序的选择，与索引前缀的选择考虑是类似的。以下面的查询为例: 1234567891011121314SELECT * FROM payment WHERE staff_id = 2 AND customer_id = 584;# 1. 首先要考虑，staff_id, customer_id 的选择性mysql&gt; SELECT COUNT(DISTINCT staff_id)/COUNT(*) AS staff_id_selectivity, &gt; COUNT(DISTINCT customer_id)/COUNT(*) AS customer_id_selectivity, &gt; COUNT(*) &gt; FROM payment\\G*************************** 1. row *************************** staff_id_selectivity: 0.0001customer_id_selectivity: 0.0373 COUNT(*): 16049# 2. 考虑数据分布，及热点数据对索引性能的影响# 3. 从诸如pt-query-digest这样的工具的报告中提取“最差”查询，运行频率最高的查询，进行定向优化 最后，尽管关于选择性和基数的经验法则值得去研究和分析，但一定要记住别忘了WHERE子句中的排序、分组和范围条件等其他因素，这些因素可能对查询的性能造成非常大的影响。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"高性能的MySQL","slug":"高性能的MySQL","permalink":"http://yoursite.com/tags/高性能的MySQL/"}]},{"title":"4 索引基本原理","slug":"mysql/高性能的mysql/04_索引基本原理","date":"2019-10-03T16:00:00.000Z","updated":"2020-05-24T15:44:36.923Z","comments":true,"path":"2019/10/04/mysql/高性能的mysql/04_索引基本原理/","link":"","permalink":"http://yoursite.com/2019/10/04/mysql/高性能的mysql/04_索引基本原理/","excerpt":"mysql 索引的基本原理","text":"mysql 索引的基本原理 1. 前言索引对性能至关重要，接下来的 3 篇文章我们将会讲解索引的相关内容。本文是第一篇有关的索引的基本原理。在MySQL中，索引是在存储引擎层而不是服务器层实现的。所以，并没有统一的索引标准，每一种存储引擎都有自己的索引实现。常见的索引类型有如下几种: B-Tree索引 哈希索引 空间数据索引（R-Tree） 全文索引 我们将分别介绍他们的实现原理和适用的查询类型 2. B-Tree 索引B-Tree 索引顾名思义就是使用 B-Tree 结构实现的索引。实际上大多数存储引擎包括 Innodb 使用的是 B+ 树。下面是 B+ 数索引的结构示意图: B+树索引 一个 B+ 树在结构上有如下一些特点: 一个 B+ 树是一个平衡多叉树，每一个叶子页到根的距离相同 叶子节点除了被索引的列，还保存了对应行的引用，在 Innodb 中保存的就是对应的行的主键 索引列在每个叶子节点中都是顺序保存的 每个叶子节点都包含指向下一个叶子节点的指针，便于进行范围查找 B+ 索引的顺序存储特性，使得 B-Tree索引适用于全键值、键值范围或键前缀查找。其中键前缀查找只适用于根据最左前缀的查找。因为索引树中的节点是有序的，所以除了按值查找之外，索引还可以用于查询中的ORDER BY和GROUP BY操作。 因为最左前缀匹配，如果不是按照索引的最左列开始查找，则无法使用索引，同时也不能跳过索引中的列使用索引。因此索引列的顺序变得至关重要。如果查询中有某个列的范围查询，则其右边所有列都无法使用索引优化查找。如果范围查询列值的数量有限，那么可以通过使用多个等于条件来代替范围条件。 3. 哈希索引哈希索引即基于哈希实现的索引。对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码（hash code）。哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。 哈希的特性是只支持精确匹配，因此哈希索引有如下一些限制: 哈希索引只包含哈希值和行指针，而不存储字段值，因此不存在覆盖索引 哈希索引数据并不是按照索引值顺序存储的，所以也就无法用于排序 不支持索引列部分匹配，只支持等值比较查询，不支持范围查询 在MySQL中，只有Memory引擎显式支持哈希索引。值得一提的是，Memory引擎是支持非唯一哈希索引的，这是与众不同的。除了Memory引擎外，NDB集群引擎也支持唯一哈希索引。当InnoDB注意到某些索引值被使用得非常频繁时，它会在内存中基于B-Tree索引之上再创建一个哈希索引。这是一个完全自动的、内部的行为，用户无法控制或者配置，不过如果有必要，完全可以关闭该功能。 可以在 Innodb 中模拟哈希索引，还是使用B-Tree进行查找，但是它使用哈希值而不是键本身进行索引查找。例如需要存储大量的URL，并需要根据URL进行搜索查找。如果使用B-Tree来存储URL，存储的内容就会很大，因为URL本身都很长。可以删除原来URL列上的索引，而新增一个被索引的url_crc列，使用CRC32做哈希，就可以使用下面的方式查询： 1234567891011121314151617181920212223242526272829# 1. 添加一个 CRC32(url_crc) 的 B-Tree 索引CREATE TABLE pseudohash ( id int unsigned NOT NULL auto_increment, url varchar(255) NOT NULL, url_crc int unsigned NOT NULL DEFAULT 0, PRIMARY KEY(id), INDEX(url_crc));# 2. 查询必须待上原值，因为哈希可能存在哈希冲突mysql&gt; SELECT id FROM url WHERE url=\"http://www.mysql.com\" -&gt; AND url_crc=CRC32(\"http://www.mysql.com\");# 3. 使用触发器实现 url_crc 的维护DELIMITER // # 先临时修改一下语句分隔符，这样就可以在触发器定义中使用分号：CREATE TRIGGER pseudohash_crc_ins BEFORE INSERT ON pseudohash FOR EACH ROW BEGINSET NEW.url_crc=crc32(NEW.url);END;//CREATE TRIGGER pseudohash_crc_upd BEFORE UPDATE ON pseudohash FOR EACH ROW BEGINSET NEW.url_crc=crc32(NEW.url);END;//DELIMITER ; 注意如果采用这种方式，记住不要使用SHA1()和MD5()作为哈希函数。因为这两个函数计算出来的哈希值是非常长的字符串，会浪费大量空间，比较时也会更慢。还可以使用如FNV64()函数作为哈希，哈希值为64位，速度快，且冲突比CRC32()要少很多。 4. 其他索引类别mysql 还有很多存储引擎使用不同的数据结构来存储索引，包括: 全文索引，查找的是文本中的关键词，而不是直接比较索引中的值 TokuDB使用分形树索引，既有B-Tree的很多优点，也避免了B-Tree的一些缺点 5. 索引的优点索引有众多优点，总结下来包括如下三个：1.索引大大减少了服务器需要扫描的数据量 – 快速定位行位置2.索引可以帮助服务器避免排序和临时表 – B-Tree 索引的有序存储，可以实现 ORDER BY和GROUP BY3.索引可以将随机I/O变为顺序I/O – 覆盖索引优化 Lahdenmaki和Leach在书中介绍了如何评价一个索引是否适合某个查询的“三星系统”（three-star system）: 索引将相关的记录放到一起则获得一星 如果索引中的数据顺序和查找中的排列顺序一致则获得二星 如果索引中的列包含了查询中需要的全部列则获得“三星” 6. 索引的适用索引并不总是最好的工具。总的来说，只有当索引帮助存储引擎快速查找到记录带来的好处大于其带来的额外工作时，索引才是有效的。对于中到大型的表，索引非常有效。但对于特大型的表，建立和使用索引的代价将随之增长。这种情况下，则需要一种技术可以直接区分出查询需要的一组数据，而不是一条记录一条记录地匹配，例如可以使用分区技术。 如果表的数量特别多，可以建立一个元数据信息表，用来查询需要用到的某些特性。对于TB级别的数据，定位单条记录的意义不大，所以经常会使用块级别元数据技术来替代索引。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"高性能的MySQL","slug":"高性能的MySQL","permalink":"http://yoursite.com/tags/高性能的MySQL/"}]},{"title":"3 Schema 设计","slug":"mysql/高性能的mysql/03_schema设计","date":"2019-10-02T16:00:00.000Z","updated":"2020-05-24T15:44:32.467Z","comments":true,"path":"2019/10/03/mysql/高性能的mysql/03_schema设计/","link":"","permalink":"http://yoursite.com/2019/10/03/mysql/高性能的mysql/03_schema设计/","excerpt":"mysql 的 Schema 设计","text":"mysql 的 Schema 设计 1. 前言上一节我们简述了 MySQL 中的数据类型，本节我们来说一说 MySQL 的 Schema 设计，包括以下内容: MySQL 特有的 Schema 设计问题，这是由 MySQL 特定的实现机制导致的 Schema 设计的范式和反范式 为提升读性能，而常采用的技巧，缓存表和汇总表 最后我们会简单说一下加快 ALTER TABLE 操作速度的方法。下面大部分内容摘自：《高性能MySQL》 — 〔美〕施瓦茨 (Baron Schwartz) 〔美〕扎伊采夫 (Peter Zaitsev) 〔美〕特卡琴科 (Vadim Tkachenko)在豆瓣阅读书店查看：https://read.douban.com/ebook/35648568/ 2. MySQL schema设计中的陷阱有一些问题是由MySQL的实现机制导致的，我们需要避免下面只会在 MySQL 中发生的特定问题: 太多的列 MySQL的存储引擎，需要在服务器层和存储引擎层之间通过行缓冲格式拷贝数据，然后在服务器层将缓冲内容解码成各个列 从行缓冲中将编码过的列转换成行数据结构的代价依赖于列的数量 如果计划使用数千个字段，必须意识到服务器的性能运行特征会有一些不同。 太多的关联 MySQL限制了每个关联操作最多只能有61张表 事实上在许多关联少于61张表的情况下，解析和优化查询的代价也会成为MySQL的问题 经验法则，单个查询最好在12个表以内做关联 NULL 的使用: 避免使用NULL的好处，但是也不能走极端 在一些场景中，使用NULL可能会比某个神奇常数更好，以避免引入Bug 伪造的全0值可能导致很多问题（可以配置MySQL的SQL_MODE来禁止不可能的日期） MySQL会在索引中存储NULL值，而Oracle则不会 2. 范式和反范式范式和反范式反映的是 MySQL 数据冗余程度。在范式化的数据库中，每个事实数据会出现并且只出现一次。相反，在反范式化的数据库中，信息是冗余的，可能会存储在多个地方。范式和反范式有他们各自的有缺点。 2.1 范式的优缺点范式，特别适用于写密集的场景，原因在于: 当数据较好地范式化时，就只有很少或者没有重复数据，所以只需要修改更少的数据 很少有多余的数据意味着检索列表数据时更少需要DISTINCT或者GROUP BY语句 范式化的缺点是通常需要关联。这不但代价昂贵，也可能使一些索引策略无效。例如，范式化可能将列存放在不同的表中，而这些列如果在一个表中本可以属于同一个索引。 2.2 反范式的优点和缺点反范式化的schema因为所有数据都在一张表中，所以有下面这些优点: 可以很好地避免关联 当数据比内存大时这可能比关联要快得多，因为这样避免了随机I/O 单独的表也能使用更有效的索引策略。 2.3 混用范式化和反范式化范式化和反范式化的schema各有优劣，在实际应用中经常需要混用，可能使用部分范式化的schema、缓存表，以及其他技巧。最常见的反范式化数据的方法是复制或者缓存，在不同的表中存储相同的特定列。在MySQL 5.0和更新版本中，可以使用触发器更新缓存值，这使得实现这样的方案变得更简单。另一个从父表冗余一些数据到子表的理由是排序的需要。 3. 缓存表和汇总表为了提升读查询的速度，经常会需要建一些额外索引，增加冗余列，甚至是创建缓存表和汇总表。下面这方面技巧的专用术语: 缓存表: 来表示存储那些可以比较简单地从schema其他表获取（但是每次获取的速度比较慢）数据的表 汇总表: 用来保存使用GROUP BY语句聚合数据的表 物化视图: 实际上是预先计算并且存储在磁盘上的表，可以通过各种各样的策略刷新和更新 计数器表: 专用来计数的表 下面我们来一一介绍这些使用技巧。 3.1 汇总表以网站为例，假设需要计算之前24小时内发送的消息数，我们可以每小时生成一张汇总表，并使用下面的 SQL 进行计算:1234567891011121314151617CREATE TABLE msg_per_hr ( hr DATETIME NOT NULL, cnt INT UNSIGNED NOT NULL, PRIMARY KEY(hr));mysql&gt; SELECT SUM(cnt) FROM msg_per_hr -&gt; WHERE hr BETWEEN -&gt; CONCAT(LEFT(NOW(), 14), '00:00') - INTERVAL 23 HOUR -&gt; AND CONCAT(LEFT(NOW(), 14), '00:00') - INTERVAL 1 HOUR;mysql&gt; SELECT COUNT(*) FROM message -&gt; WHERE posted &gt;= NOW() - INTERVAL 24 HOUR -&gt; AND posted &lt; CONCAT(LEFT(NOW(), 14), '00:00') - INTERVAL 23 HOUR;mysql&gt; SELECT COUNT(*) FROM message -&gt; WHERE posted &gt;= CONCAT(LEFT(NOW(), 14), '00:00'); 3.2 缓存表缓存表的一个有用的技巧是对缓存表使用不同的存储引擎例如，如果主表使用InnoDB，用MyISAM作为缓存表的引擎将会得到更小的索引占用空间，并且可以做全文搜索。有时甚至可以把整个表导出MySQL，插入到专门的搜索系统中获得更高的搜索效率。 在使用缓存表和汇总表时，必须决定是实时维护数据还是定期重建。哪个更好依赖于应用程序，但是定期重建并不只是节省资源，也可以保持表不会有很多碎片，以及有完全顺序组织的索引（这会更加高效）。 当重建汇总表和缓存表时，通常需要保证数据在操作时依然可用。这就需要通过使用“影子表”来实现，下面是影子表的使用技巧: 1234mysql&gt; DROP TABLE IF EXISTS my_summary_new, my_summary_old;mysql&gt; CREATE TABLE my_summary_new LIKE my_summary;-- populate my_summary_new as desiredmysql&gt; RENAME TABLE my_summary TO my_summary_old, my_summary_new TO my_summary; 3.3 物化视图(额外学习)。MySQL并不原生支持物化视图。可以使用开源工具Flexviews 在 MySQL中实现物化视图。 对比传统的维护汇总表和缓存表的方法，Flexviews通过提取对源表的更改，可以增量地重新计算物化视图的内容。这意味着不需要通过查询原始数据来更新视图。 3.2 计数器表计数器表可以统计计数，但是计数器表可能会碰到并发问题。 要获得更高的并发更新性能，一种方案是可以将计数器保存在多行中，每次随机选择一行进行更新。像下面这样 123456789# 1. 创建如下计数表mysql&gt; CREATE TABLE hit_counter ( -&gt; slot tinyint unsigned not null primary key, -&gt; cnt int unsigned not null -&gt; ) ENGINE=InnoDB;# 2. 预先增加100行数据# 3. 现在选择一个随机的槽（slot）进行更新mysql&gt; UPDATE hit_counter SET cnt = cnt + 1 WHERE slot = RAND() * 100; 如果像每天开始一个计数器，可以像下面这样:123456789101112131415161718192021222324# 1. 创建下表mysql&gt; CREATE TABLE daily_hit_counter ( -&gt; day date not null, -&gt; slot tinyint unsigned not null, -&gt; cnt int unsigned not null, -&gt; primary key(day, slot) -&gt; ) ENGINE=InnoDB;# 2. 使用 ON DUPLICATE 进行更新mysql&gt; INSERT INTO daily_hit_counter(day, slot, cnt) -&gt; VALUES(CURRENT_DATE, RAND() * 100, 1) -&gt; ON DUPLICATE KEY UPDATE cnt = cnt + 1;# 3. 希望减少表的行数，避免表变大，可以写一个周期执行的任务，# 合并所有结果到0号槽，并且删除所有其他的槽：mysql&gt; UPDATE daily_hit_counter as c -&gt; INNER JOIN ( -&gt; SELECT day, SUM(cnt) AS cnt, MIN(slot) AS mslot -&gt; FROM daily_hit_counter -&gt; GROUP BY day -&gt; ) AS x USING(day) -&gt; SET c.cnt = IF(c.slot = x.mslot, x.cnt, 0), -&gt; c.slot = IF(c.slot = x.mslot, 0, c.slot);mysql&gt; DELETE FROM daily_hit_counter WHERE slot &lt;&gt; 0 AND cnt = 0; 4. 加速ALTER TABLE操作MySQL的ALTER TABLE操作的性能对大表来说是个大问题。MySQL执行大部分修改表结构操作的方法是用新的结构创建一个空表，从旧表中查出所有数据插入新表，然后删除旧表。这样操作可能需要花费很长时间，如果内存不足而表又很大，而且还有很多索引的情况下尤其如此。 大部分ALTER TABLE操作将导致MySQL服务中断，常见场景中，避免中断的技巧只有两个: 一种是先在一台不提供服务的机器上执行ALTER TABLE操作，然后和提供服务的主库进行切换 另外一种技巧是“影子拷贝”。影子拷贝的技巧是用要求的表结构创建一张和源表无关的新表 一些工具可以帮助完成影子拷贝工作： Facebook数据库运维团队的“online schema change”工具 Shlomi Noach的openark toolkit Percona Toolkit 如果使用Flexviews，也可以通过其CDC工具执行无锁的表结构变更 一些特殊的 ALTER TABLE 操作则有特殊的优化技巧。 4.1 更改列的默认值默认像下面更改列的默认值会导致表重建:12345# 1. 所有的MODIFY COLUMN操作都将导致表重建。mysql&gt; ALTER TABLE sakila.film -&gt; MODIFY COLUMN rental_duration TINYINT(3) NOT NULL DEFAULT 5;mysql&gt; SHOW STATUS显 理论上，MySQL可以跳过创建新表的步骤。列的默认值实际上存在表的.frm文件中，所以可以直接修改这个文件而不需要改动表本身。另外一种方法是通过ALTER COLUMN操作来改变列的默认值： 123mysql&gt; ALTER TABLE sakila.film -&gt; ALTER COLUMN rental_duration SET DEFAULT 5; 这个语句会直接修改.frm文件而不涉及表数据。所以，这个操作是非常快的。ALTER TABLE允许使用ALTER COLUMN、MODIFY COLUMN和CHANGE COLUMN语句修改列。这三种操作都是不一样的。 4.2 只修改.frm文件下面要演示的技巧是不受官方支持的，也没有文档记录，并且也可能不能正常工作，采用这些技术需要自己承担风险。建议在执行之前首先备份数据！下面这些操作是有可能不需要重建表的： 移除（不是增加）一个列的AUTO_INCREMENT属性。 增加、移除，或更改ENUM和SET常量。如果移除的是已经有行数据用到其值的常量，查询将会返回一个空字串值。 基本的技术是为想要的表结构创建一个新的.frm文件，然后用它替换掉已经存在的那张表的.frm文件，像下面这样： 创建一张有相同结构的空表，并进行所需要的修改（例如增加ENUM常量）。 执行FLUSH TABLES WITH READ LOCK 这将会关闭所有正在使用的表，并且禁止任何表被打开 交换.frm文件 执行UNLOCK TABLES来释放第2步的读锁 4.3 快速创建MyISAM索引为了高效地载入数据到MyISAM表中，有一个常用的技巧是先禁用索引、载入数据，然后重新启用索引： 123mysql&gt; ALTER TABLE test.load_data ENABLE KEYS;-- load the datamysql&gt; ALTER TABLE test.load_data ENABLE KEYS; 这个技巧能够发挥作用，是因为构建索引的工作被延迟到数据完全载入以后，这个时候已经可以通过排序来构建索引了。这样做会快很多，并且使得索引树的碎片更少、更紧凑。 这个办法对唯一索引无效，因为DISABLE KEYS只对非唯一索引有效。MyISAM会在内存中构造唯一索引，并且为载入的每一行检查唯一性。一旦索引的大小超过了有效内存大小，载入操作就会变得越来越慢。 在现代版本的InnoDB版本中，有一个类似的技巧，这依赖于InnoDB的快速在线索引创建功能。这个技巧是，先删除所有的非唯一索引，然后增加新的列，最后重新创建删除掉的索引。Percona Server可以自动完成这些操作步骤。 也可以使用像前面说的ALTER TABLE的骇客方法来加速这个操作，但需要多做一些工作并且承担一定的风险。这对从备份中载入数据是很有用的，例如，当已经知道所有数据都是有效的并且没有必要做唯一性检查时就可以这么来操作。下面是操作步骤： 用需要的表结构创建一张表，但是不包括索引。 载入数据到表中以构建.MYD文件。 按照需要的结构创建另外一张空表，这次要包含索引。这会创建需要的.frm和.MYI文件。 获取读锁并刷新表。 重命名第二张表的.frm和.MYI文件，让MySQL认为是第一张表的文件。 释放读锁。 使用REPAIR TABLE来重建表的索引。该操作会通过排序来构建所有索引，包括唯一索引。这个操作步骤对大表来说会快很多。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"高性能的MySQL","slug":"高性能的MySQL","permalink":"http://yoursite.com/tags/高性能的MySQL/"}]},{"title":"2 MySQL 数据类型","slug":"mysql/高性能的mysql/02_mysql数据类型","date":"2019-10-01T16:00:00.000Z","updated":"2020-05-24T15:44:28.459Z","comments":true,"path":"2019/10/02/mysql/高性能的mysql/02_mysql数据类型/","link":"","permalink":"http://yoursite.com/2019/10/02/mysql/高性能的mysql/02_mysql数据类型/","excerpt":"mysql 的数据类型。","text":"mysql 的数据类型。 1. 前言本章是mysql 的数据类型方面的内容。这一部分内容大家可能都比较熟悉，但是其中的一些细节依然值得一在强调。 本文大部分内容摘自：《高性能MySQL》 — 〔美〕施瓦茨 (Baron Schwartz) 〔美〕扎伊采夫 (Peter Zaitsev) 〔美〕特卡琴科 (Vadim Tkachenko)可在豆瓣阅读书店查看：https://read.douban.com/ebook/35648568/ 2. 数据类型在详细介绍具体的数据类型之前，简单说一下选择最优的数据类型的几个简单原则: 更小的通常更好 应该尽量使用可以正确存储数据的最小数据类型 但是要确保没有低估需要存储的值的范围 简单就好 简单数据类型的操作通常需要更少的CPU周期 尽量避免NULL 通常情况下最好指定列为NOT NULL，除非真的需要存储NULL值。 如果查询中包含可为NULL的列，对MySQL来说更难优化 因为可为NULL的列使得索引、索引统计和值比较都更复杂 可为NULL的列会使用更多的存储空间，在MySQL里也需要特殊处理 如果计划在列上建索引，就应该尽量避免设计成可为NULL的列。 InnoDB是一个例外，它使用单独的位（bit）存储NULL值，所以对于稀疏数据有很好的空间效率 与VARCHAR（5）相比 更长的列 VARCHAR（5）会消耗更多的内存，因为MySQL通常会分配固定大小的内存块来保存内部值。尤其是使用内存临时表进行排序或操作时会特别糟糕。在利用磁盘临时表进行排序时也同样糟糕。所以最好的策略是只分配真正需要的空间。 在为列选择数据类型时，第一步需要确定合适的大类型：数字、字符串、时间等。下一步是选择具体类型，相同大类型的不同子类型数据有时也有一些特殊的行为和属性。接下来我们就简要说明一下 MySQL 中基本类型的一些重要特性。 2.1 整数整数类型有：TINYINT，SMALLINT，MEDIUMINT，INT，BIGINT。分别使用8，16，24，32，64位存储空间，可以存储−2^{（N−1）}到2^{（N−1）}−1 的值，其中N是存储空间的位数。MySQL可以为整数类型指定宽度，但它不会限制值的合法范围，对于存储和计算来说，INT（1）和INT（20）是相同的。整数的类型只影响存储，整数计算一般使用64位的BIGINT整数。 2.2 实数FLOAT和DOUBLE类型支持使用标准的浮点运算进行近似计算。DECIMAL类型用于存储精确的小数。在MySQL 5.0和更高版本，DECIMAL类型支持精确计算。因为CPU不支持对DECIMAL的直接计算，而支持原生浮点计算，所以FLOAT和DOUBLE比 DECIMAL 计算快。 MySQL 5.0和更高版本将 DECIMAL 打包保存到一个二进制字符串中（每4个字节存9个数字）。例如，DECIMAL（18,9）小数点两边将各存储9个数字，一共使用9个字节：小数点前的数字用4个字节，小数点后的数字用4个字节，小数点本身占1个字节。MySQL 5.0和更高版本中的DECIMAL类型允许最多65个数字。 有多种方法可以指定浮点列所需要的精度，但是浮点数的精度定义是非标准的，所以建议只指定数据类型，不指定精度。和整数类型一样，能选择的只是浮点数的存储类型；MySQL使用DOUBLE作为内部浮点计算的类型。 2.2 字符串从MySQL 字符串类型有很多变种，包括: VARCHAR和CHAR: 两种最主要的字符串类型。 BINARY和VARBINARY: 存储的是二进制字符串，二进制字符串存储的是字节码而不是字符 BLOB和TEXT: 为存储很大的数据而设计的字符串数据类型，分别为二进制和字符 VARCHAR 和 CHAR很难精确地解释VARCHAR和CHAR是怎么存储在磁盘和内存中的，因为这跟存储引擎的具体实现有关。下面论述以InnoDB和/或者MyISAM为基础。需要注意的是存储引擎存储CHAR或者VARCHAR值的方式在内存中和在磁盘上可能不一样，所以MySQL服务器从存储引擎读出的值可能需要转换为另一种存储格式。下面 VARCHAR 与 CHAR 的典型对比 VARCHAR: 存储可变长字符串，需要使用1或2个额外字节记录字符串的长度，列的最大长度小于或等于255字节，则只使用1个字节表示，否则使用2个字节 VARCHAR节省了存储空间，但在UPDATE时可能使行变得比原来更长时需要做额外的工作，如果一个行占用的空间增长，并且在页内没有更多的空间可以存储，则需要进一步处理(不同的存储引擎处理方式不一样) VARCHAR 节省存储空间有一个例外情况，即 MySQL 表使用ROW_FORMAT=FIXED创建，此时每一行都会使用定长存储 在5.0或者更高版本，MySQL在存储和检索时会保留末尾空格。但在4.1或更老的版本，MySQL会剔除末尾空格 CHAR: CHAR类型是定长的 当存储CHAR值时，MySQL会删除所有的末尾空格 CHAR适合存储很短的字符串，或者所有值都接近同一个长度 对于经常变更的数据，CHAR也比VARCHAR更好，因为定长的CHAR类型不容易产生碎片 对于非常短的列，CHAR比VARCHAR在存储空间上也更有效率 BINARY和VARBINARYMySQL填充BINARY采用的是\\0（零字节）而不是空格，在检索时也不会去掉填充值 当需要存储二进制数据，并进行二进制比较时 BINARY和VARBINARY 非常有用。二进制比较的优势并不仅仅体现在大小写敏感上。MySQL比较BINARY字符串时，每次按一个字节，并且根据该字节的数值进行比较。因此，二进制比较比字符比较简单很多，所以也就更快 BLOB 和 TEXTMySQL把每个BLOB和TEXT值当作一个独立的对象处理。当BLOB和TEXT值太大时，InnoDB会使用专门的“外部”存储区域来进行存储，此时每个值在行内需要1～4个字节存储一个指针，然后在外部存储区域存储实际的值。 BLOB和TEXT家族之间仅有的不同是BLOB类型存储的是二进制数据，没有排序规则或字符集，而TEXT类型有字符集和排序规则。 MySQL对BLOB和TEXT列进行排序与其他类型是不同的：它只对每个列的最前max_sort_length字节而不是整个字符串做排序。如果只需要排序前面一小部分字符，则可以减小max_sort_length的配置，或者使用ORDER BY SUSTRING（column，length）。 MySQL不能将BLOB和TEXT列全部长度的字符串进行索引，也不能使用这些索引消除排序。 因为Memory引擎不支持BLOB和TEXT类型，所以，如果查询使用了BLOB或TEXT列并且需要使用隐式临时表，将不得不使用MyISAM磁盘临时表，即使只有几行数据也是如此。这会导致严重的性能开销。即使配置MySQL将临时表存储在内存块设备上（RAM Disk），依然需要许多昂贵的系统调用。 最好的解决方案是尽量避免使用BLOB和TEXT类型。如果实在无法避免，有一个技巧是在所有用到BLOB字段的地方都使用SUBSTRING（column，length）将列值转换为字符串（在ORDER BY子句中也适用），这样就可以使用内存临时表了。但是要确保截取的子字符串足够短，不会使临时表的大小超过max_heap_table_size或tmp_table_size，超过以后MySQL会将内存临时表转换为MyISAM磁盘临时表。最坏情况下的长度分配对于排序的时候也是一样的，所以这一招对于内存中创建大临时表和文件排序，以及在磁盘上创建大临时表和文件排序这两种情况都很有帮助。 2.3 枚举（ENUM）类型MySQL在存储枚举时非常紧凑，会根据列表值的数量压缩到一个或者两个字节中。MySQL在内部会将每个值在列表中的位置保存为整数，并且在表的.frm文件中保存“数字-字符串”映射关系的“查找表”。 但是让人吃惊的地方是，枚举字段是按照内部存储的整数而不是定义的字符串进行排序的。一种绕过这种限制的方式是按照需要的顺序来定义枚举列。另外也可以在查询中使用FIELD()函数显式地指定排序顺序，但这会导致MySQL无法利用索引消除排序。 枚举最不好的地方是，字符串列表是固定的，添加或删除字符串必须使用ALTER TABLE。因此，对于一系列未来可能会改变的字符串，使用枚举不是一个好主意，除非能接受只在列表末尾添加元素，这样在MySQL 5.1中就可以不用重建整个表来完成修改。 把列都转换成ENUM以后，关联变得很快。在特定情况下，把CHAR/VARCHAR列与枚举列进行关联可能会比直接关联CHAR/VARCHAR列更慢。这是一个通用的设计实践，在“查找表”时采用整数主键而避免采用基于字符串的值进行关联。 使用 ENUM 替换字符串类型最大好处是，可以使得表更小，相比于字符串主键，转换后主键以及依赖于主键的非主键索引也会更小。 2.4 日期和时间MySQL可以使用许多类型来保存日期和时间值，例如YEAR和DATE。大部分时间类型都没有替代品，因此没有什么是最佳选择的问题。唯一的问题是如何保存日期和时间，MySQL提供两种相似的日期类型：DATETIME和TIMESTAMP。 DATETIME: 这个类型能保存大范围的值，从1001年到9999年，精度为秒 它把日期和时间封装到格式为YYYYMMDDHHMMSS的整数中，与时区无关。使用8个字节的存储空间 TIMESTAMP UNIX时间戳，只使用4个字节的存储空间，只能表示从1970年到2038年 FROM_UNIXTIME()把Unix时间戳转换为日期，UNIX_TIMESTAMP()把日期转换为Unix时间戳 TIMESTAMP显示的值也依赖于时区，MySQL服务器、操作系统，以及客户端连接都有时区设置 默认情况下，如果插入时没有指定第一个TIMESTAMP列的值，MySQL则设置这个列的值为当前时间 在更新一行记录时，MySQL默认也会更新第一个TIMESTAMP列的值 可以配置任何TIMESTAMP列的插入和更新行为 最后，TIMESTAMP列默认为NOT NULL，这也和其他的数据类型不一样 除了特殊行为之外，通常也应该尽量使用TIMESTAMP，因为它比DATETIME空间效率更高。有时候人们会将Unix时间截存储为整数值，但这不会带来任何收益。用整数保存时间截的格式通常不方便处理。 MySQL能存储的最小时间粒度为秒，如果需要存储比秒更小粒度的日期和时间值可以使用 BIGINT 模拟也可以改使用 MariaDB （MariaDB支持微秒级别的时间类型）。 2.5 位数据类型MySQL有少数几种存储类型使用紧凑的位存储数据。所有这些位类型，不管底层存储格式和处理方式如何，从技术上来说都是字符串类型。 BIT:可以使用BIT列在一列中存储一个或多个true/false值，最大长度是64个位。 BIT的行为因存储引擎而异。MyISAM会打包存储所有的BIT列，所以17个单独的BIT列只需要17个位存储（假设没有可为NULL的列），这样MyISAM只使用3个字节就能存储这17个BIT列。其他存储引擎例如Memory和InnoDB，为每个BIT列使用一个足够存储的最小整数类型来存放，所以不能节省存储空间。 MySQL把BIT当作字符串类型，而不是数字类型。但是 BIT 的使用依赖于其使用的上下文。例如，如果存储一个值b’00111001’（二进制值等于57）到BIT（8）的列并且检索它，得到的内容是字符码为57的字符串。也就是说得到ASCII码为57的字符“9”。但是在数字上下文场景中，得到的是数字57： 所以我们认为应该谨慎使用BIT类型。对于大部分应用，最好避免使用这种类型。 SETSET: 在MySQL内部是以一系列打包的位的集合,如果需要保存很多true/false值，可以考虑合并这些列到一个SET FIND_IN_SET()和FIELD()这样的函数，方便地在查询中使用 缺点是改变列的定义的代价较高：需要ALTER TABLE，一般来说，也无法在SET列上通过索引查找 一种替代SET的方式是使用一个整数包装一系列的位，并且按位操作来使用。好处是可以不使用ALTER TABLE改变字段代表的“枚举”值，缺点是查询语句更难写，并且更难理解。最后 MySQL在内部使用整数存储ENUM和SET类型，然后在做比较操作时转换为字符串。 2.6 选择标识符为标识列（identifier column）选择合适的数据类型非常重要，不仅仅需要考虑存储类型，还需要考虑MySQL对这种类型怎么执行计算和比较。一旦选定了一种类型，要确保在所有关联表中都使用同样的类型。类型之间需要精确匹配，包括像UNSIGNED这样的属性。混用不同数据类型可能导致性能问题，即使没有性能影响，在比较操作时隐式类型转换也可能导致很难发现的错误。下面是各类型作为标识符的适用情况: 整数: 通常是标识列最好的选择，很快并且可以使用AUTO_INCREMENT EMUM和SET: 通常是一个糟糕的选择，它们仅适合在存储固定信息作为标识列类型 字符串: 如果可能，应该避免使用字符串类型作为标识列，因为它们很消耗空间，并且通常比数字类型慢 MyISAM默认对字符串使用压缩索引，这会导致查询慢得多 特别的对于像 MD5() 生成的随机字符串，作为标识符需要注意。随机字符串分布在很大的空间内，会导致INSERT以及一些SELECT语句变得很慢。 因为插入值会随机地写到索引的不同位置，所以使得INSERT语句更慢。这会导致页分裂、磁盘随机访问，以及对于聚簇存储引擎产生聚簇索引碎片 SELECT语句会变得更慢，因为逻辑上相邻的行会分布在磁盘和内存的不同地方。 随机值导致缓存对所有类型的查询语句效果都很差，因为会使得缓存赖以工作的访问局部性原理失效。 如果存储UUID值，则应该移除“-”符号；更好的做法是，用UNHEX()函数转换UUID值为16字节的数字，并且存储在一个BINARY（16）列中。检索时可以通过HEX()函数来格式化为十六进制格式。 UUID()生成的值与加密散列函数例如SHA1()生成的值有不同的特征：UUID值虽然分布也不均匀，但还是有一定顺序的。尽管如此，但还是不如递增的整数好用。 2.7 IP 的存储应该用无符号整数存储IP地址。MySQL提供INET_ATON()和INET_NTOA()函数在这两种表示方法之间转换。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"高性能的MySQL","slug":"高性能的MySQL","permalink":"http://yoursite.com/tags/高性能的MySQL/"}]},{"title":"1 MYSQL EXPLAIN","slug":"mysql/高性能的mysql/01_explain","date":"2019-09-30T16:00:00.000Z","updated":"2020-05-24T15:44:24.159Z","comments":true,"path":"2019/10/01/mysql/高性能的mysql/01_explain/","link":"","permalink":"http://yoursite.com/2019/10/01/mysql/高性能的mysql/01_explain/","excerpt":"你以为很懂，其实一点也不懂的 mysql。","text":"你以为很懂，其实一点也不懂的 mysql。 1. mysql 开篇说来惭愧，相比于入程序员这行的时间，对 mysql 的了解太少了。接下来的两个月里，希望借助于 高性能的MySQL 一书和林晓斌老师的专栏 MySQL实战45讲 来系统的学习 MySQL。 在我们正式学习其他内容之前，我想先介绍一下如何调用“EXPLAIN”来获取关于查询执行计划的信息，以及如何解释输出。这将能帮助我们了解 mysql 在执行 sql 背后的每一步。我们将分成以下 4 个部分来介绍 EXPLAIN 的相关内容: EXPLAIN 的三种用法 EXPLAIN 的数据 EXPLAIN 的缺陷 MySQL5.6 对EXPLAIN 的改进 2. EXPLAIN2.1 EXPLAIN 用法EXPLAIN 有三种用法: EXPLAIN SELECT ...: 显示出执行计划中的每一部分和执行的次序 在查询中每个表在输出中只有一行,如果查询是两个表的联接，那么输出中将有两行 别名表单算为一个表，“表”的意义在这里相当广，可以是一个子查询，一个UNION结果，等等 输出中的行以MySQL实际执行的查询部分的顺序出现，而这个顺序不总是与其在原始SQL中的相一致，因为 MySQL查询优化器会优化 SQL 的执行顺序。 EXPLAIN EXTENDED SELECT ...: 看起来和正常的EXPLAIN的行为一样，但它会告诉服务器“逆向编译”执行计划为一个SELECT语句 可以通过紧接其后运行SHOW WARNINGS看到这个生成的语句。这个语句直接来自执行计划，而不是原SQL语句，到这点上已经变成一个数据结构。 EXPLAIN PARTITIONS: 会显示查询将访问的分区，如果查询是基于分区表的话 下面是使用EXPLAIN EXTENDED的一个示例: 1234567891011121314151617181920explain extended select * from floatTest \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: floatTest type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 2 filtered: 100.00 Extra:show warnings \\G*************************** 1. row *************************** Level: Note Code: 1003Message: select `enlightent_daily`.`floatTest`.`dd` AS `dd` from `enlightent_daily`.`floatTest` 需要注意的时，认为执行 EXPLAIN 时MySQL不会执行查询是错误。事实上，如果查询在FROM子句中包括子查询，那么MySQL实际上会执行子查询，将其结果放在一个临时表中，然后完成外层查询优化。 MySQL 必须在可以完成外层查询优化之前处理所有类似的子查询，这对于EXPLAIN来说是必须要做的。这意味着如果语句包含开销较大的子查询或使用临时表算法的视图，实际上会给服务器带来大量工作。这个限制将在 MySQL5.6 之后取消。 2.2 EXPLAIN 中的列要想明白 EXPLAIN 的输出，首先我们要明白EXPLAIN 中的列的含义，其次是每个列可能的取值范围，以及每个值代表的含义。下面 EXPLAIN 输出的每一列的含义: 列名 含义 id 标识SELECT所属的行，内层的SELECT语句一般会顺序编号，对应于其在原始语句中的位置 select_type 表示对应行是简单还是复杂查询，如果复杂查询对应的是哪种复杂查询 table 对应行正在访问的表名 type 访问类型,即MySQL决定如何查找表中的行 possible_keys 显示了查询可以使用哪些索引 key MySQL 决定采用哪个索引来优化对该表的访问，可以是不出现在 possible_keys 的索引 key_len MySQL在索引里使用的字节数，可以用这个值来算出具体是哪些列 ref 显示了在key列记录的索引中查找值所用的列或常量 rows MySQL估计为了找到所需的行而要读取的行数。这个数字是内嵌循环关联计划里的循环数目 filter 它显示的是针对表里符合某个条件（WHERE子句或联接条件）的记录数的百分比所做的一个悲观估算 Extra 不适合在其他列显示的额外信息 idid 标识的是SELECT出现的顺序(不是表出现的顺序)。MySQL将SELECT查询分为简单和复杂类型，复杂类型可分成三大类：简单子查询、所谓的派生表（在FROM子句中的子查询），以及UNION查询。FROM子句中的子查询和联合给id列增加了更多复杂性。 select_typeselect_type 表示查询的类型，有如下几种取值: SIMPLE: 意味着查询不包括子查询和UNION PRIMARY: 如果查询有任何复杂的子部分，则最外层部分标记为 PRIMARY,其他部分标记为下面几种类型 SUBQUERY: 表示在SELECT列表中的子查询中的SELECT（换句话说，不在FROM子句中） DERIVED: 表示包含在FROM子句的子查询中的SELECT UNION: UNION中的第二个和随后的SELECT被标记为UNION，SUBQUERY和UNION还可以被标记为DEPENDENT和UNCACHEABLE UNION RESULT: 表示用来从UNION的匿名临时表检索结果的SELECT DEPENDENT: 意味着SELECT依赖于外层查询中发现的数据 UNCACHEABLE: 意味着SELECT中的某些特性阻止结果被缓存于一个Item_cache中 tabletable 对应行正在访问的表名，可以在这一列中从上往下观察MySQL的关联优化器为查询选择的关联顺序。当FROM子句中有子查询或有UNION时，table列会变得复杂得多: 当在FROM子句中有子查询时，table列是的形式，其中N是子查询的id 当有UNION时，UNION RESULT的table列包含一个参与UNION的id列表 下面是一个复杂SELECT的示例12345678910111213141516171 EXPLAIN2 SELECT actor_id,3 (SELECT 1 FROM sakila.film_actor WHERE film_actor.actor_id =4 der_1.actor_id LIMIT 1)5 FROM (6 SELECT actor_id7 FROM sakila.actor LIMIT 58 ) AS der_19 UNION ALL10 SELECT film_id,11 (SELECT @var1 FROM sakila.rental LIMIT 1)12 FROM (13 SELECT film_id,14 (SELECT 1 FROM sakila.store LIMIT 1)15 FROM sakila.film LIMIT 516 ) AS der_2; typetype 表示 MySQL 决定如何查找表中的行，从最差到最优有如下几种取值: ALL: 全表扫描，通常意味着MySQL必须扫描整张表，从头到尾，去找到需要的行 有例外，如在查询里使用了LIMIT，或者在Extra列中显示“Using distinct/not exists” index: 这个跟全表扫描一样，只是MySQL扫描表时按索引次序进行而不是行，主要优点是避免了排序 缺点是要承担按索引次序读取整个表的开销。这通常意味着若是按随机次序访问行，开销将会非常大 如果在Extra列中看到“Using index”，说明MySQL正在使用覆盖索引，它只扫描索引的数据，而不是按索引次序的每一行 range： 范围扫描就是一个有限制的索引扫描，比全索引扫描好一些，因为它用不着遍历全部索引 当MySQL使用索引去查找一系列值时，例如IN()和OR列表，也会显示为范围扫描。然而，这两者其实是相当不同的访问类型，在性能上有重要的差异 ref: 一种索引访问（有时也叫做索引查找），它返回所有匹配某个单个值的行 只有当使用非唯一性索引或者唯一性索引的非唯一性前缀时才会发生 ref_or_null 是ref之上的一个变体，它意味着MySQL必须在初次查找的结果里进行第二次查找以找出NULL条目 eq_ref 一种索引访问，MySQL知道最多只返回一条符合条件的记录 在MySQL使用主键或者唯一性索引查找时发生 const, system 当MySQL能对查询的某部分进行优化并将其转换成一个常量时，就会使用这些访问类型 例如，如果你通过将某一行的主键放入WHERE子句里的方式来选取此行的主键，MySQL 就能把这个查询转换为一个常量。然后就可以高效地将表从联接执行中移除。 NULL 这种访问方式意味着MySQL能在优化阶段分解查询语句，在执行阶段甚至用不着再访问表或者索引 例如，从一个索引列里选取最小值可以通过单独查找索引来完成，不需要在执行时访问表。 possible_keyspossible_keys 显示了查询可以使用哪些索引,这个列表是在优化过程的早期，基于查询访问的列和使用的比较操作符创建的 key_lenMySQL在索引里使用的字节数， 它并不总显示一个索引真正使用了多少。例如，如果对一个前缀模式匹配执行LIKE查询，它会显示列的完全宽度正在被使用。 rowsrows 显示的行数不是MySQL认为它最终要从表里读取出来的行数，而是MySQL为了找到符合查询的每一点上标准的那些行而必须读取的行的平均数。根据表的统计信息和索引的选用情况，这个估算可能很不精确。在MySQL 5.0及更早的版本里，它也反映不出LIMIT子句。同时很多优化手段，例如关联缓冲区和缓存，无法影响到行数的显示。 filteredMySQL 5.1里新加进去的，在使用EXPLAIN EXTENDED时出现。如果你把rows列和这个百分比相乘，就能看到MySQL估算它将和查询计划里前一个表关联的行数。 ExtraExtra 是额外的提示信息，常见的最重要的值如下： Using index: MySQL将使用覆盖索引，以避免访问表 Using where: MySQL服务器将在存储引擎检索行后再进行过滤 不是所有带WHERE子句的查询都会显示“Using where”，因为在索引列上的 where在存储引擎就可以过滤 有时“Using where”的出现就是一个暗示：查询可受益于不同的索引。 Using temporary: MySQL在对查询结果排序时会使用一个临时表。 Using filesort: 这意味着MySQL会对结果使用一个外部索引排序，而不是按索引次序从表里读取行 MySQL有两种文件排序算法，两种方式都可以在内存或磁盘上完成。EXPLAIN不会告诉你MySQL将使用哪一种文件排序，也不会告诉你排序会在内存里还是磁盘上完成。 Range checked for each record (index map: N): 意味着没有好用的索引，新的索引将在联接的每一行上重新估算 N是显示在possible_keys列中索引的位图，并且是冗余的 3. EXPLAIN 缺陷EXPLAIN只是个近似结果，存在以下缺陷: EXPLAIN根本不会告诉你触发器、存储过程或UDF会如何影响查询。 它并不支持存储过程，尽管可以手动抽取查询并单独地对其进行EXPLAIN操作。 它并不会告诉你MySQL在查询执行中所做的特定优化。 它并不会显示关于查询的执行计划的所有信息 它并不区分具有相同名字的事物。例如，它对内存排序和临时文件都使用“filesort”，并且对于磁盘上和内存中的临时表都显示“Using temporary”。 可能会误导 MySQL EXPLAIN只能解释SELECT查询，并不会对存储程序调用和INSERT、UPDATE、DELETE或其他语句做解释。你可以重写某些非SELECT查询以利用EXPLAIN；MySQL5.6 之后将允许解释非 SELECT 操作 4. MySQL 5.6 对 EXPLAIN 的改进MySQL 5.6中将包括一些对EXPLAIN的重要改进： 能对类似UPDATE、INSERT等的查询进行解释。 允许匿名的临时表尽可能晚地被具体化，而不总是在优化和执行使用到此临时表的部分查询时创建并填充它们。这将允许MySQL可以直接解释带子查询的查询语句，而不需要先实际地执行子查询 这些改进可以帮助我们更好的查看MySQL的执行计划，最后使用 Percona Toolkit包含的 pt-visual-explain 工具可以以树形格式查看查询计划，更加直观容易理解。","categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"}],"tags":[{"name":"高性能的MySQL","slug":"高性能的MySQL","permalink":"http://yoursite.com/tags/高性能的MySQL/"}]},{"title":"3. 数据模型与查询语言","slug":"db/db_3","date":"2019-04-02T16:00:00.000Z","updated":"2020-05-24T15:33:44.854Z","comments":true,"path":"2019/04/03/db/db_3/","link":"","permalink":"http://yoursite.com/2019/04/03/db/db_3/","excerpt":"用于数据存储和查询的通用数据模型","text":"用于数据存储和查询的通用数据模型 1. 数据抽象上一节我们讲解了，在进行应用程序设计时，我们要达到的目标(可靠性，可扩展性和可维护性)，并精确的解释了这些术语的含义。下面我们就正式进入数据系统设计的细节。 需要强调的是在设计我们的应用时，应用的需求决定了我们要设计怎样的系统。而另一方面，我们的应用是建立在层层抽象的基础上的。只有系统提供的抽象与应用的需求相符时，我们才能达到设计可靠、可扩展、可维护应用的目标。因为我们设计的是数据密集型应用，因此书中将系统的抽象称为数据模型。 我们可以理解为多数应用是使用层层叠加的数据模型构建的，每层数据模型的关键问题是，如何用低一层数据模型来表示，而每一层都通过提供一个明确的数据模型来隐藏更低层次的复杂性。本节我们将研究一系列用于数据存储和查询的通用数据模型，包括: 关系模型 文档模型 基于图形的数据模型 通过本节的学习，你将明白主流数据库提供的数据模型的优劣，支持的查询语言以及它们的适用场景。 2. 数据模型目前大多数应用程序开发都使用面向对象的编程语言来开发，程序运行时，数据是程序中的对象，保存在程序的数据结构中，数据之间通过引用，可以进行任意关联。当数据被存储时，程序中的对象被转换成数据模型，数据结构之间的任意引用反映在模型构造的数据关系中。因此，对于数据而言，应用的需求就是如何更加便利的表示程序中的对象，并反应应用内数据之间的关系(一对一，一对多，多对一还是多对多)。 2.1 关系模型关系模型中，数据被组织成关系（SQL中称作表），其中每个关系是元组（SQL中称作行)的无序集合。在这种模型中: 表具有固定的模式，并通过外键和连接，来表示数据之间的关系 将应用程序中的对象保存在关系型数据库中的表，行，列时需要一个显示的中间转换层，虽然对象关系映射（object-relational mapping, ORM） 框架可以减少这个转换层所需的样板代码的数量，但是它们不能完全隐藏这两个模型之间的差异。 2.2 文档模型文档有多种形式，比如 xml，json，最常见属 json。 json 通过在其父记录中存储嵌套记录，而不是在单独的表中，可以很自然的表示一对多关系。这种自包含的文档比关系型中的多表模式具有更好的局部性。但是在文档数据库中，一对多树结构没有必要用连接，因此对连接的支持通常很弱。 这种局部性的性能优势，只有在应用程序经常需要访问整个文档时才会显现。只访问其中的一小部分，这对于大型文档来说是很浪费的。更新文档时，通常需要整个重写。只有不改变文档大小的修改才可以容易地原地执行。因此，通常建议保持相对小的文档，并避免增加文档大小的写入。这些性能限制大大减少了文档数据库的实用场景。 2.3 模型比较支持文档数据模型的主要论据是架构灵活性，因局部性而拥有更好的性能，以及对于某些应用程序而言更接近于应用程序使用的数据结构。关系模型通过为连接提供更好的支持以及支持多对一和多对多的关系来反击。 如果应用程序中的数据具有类似文档的结构（即，一对多关系树，通常一次性加载整个树） ，那么使用文档模型可能是一个好主意。将类似文档的结构分解成多个表的关系技术可能导致繁琐的模式和不必要的复杂的应用程序代码。文档模型有一定的局限性：例如，不能直接引用文档中的嵌套的项目，而是需要说“用户251的位置列表中的第二项”。但是，只要文件嵌套不太深，这通常不是问题。 文档数据库对连接的糟糕支持也许或也许不是一个问题，这取决于应用程序。例如，分析应用程可能永远不需要多对多的关系，如果它使用文档数据库来记录何事发生于何时但是，如果你的应用程序确实使用多对多关系，那么文档模型就没有那么吸引人了。通过反规范化可以减少对连接的需求，但是应用程序代码需要做额外的工作来保持数据的一致性。通过向数据库发出多个请求，可以在应用程序代码中模拟连接，但是这也将复杂性转移到应用程序中，并且通常比由数据库内的专用代码执行的连接慢。在这种情况下，使用文档模型会导致更复杂的应用程序代码和更差的性能 对于高度相联的数据，选用文档模型是糟糕的，选用关系模型是可接受的，而选用图形模型（参见“图数据模型”） 是最自然的 文档数据库有时称为无模式（schemaless） 一个更精确的术语是读时模式（schema-on-read） （数据的结构是隐含的，只有在数据被读取时才被解释） ，相应的是写时模式（schema-on-write） （传统的关系数据库方法中，模式明确，且数据库确保所有的数据都符合其模式） 当由于某种原因（例如，数据是异构的） 集合中的项目并不都具有相同的结构时,读时模式更具优势。例如， 如果：存在许多不同类型的对象，将每种类型的对象放在自己的表中是不现实的。 数据的结构由外部系统决定。你无法控制外部系统且它随时可能变化。 在上述情况下，模式的坏处远大于它的帮助，无模式文档可能是一个更加自然的数据模型。但是，要是所有记录都具有相同的结构，那么模式是记录并强制这种结构的有效机制。 2.4 模型的融合随着时间的推移，关系模型和文档模型已经开始相互借鉴。SQL 标准增加了对结构化数据类型， XML 和 json 数据的支持；这允许将多值存储在单行内，并支持在这些文档内的查询和索引。 文档数据也越来越多的支持连接。RethinkDB在其查询语言中支持类似关系的连接，一些MongoDB驱动程序可以自动解析数据库引用。在表示多对一和多对多的关系时，关系数据库和文档数据库并没有根本的不同。在这两种情况下，相关项目都被一个唯一的标识符引用，这个标识符在关系模型中被称为外键，在文档模型中称为文档引用。该标识符在读取时通过连接或后续查询来解析。 随着时间的推移，关系数据库和文档数据库似乎变得越来越相似，这是一件好事：数据模型相互补充 ，如果一个数据库能够处理类似文档的数据，并能够对其执行关系查询，那么应用程序就可以使用最符合其需求的功能组合。关系模型和文档模型的混合是未来数据库一条很好的路线。 3. 查询语言我们将数据存入数据库的目的是为了在需要的时候将其读取出来再次使用。数据库的查询有两种明显不同的方式: 声明式语言: 指定所需数据的模式 - 结果必须符合哪些条件，以及如何将数据转换（ 例如，排序，分组和集合） - 但不是如何实现这一目标。数据库系统的查询优化器决定使用哪些索引和哪些连接方法，以及以何种顺序执行查询的各个部分 命令式代码: 告诉计算机以特定顺序执行某些操作 声明式查询语言通常比命令式API更加简洁和容易。但更重要的是，它还隐藏了数据库引擎的实现细节，这使得数据库系统可以在无需对查询做任何更改的情况下进行性能提升。 4. 图数据模型随着数据之间的连接变得更加复杂，将数据建模为图形显得更加自然。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/categories/分布式/"}],"tags":[{"name":"数据密集型应用","slug":"数据密集型应用","permalink":"http://yoursite.com/tags/数据密集型应用/"}]},{"title":"2. 构建怎样的数据系统","slug":"db/db_2","date":"2019-04-01T16:00:00.000Z","updated":"2020-05-24T15:34:35.902Z","comments":true,"path":"2019/04/02/db/db_2/","link":"","permalink":"http://yoursite.com/2019/04/02/db/db_2/","excerpt":"构建数据密集型应用的目标","text":"构建数据密集型应用的目标 1. 构建怎样的系统影响数据系统设计的因素很多，每个因素都需要具体问题具体分析。而我们着重讨论三个在大多数软件系统中都很重要的问题： 可靠性(Reliability) 可扩展性(Scalability) 可维护性(Maintainability) 相比于模棱两可的概念，我们需要清楚的知道它们意味着什么，如何系统的衡量和评估他们。 1.1 说明要想说清楚这些概念非常不容易，它们其实涵盖了非常广泛的话题。下面大多数内容都是摘录自原著的开源翻译版本，特此说明。 2. 可靠性可靠性可粗略理解为“即使出现问题，也能继续正确工作”。造成错误的原因叫做故障（fault） ，能预料并应对故障的系统特性可称为容错（faulttolerant）或韧性（resilient）。但显然系统不能容忍所有错误，所以在讨论容错时，只有谈论特定类型的错误才有意义。 注意故障（fault）不同于失效（failure） 。故障通常定义为系统的一部分状态偏离其标准，而失效则是系统作为一个整体停止向用户提供服务。故障的概率不可能降到零，因此最好设计容错机制以防因故障而导致失效。 尽管比起阻止错误（prevent error），我们通常更倾向于容忍错误。但也存在预防错误的情况，比如安全问题就属于这种情况。但我们主要讨论的是可以恢复的故障种类，包括: 硬件故障(hardware faults) 软件错误 人为错误 3. 可扩展性可扩展性（Scalability）意味着即使在负载增加的情况下也有保持性能的策略,是用来描述系统应对负载增长能力的术语。讨论可扩展性意味着考虑诸如 “如果系统以特定方式增长，有什么选项可以应对增长？” “如何增加计算资源来处理额外的负载？”等问题。在我们讨论可扩展性之前，我们需要能够衡量系统的负载和性能。 3.1 系统负载负载可以用一些称为负载参数（load parameters） 的数字来描述。参数的最佳选择取决于系统架构，它可能是每秒向Web服务器发出的请求、数据库中的读写比率、聊天室中同时活跃的用户数量、缓存命中率或其他东西。除此之外，也许平均情况对你很重要，也许你的瓶颈是少数极端场景。你需要基于对系统的监控，找到系统中最耗费资源的地方。 3.2 描述性能量化了系统负载之后，我们要简单地看一下如何描述系统性能。 对于Hadoop这样的批处理系统，通常关心的是吞吐量（throughput） ，即每秒可以处理的记录数量，或者在特定规模数据集上运行作业的总时间 。对于在线系统，通常更重要的是服务的响应时间（response time） ，即客户端发送请求到接收响应之间的时间。 响应时间延迟（latency） 和响应时间（response time）经常用作同义词，但实际上它们并不一样。响应时间是客户所看到的，除了实际处理请求的时间（服务时间 service time）之外，还包括网络延迟和排队延迟。延迟是某个请求等待处理的持续时长，在此期间它处于休眠（latent） 状态，并等待服务。 通常使用百分位点来衡量系统的响应时间。中位数是一个好的度量标准，中位数也被称为第50百分位点，有时缩写为p50。为了弄清异常值有多糟糕，可以看看更高的百分位点，例如第95、99和99.9百分位点（缩写为p95，p99和p999） 响应时间的高百分位点（也称为尾部延迟 tail latencies） 非常重要，因为它们直接影响用户的服务体验。例如亚马逊在描述内部服务的响应时间要求时以99.9百分位点为准。这是因为请求响应最慢的客户往往也是数据最多的客户，也可以说是最有价值的客户 —— 因为他们掏钱了 另一方面，优化第99.99百分位点（一万个请求中最慢的一个） 被认为太昂贵了，减小高百分位点处的响应时间相当困难，因为它很容易受到随机事件的影响，这超出了控制范围，而且效益也很小。 排队延迟（queueing delay） 通常占了高百分位点处响应时间的很大一部分。由于服务器只能并行处理少量的事务（如受其CPU核数的限制），所以只要有少量缓慢的请求就能阻碍后续请求的处理，这种效应有时被称为头部阻塞（head-of-line blocking） 。 在多重调用的后端服务里，高百分位数变得特别重要。即使并行调用，最终用户请求仍然需要等待最慢的并行呼叫完成。如果您想将响应时间百分点添加到您的服务的监视仪表板，则需要持续有效地计算它们。有一些算法能够以最小的CPU和内存成本（如前向衰减，t-digest 或HdrHistogram）来计算百分位数的近似值。 3.3 应对负载增加应对负载增加，包括两种方式: 纵向扩展: scaling up 又称 垂直扩展（vertical scaling），即转向更强大的机器 横向扩展: scaling out 又称 水平扩展（horizontal scaling），即将负载分布到多台小机器上 可以在单台机器上运行的系统通常更简单，但高端机器可能非常贵，所以非常密集的负载通常无法避免地需要横向扩展。现实世界中的优秀架构需要将这两种方法务实地结合，因为使用几台足够强大的机器可能比使用大量的小型虚拟机更简单也更便宜。 跨多台机器部署无状态服务（stateless services） 非常简单，但将带状态的数据系统从单节点变为分布式配置则可能引入许多额外复杂度。出于这个原因，常识告诉我们应该将数据库放在单个节点上（纵向扩展），直到扩展成本或可用性需求迫使其改为分布式。 ​随着分布式系统的工具和抽象越来越好，至少对于某些类型的应用而言，这种常识可能会改变。可以预见分布式数据系统将成为未来的默认设置，即使对不处理大量数据或流量的场景也如此。 大规模的系统架构通常是应用特定的—— 没有一招鲜吃遍天的通用可扩展架构。应用的问题可能是读取量、写入量、要存储的数据量、数据的复杂度、响应时间要求、访问模式或者所有问题的大杂烩。 一个良好适配应用的可扩展架构，是围绕着假设（assumption） 建立的：哪些操作是常见的？哪些操作是罕见的？这就是所谓负载参数。如果假设最终是错误的，那么为扩展所做的工程投入就白费了，最糟糕的是适得其反。 尽管这些架构是应用程序特定的，但可扩展的架构通常也是从通用的积木块搭建而成的，并以常见的模式排列。在本书中，我们将讨论这些构件和模式。 4. 可维护性软件的大部分开销并不在最初的开发阶段，而是在持续的维护阶段，包括修复漏洞、保持系统正常运行、调查失效、适配新的平台、为新的场景进行修改、偿还技术债、添加新的功能等等。 为此，我们将特别关注软件系统的三个设计原则： 可操作性（Operability）: 意味着对系统的健康状态具有良好的可见性，并拥有有效的管理手段 便于运维团队保持系统平稳运行，意味着更轻松的日常工作，进而运维团队能专注于高价值的事情 简单性（Simplicity）: 从系统中消除尽可能多的复杂度（complexity），使新工程师也能轻松理解系统 复杂度（complexity）有各种可能的症状，例如：状态空间激增、模块间紧密耦合、纠结的依赖关系、不一致的命名和术语、解决性能问题的Hack、需要绕开的特例等 用于消除额外复杂度的最好工具之一是抽象（abstraction） 可演化性（evolability）: 使工程师在未来能轻松地对系统进行更改，当需求变化时为新应用场景做适配 也称为可扩展性（extensibility），可修改性（modifiability） 或可塑性（plasticity）","categories":[{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/categories/分布式/"}],"tags":[{"name":"数据密集型应用","slug":"数据密集型应用","permalink":"http://yoursite.com/tags/数据密集型应用/"}]},{"title":"1. 构建数据密集型应用","slug":"db/db_1","date":"2019-03-31T16:00:00.000Z","updated":"2020-05-24T15:33:36.288Z","comments":true,"path":"2019/04/01/db/db_1/","link":"","permalink":"http://yoursite.com/2019/04/01/db/db_1/","excerpt":"《构建数据密集型应用》读书笔记","text":"《构建数据密集型应用》读书笔记 1. 写在开始2014 年我开始自学编程的时候，在各种培训机构的课程里 Mysql 几乎还是唯一的数据库系统。现如今每一个工程师可能都听过 NoSQL，hadoop，Elasticsearch。”大数据”已经成为了几乎所有公司看中的技能。 然后当我尝试去学习所谓的”大数据”时，却只能用不知所措去形容我所面临的处境。到处都是有关大数据的零散知识点(诸如CAP, Paxos,Mapreduce)，以及无处不在推荐的 Google 三大论文。很难弄清楚充斥在这些文章中的专业术语到底是什么含义。 显然对于相我这样的初学者，需要有一本系统的书来帮我们搭建起“大数据”学习的知识框架。而《构建数据密集型应用》正是我梦寐以求的书。 我想我也无需去吹赞这本书，只想把他推荐给需要的人。好记性不如烂笔头，这个系列的博客就是《构建数据密集型应用》的读书笔记。 2. 本书结构什么是数据密集型应用，一个应用，如果数据是其主要挑战(数据量，数据复杂度和数据变化速度) ，它就被称为数据密集型应用。本书就是围绕应用中的数据问题展开的，分为如下三个部分： 1. 第一部分讨论了设计数据密集型应用所赖的基本思想。这些事数据系统底层的基础概念，无论是在单台机器上运行的单点数据系统，还是分布在多台机器上的分布式数据系统都适用: 第一章: 介绍应用设计的目标。可靠性，可扩展性和可维护性 ，这些词汇到底意味着什么，如何实现这些目标 第二章: 将对几种不同的数据模型和查询语言进行比较。从程序员的角度看，这是数据库之间最明显的区别。不同的数据模型适用于不同的应用场景 第三章: 将深入存储引擎内部，研究数据库如何在磁盘上摆放数据。不同的存储引擎针对不同的负载进行优化，选择合适的存储引擎对系统性能有巨大影响 第四章: 将对几种不同的 数据编码进行比较。特别研究了这些格式在应用需求经常变化、模式需要随时间演变的环境中表现如何 2. 第二部分我们从讨论存储在一台机器上的数据转向讨论分布在多台机器上的数据。这对于可扩展性通常是必需的，但带来了各种独特的挑战。我们首先讨论复制（ 第5章） ，分区/分片（ 第6章） 和事务（ 第7章） 。然后我们将探索关于分布式系统问题的更多细节（ 第8章） ，以及在分布式系统中实现一致性与共识意味着什么（ 第9章） 。 3. 第三部分我们讨论那些从其他数据集衍生出一些数据集的系统。衍生数据经常出现在异构系统中：当没有单个数据库可以把所有事情都做的很好时，应用需要集成几种不同的数据库，缓存，索引等。在第10章中我们将从一种衍生数据的批处理方法开始，然后在此基础上建立在第11章中讨论的流处理。最后，在第12章中，我们将所有内容汇总，讨论在将来构建可靠，可伸缩和可维护的应用程序的方法。 3. 资源收录除了内容外，本书还引用了大量的论文，博客，这些都是扩展学习非常好的学习资料。下面是与本书相关的学习资源: 书中引用持续维护的github仓库 原著的开源翻译版本，对于英文不好的同学真是福音 耗子叔在极客时间的专栏左耳听风，《构建数据密集型应用》是这个专栏推荐的书，也推荐这个专栏。 可扩展的Web架构和分布式系统","categories":[{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/categories/分布式/"}],"tags":[{"name":"数据密集型应用","slug":"数据密集型应用","permalink":"http://yoursite.com/tags/数据密集型应用/"}]},{"title":"10 go reflect","slug":"go/go_grammar/go_13","date":"2019-01-12T16:00:00.000Z","updated":"2019-11-09T07:15:22.071Z","comments":true,"path":"2019/01/13/go/go_grammar/go_13/","link":"","permalink":"http://yoursite.com/2019/01/13/go/go_grammar/go_13/","excerpt":"go 反射机制","text":"go 反射机制 1. 反射机制反射是一个复杂的内省技术。所谓内省即可以动态获取变量的类型，值，以及方法属性等元数据。需要反射的根本原因是，很多时候我们在编程时，并不能确定输入的具体类型，需要我们动态去判断。 Go语言提供的反射机制，能够让我们在运行时更新变量和检查它们的值、调用它们的方法和它们支持的内在操作，而不需要在编译时就知道这些变量的具体类型。也可以让我们将类型本身作为第一类的值类型处理。 GO 中有两个至关重要的API是使用反射机制实现： fmt包提供的字符串格式功能 类似encoding/json和encoding/xml提供的针对特定协议的编解码功能。 本节我们就来看看如何使用 Go 的反射机制，以及上述两个包使用 reflect 的方式。 2. Reflect API反射是由 reflect 包提供的。 它定义了两个重要的类型, Type 和 Value 2.1 TypeType 是一个接口类型，唯一能反映 reflect.Type 实现的是接口的类型描述信息。 我们在接口一节说过，接口的值，由两个部分组成，一个具体的类型和那个类型的值。它们被称为接口的动态类型和动态值。对于像Go语言这种静态类型的语言，类型是编译期的概念；因此一个类型不是一个值。在我们的概念模型中，一些提供每个类型信息的值被称为类型描述符，比如类型的名称和方法。在一个接口值中，类型部分代表与之相关类型的描述符。而 reflect.Type 的实现方式就与接口中的类型描述符类似。 reflect.Type 有许多办法来区分类型以及检查它们的组成部分, 例如一个结构体的成员或一个函数的参数等。 TypeOfreflect.TypeOf 接受任意的 interface{} 类型, 并以reflect.Type形式返回一个动态类型的接口值。reflect.Type 满足 fmt.Stringer 接口。 fmt.Printf 提供的 %T 参数, 内部就是使用 reflect.TypeOf 来输出接口的动态类型。 12345t := reflect.TypeOf(3) // a reflect.Typefmt.Println(t.String()) // \"int\"fmt.Println(t) // \"int\"fmt.Printf(\"%T\\n\", 3) // \"int\" 2.2 Valuereflect.Value 可以装载任意类型的值。函数 reflect.ValueOf 接受任意的 interface{} 类型, 并返回一个装载着其动态值的 reflect.Value。和 reflect.Type 类似, reflect.Value 也满足 fmt.Stringer 接口, 但是除非 Value 持有的是字符串, 否则 String 方法只返回其类型. 而使用 fmt 包的 %v 标志参数会对 reflect.Values 特殊处理. 1234v := reflect.ValueOf(3) // a reflect.Valuefmt.Println(v) // \"3\"fmt.Printf(\"%v\\n\", v) // \"3\"fmt.Println(v.String()) // NOTE: \"&lt;int Value&gt;\" 对 Value 调用 Type 方法将返回具体类型所对应的 reflect.Type:12t := v.Type() // a reflect.Typefmt.Println(t.String()) // \"int\" 2.3 Value 与 interface{}reflect.ValueOf 的逆操作是 reflect.Value.Interface 方法. 它返回一个 interface{} 类型，装载着与 reflect.Value 相同的具体值。 1234v := reflect.ValueOf(3) // a reflect.Valuex := v.Interface() // an interface&#123;&#125;i := x.(int) // an intfmt.Printf(\"%d\\n\", i) // \"3 reflect.Value 和 interface{} 都能装载任意的值. 所不同的是, 一个空的接口隐藏了值内部的表示方式和所有方法, 因此只有我们知道具体的动态类型才能使用类型断言来访问内部的值(就像上面那样),内部值我们没法访问. 相比之下, 一个 Value 则有很多方法来检查其内容, 无论它的具体类型是什么。 与 switch x := x.(type) 相比 reflect.Value.Kind 返回的数据类型是有限的: Bool, String 和 所有数字类型的基础类型 Array 和 Struct 对应的聚合类型; Chan, Func, Ptr, Slice, 和 Map 对应的引用类型; interface 类型; 还有表示空值的 Invalid 类型 (空的 reflect.Value 的 kind 即为 Invalid.) Kind 只关心底层表示, 所有的具名类型都会归属到对应的原始类型之上。 2.4 示例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364func Display(name string, x interface&#123;&#125;) &#123; fmt.Printf(\"Display %s (%T):\\n\", name, x) display(name, reflect.ValueOf(x))&#125;func formatAtom(v reflect.Value) string &#123; switch v.Kind() &#123; case reflect.Invalid: return \"invalid\" case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64: return strconv.FormatInt(v.Int(), 10) case reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64, reflect.Uintptr: return strconv.FormatUint(v.Uint(), 10) // ...floating‐point and complex cases omitted for brevity... case reflect.Bool: return strconv.FormatBool(v.Bool()) case reflect.String: return strconv.Quote(v.String()) case reflect.Chan, reflect.Func, reflect.Ptr, reflect.Slice, reflect.Map: return v.Type().String() + \" 0x\" + strconv.FormatUint(uint64(v.Pointer()), 16) default: // reflect.Array, reflect.Struct, reflect.Interface return v.Type().String() + \" value\" &#125;&#125;func display(path string, v reflect.Value) &#123; switch v.Kind() &#123; case reflect.Invalid: fmt.Printf(\"%s = invalid\\n\", path) case reflect.Slice, reflect.Array: for i := 0; i &lt; v.Len(); i++ &#123; display(fmt.Sprintf(\"%s[%d]\", path, i), v.Index(i))&#125; case reflect.Struct: for i := 0; i &lt; v.NumField(); i++ &#123; fieldPath := fmt.Sprintf(\"%s.%s\", path, v.Type().Field(i).Name) display(fieldPath, v.Field(i))&#125; case reflect.Map: for _, key := range v.MapKeys() &#123; display(fmt.Sprintf(\"%s[%s]\", path, formatAtom(key)), v.MapIndex(key))&#125; case reflect.Ptr: if v.IsNil() &#123; fmt.Printf(\"%s = nil\\n\", path) &#125; else &#123; display(fmt.Sprintf(\"(*%s)\", path), v.Elem())&#125; case reflect.Interface: if v.IsNil() &#123; fmt.Printf(\"%s = nil\\n\", path) &#125; else &#123; fmt.Printf(\"%s.type = %s\\n\", path, v.Elem().Type()) display(path+\".value\", v.Elem()) &#125; default: // basic types, channels, funcs fmt.Printf(\"%s = %s\\n\", path, formatAtom(v)) &#125;&#125;","categories":[{"name":"Go","slug":"Go","permalink":"http://yoursite.com/categories/Go/"}],"tags":[{"name":"go语言入门","slug":"go语言入门","permalink":"http://yoursite.com/tags/go语言入门/"}]},{"title":"9 go 包和管理工具","slug":"go/go_grammar/go_11","date":"2019-01-10T16:00:00.000Z","updated":"2019-11-09T07:15:22.071Z","comments":true,"path":"2019/01/11/go/go_grammar/go_11/","link":"","permalink":"http://yoursite.com/2019/01/11/go/go_grammar/go_11/","excerpt":"go 程序包组织结构和程序管理工具箱","text":"go 程序包组织结构和程序管理工具箱 1. 包简介包和模块的概念几乎存在于所有的编程语言之中，它的存在是为了简化大型程序的设计和维护工作。通过将一组相关的特性放进一个独立的单元以便于理解和更新，这种特性提供诸多益处: 每个包可以被其它的不同项目共享和重用 包提供了一个独立的命名空间，减少了与其他部分的命名冲突 通过控制包内名字的可见性和是否导出来实现封装 Go 通过使用名字的开头字母的大小写决定了名字在包外的可见性，小写字符开头包成员不会导出，在包外不可见。通过这种方式可以严格的隐藏包内实现的 API，通过强制用户使用特定函数来访问和更新内部变量，可以保证内部变量的一致性和并发时的互斥约束。 当我们修改了一个源文件，我们必须重新编译该源文件对应的包和所有依赖该包的其他包。即使是从头构建，Go语言编译器的编译速度也明显快于其它编译语言。Go语言的闪电般的编译速度主要得益于三个语言特性: 第一点，所有导入的包必须在每个文件的开头显式声明，这样的话编译器就没有必要读取和分析整个源文件来判断包的依赖关系 第二点，禁止包的环状依赖，因为没有循环依赖，包的依赖关系形成一个有向无环图，每个包可以被独立编译，而且很可能是被并发编译 第三点，编译后包的目标文件不仅仅记录包本身的导出信息，目标文件同时还记录了包的依赖关系。因此，在编译一个包的时候，编译器只需要读取每个直接导入包的目标文件，而不需要遍历所有依赖的的文件。 本节我们就来学习与 Go 语言包相关的内容。 2. Go 程序包2.1 包声明Go 语言的源码也是以代码包为基本组织单位的。在文件系统中，这些代码包其实是与目录一一对应的。由于目录可以有子目录，所以代码包也可以有子包。一个代码包中可以包含任意个以.go 为扩展名的源码文件，这些源码文件都需要被声明属于同一个代码包。 在每个Go语言源文件的开头都必须有包声明语句。包声明语句的主要目的是确定当前包被其它包导入时默认的标识符（也称为包名）。代码包的名称一般会与源码文件所在的目录同名。如果不同名，那么在构建、安装的过程中会以代码包名称为准。 通常来说，默认的包名就是包导入路径名的最后一段，因此即使两个包的导入路径不同，它们依然可能有一个相同的包名。例如，math/rand包和crypto/rand包的包名都是rand。这也有三种例外情况。 第一个例外，包对应一个可执行程序，也就是main包，这时候main包本身的导入路径是无关紧要的。名字为main的包是给go build 构建命令一个信息，这个包编译完之后必须调用连接器生成一个可执行程序。 第二个例外，包所在的目录中可能有一些文件名是以 _test.go为后缀的Go源文件（译注：前面必须有其它的字符，因为以 _前缀的源文件是被忽略的），并且这些源文件声明的包名也是以_test为后缀名的。这种目录可以包含两种包：一种普通包，加一种则是测试的外部扩展包。所有以_test为后缀包名的测试外部扩展包都由go test命令独立编译，普通包和测试的外部扩展包是相互独立的。测试的外部扩展包一般用来避免测试代码中的循环导入依赖，具体细节我们将在下一章讲解。 第三个例外，一些依赖版本号的管理工具会在导入路径后追加版本号信息，例如”gopkg.in/yaml.v2”。这种情况下包的名字并不包含版本号后缀，而是yaml 2.2 包的导入每个包是由一个全局唯一的字符串所标识的导入路径定位。在实际使用程序实体之前，我们必须先导入其所在的代码包。在工作区中，一个代码包的导入路径实际上就是从 src 子目录，到该包的实际存储位置的相对路径。而导入时包可以被重命名，被隐藏。下面是包导入时常用的语法: 12345678package package_nameimport fmtimport ( \"crypto/rand\" mrand \"math/rand\" // 包导入重命名，避免冲突 import _ \"image/png\" // 匿名导入) 需要注意的事: 包的导入必须在包声明语句之后，其它非导入声明语句之前 每个导入声明语句都明确指定了当前包和被导入包之间的依赖关系。如果遇到包循环导入的情况，Go语言的构建工具将报告错误。 Go语言的规范并没有指明包的导入路径字符串的具体含义，导入路径的具体含义是由构建工具来解释的，当使用Go语言自带的go工具箱时，一个导入路径代表一个包在文件系统的路径 2.3 包的初始化每个包在解决依赖的前提下，包会以导入声明的顺序初始化，包的初始化首先是解决包级变量的依赖顺序，然后按照包级变量声明出现的顺序依次初始化： 1234var a = b + c // a 第三个初始化, 为 3var b = f() // b 第二个初始化, 为 2, 通过调用 f (依赖c)var c = 1 // c 第一个初始化, 为 1func f() int &#123; return c + 1 &#125; 如果包中含有多个.go源文件，它们将按照发给编译器的顺序进行初始化，Go语言的构建工具首先会将.go文件根据文件名排序，然后依次调用编译器编译。 每个包只会被初始化一次。因此，如果一个p包导入了q包，那么在p包初始化的时候可以认为q包必然已经初始化过了。初始化工作是自下而上进行的，main包最后被初始化。以这种方式，可以确保在main函数执行之前，所有依赖的包都已经完成初始化工作了。 对于在包级别声明的变量，如果有初始化表达式则用表达式初始化，还有一些没有初始化表达式的，例如某些表格数据初始化并不是一个简单的赋值过程。在这种情况下，我们可以用一个特殊的init初始化函数来简化初始化工作。每个文件都可以包含多个init初始化函数 1func init() &#123; /* ... */ &#125; 这样的init初始化函数除了不能被调用或引用外，其他行为和普通函数类似。在每个文件中的init初始化函数，在程序开始执行时按照它们声明的顺序被自动调用。 匿名导入如果只是导入一个包而并不使用导入的包将会导致一个编译错误。但是有时候我们只是想利用导入包而产生的副作用：计算包级变量的初始化表达式和执行导入包的init初始化函数。我们可以用下划线 _来重命名导入的包。像往常一样，下划线 _为空白标识符，并不能被访问。 包文档Go语言中包文档注释一般是完整的句子，第一行是包的摘要说明，注释后仅跟着包声明语句。包注释可以出现在任何一个源文件中。如果包的注释内容比较长，一般会放到一个独立的源文件中；fmt包注释就有300行之多。这个专门用于保存包文档的源文件通常叫doc.go。 内部包有时候，一个中间的状态可能也是有用的，对于一小部分信任的包是可见的，但并不是对所有调用者都可见。例如，当我们计划将一个大的包拆分为很多小的更容易维护的子包，但是我们并不想将内部的子包结构也完全暴露出去。同时，我们可能还希望在内部子包之间共享一些通用的处理包，或者我们只是想实验一个新包的还并不稳定的接口，暂时只暴露给一些受限制的用户使用 为了满足这些需求，Go语言的构建工具对包含internal名字的路径段的包导入路径做了特殊处理。这种包叫internal包，一个internal包只能被和internal目录有同一个父目录的包所导入。例如，net/http/internal/chunked内部包只能被net/http/httputil或net/http包导入，但是不能被net/url包导入。不过net/url包却可以导入net/http/httputil包。1234net/httpnet/http/internal/chunkednet/http/httputilnet/url go 命令使用go get使用命令 go get可以下载一个单一的包或者用 …下载整个子目录里面的每个包。Go语言工具箱的go命令同时计算并下载所依赖的每个包，一旦 go get命令下载了包，然后就是安装包或包对应的可执行的程序 go get命令支持当前流行的托管网站GitHub、Bitbucket和Launchpad，可以直接向它们的版本控制系统请求代码。对于其它的网站，你可能需要指定版本控制系统的具体路径和协议，例如 Git或Mercurial。 123456go get github.com/golang/lint/golintcd $GOPATH/src/golang.org/x/netgit remote ‐vorigin https://go.googlesource.com/net (fetch)origin https://go.googlesource.com/net (push) 需要注意的是导入路径含有的网站域名和本地Git仓库对应远程服务地址并不相同，真实的Git地址是go.googlesource.com。这其实是Go语言工具的一个特性，可以让包用一个自定义的导入路径，但是真实的代码却是由更通用的服务提供，例如googlesource.com或github.com。因为页面 https://golang.org/x/net/html 包含了如下的元数据，它告诉Go语言的工具当前包真实的Git仓库托管地址： 12&lt;meta name=&quot;go‐import&quot;content=&quot;golang.org/x/net git https://go.googlesource.com/net&quot;&gt; 如果指定 ‐u命令行标志参数， go get命令将确保所有的包和依赖的包的版本都是最新的，然后重新编译和安装它们。如果不包含该标志参数的话，而且如果包已经在本地存在，那么代码那么将不会被自动更新。 go buildgo build命令编译命令行参数指定的每个包。如果包是一个库，则忽略输出结果；这可以用于检测包的可以正确编译的。如果包的名字是main， go build将调用连接器在当前目录创建一个可执行程序；以导入路径的最后一段作为可执行程序的名字 默认情况下， go build命令构建指定的包和它依赖的包，然后丢弃除了最后的可执行文件之外所有的中间编译结果。 go install命令和 go build命令很相似，但是它会保存每个包的编译成果，而不是将它们都丢弃。被编译的包会被保存到$GOPATH/pkg目录下，目录路径和 src目录路径对应，可执行程序被保存到$GOPATH/bin目录。 goinstall命令和 go build命令都不会重新编译没有发生变化的包，这可以使后续构建更快捷。为了方便编译依赖的包， go build ‐i命令将安装每个目标所依赖的包。 因为编译对应不同的操作系统平台和CPU架构， go install命令会将编译结果安装到GOOS和GOARCH对应的目录。例如，在Mac系统，golang.org/x/net/html包将被安装到$GOPATH/pkg/darwin_amd64目录下的golang.org/x/net/html.a文件。 针对不同操作系统或CPU的交叉构建也是很简单的。只需要设置好目标对应的GOOS和GOARCH，然后运行构建命令即可。下面交叉编译的程序将输出它在编译时操作系统和CPU类型：有些包可能需要针对不同平台和处理器类型使用不同版本的代码文件，以便于处理底层的可移植性问题或提供为一些特定代码提供优化。如果一个文件名包含了一个操作系统或处理器类型名字，例如net_linux.go或asm_amd64.s，Go语言的构建工具将只在对应的平台编译这些文件。还有一个特别的构建注释注释可以提供更多的构建过程控制。例如，文件中可能包含下面的注释： // +build linux darwin在包声明和包注释的前面，该构建注释参数告诉 go build只在编译程序对应的目标操作系统是Linux或Mac OS X时才编译这个文件。下面的构建注释则表示不编译这个文件 // +build ignore更多细节，可以参考go/build包的构建约束部分的文档 go doc go doc命令，该命令打印包的声明和每个成员的文档注释，该命令并不需要输入完整的包导入路径或正确的大小写 1234go doc timego doc time.Sincego doc time.Duration.Secondsgo doc json.decode godoc，它提供可以相互交叉引用的HTML页面，但是包含和 go doc命令相同以及更多的信息。godoc的在线服务 https://godoc.org ，包含了成千上万的开源包的检索工具。你也可以在自己的工作区目录运行godoc服务。运行下面的命令，然后在浏览器查看 http://localhost:8000/pkg 页面： $ godoc ‐http :8000 其中 ‐analysis=type和 ‐analysis=pointer命令行标志参数用于打开文档和代码中关于静态分析的结果 go listgo list命令可以查询可用包的信息。其最简单的形式，可以测试包是否在工作区并打印它的导入路径，还可以用 “…”表示匹配任意的包的导入路径。我们可以用它来列表工作区中的所有包： 123456$ go list github.com/go‐sql‐driver/mysqlgithub.com/go‐sql‐driver/mysql$ go list gopl.io/ch3/...$ go list ...xml... go list命令还可以获取每个包完整的元信息，而不仅仅只是导入路径，这些元信息可以以不同格式提供给用户。其中 ‐json命令行参数表示用JSON格式打印每个包的元信息。命令行参数 ‐f则允许用户使用text/template包（§4.6）的模板语言定义输出文本的格式。 123go list ‐json hashgo list ‐f '&#123;&#123;join .Deps \" \"&#125;&#125;' strconvgo list ‐f '&#123;&#123;.ImportPath&#125;&#125; ‐&gt; &#123;&#123;join .Imports \" \"&#125;&#125;' compress/...","categories":[{"name":"Go","slug":"Go","permalink":"http://yoursite.com/categories/Go/"}],"tags":[{"name":"go语言入门","slug":"go语言入门","permalink":"http://yoursite.com/tags/go语言入门/"}]},{"title":"8 go 并发编程一","slug":"go/go_grammar/go_8","date":"2019-01-07T16:00:00.000Z","updated":"2019-04-19T14:19:50.439Z","comments":true,"path":"2019/01/08/go/go_grammar/go_8/","link":"","permalink":"http://yoursite.com/2019/01/08/go/go_grammar/go_8/","excerpt":"Go 并发编程原语，Goroutines和Channels","text":"Go 并发编程原语，Goroutines和Channels 1. GO 并发编程简介上一篇我们讲解了 Go 语言中的接口，至此对于 Go 语言的类型系统我们基本上讲的差不都了。接下来我们将深入了解 Go 最为人推广的特性并发编程。对于那些完全独立的子问题，并发是简单的，但是真正复杂的是处理那些存在资源共享的多进程多线程并发问题。我们需要有效的通信机制来处理程序中的竞争条件，同时避免可能出现的死锁问题。 Go 之所以在并发编程中被人推广，是因为它提供的 goroutine 和 channel 支持“顺序通信进程”(communicating sequential processes)简称为CSP，这是一种现代的并发编程模型。CSP的具体原理我也不是很懂，但是 Go 有一句口头禅“不要使用共享数据来通信；使用通信来共享数据” 。学完这部分内容，你就能理解这句话的含义了。 没有一招鲜吃遍天的技术，每个模型都是特定的假设条件和使用情景，CSP 也不例外。相比于 GSP 传统的并发模型：多线程共享内存，可能更容易出错(竞争条件和死锁)，但是也更加灵活。所以要想写出正确的并发程序，对操作系统提供的锁，信号量等进程间通信的底层机制的了解必不可少。我们将分为三节来介绍这些并发编程的技巧，本节我们先来学习 goroutine 和 channel。 2. Goroutine在Go语言中，每一个并发的执行单元叫作一个goroutine。当一个程序启动时，其主函数即在一个单独的goroutine中运行，我们叫它main goroutine 。 2.1 goroutine 创建新的goroutine会用go语句来创建。在语法上，go语句是一个普通的函数或方法调用前加上关键字go。go语句会使其语句中的函数在一个新创建的goroutine中运行。而go语句本身会迅速地完成。 12f() // call f(); wait for it to returngo f() // create a new goroutine that calls f(); don't wait 2.2 goroutine 退出与回收通常goroutine在执行完毕时会自动回收，当主函数返回时，所有未执行完毕的 goroutine 会被直接打断，程序退出。如果 goroutine 因为阻塞永远被卡住，我们称发生了goroutine泄漏，和垃圾变量不同，泄漏的goroutines并不会被自动回收，因此确保每个不再需要的goroutine能正常退出是重要的。 2.3 goroutine 中断除了从主函数退出或者直接终止程序之外，没有其它的编程方法能够让一个goroutine来打断另一个的执行。但是通过 goroutine 之间的通信机制，可以实现让一个 goroutine 在收到其它的 goroutine 特定信号时终止退出。这个必须得等到我们讲完 channel 时才能继续说明。 3. channels如果说goroutine是Go语言程序的并发体的话，那么 channels 则是它们之间的通信机制。一个 channels 可以让一个 goroutine 通过它给另一个 goroutine 发送值信息。 每个channel都有一个特殊的类型，也就是channels可发送数据的类型。和其它的引用类型一样，channel的零值也是nil，因此channel 可以与 nil 值比较。两个相同类型的channel可以使用==运算符比较。如果两个channel引用的是相通的对象，那么比较的结果为真。 3.1 channel 创建创建 channel 最简单的方式是使用 make 函数，第二个可选参数，用于指定 channel 的容量。 123456ch = make(chan int) // 无缓存 channel ch = make(chan int, 0) // 无缓存 channel ch = make(chan int, 3) // 待缓存的 channelcap(ch) // 获取 channel 容量len(ch) // 返回 channel 中有效元素个数 channel 与并发的先进先出队列极其相似: 发送在队尾插入元素，接收从队首删除元素 当 channel 空时，从 channel 接收值的 goroutine 将被阻塞，直至另一个 goroutine 向 channel 发送值 当 channel 满时，向 channel 发送值的 goroutine 将被阻塞，直至另一个 goroutine 从 channel 接收值 特别的对于无缓存 channels 的发送和接收操作将导致两个goroutine做一次同步操作，需要注意的是当通过一个无缓存 channels 发送数据时，接收者收到数据发生在唤醒发送者 goroutine 之前。 3.2 发送与接收channel有发送和接受两种操作: 12345ch &lt;‐ x // 向 channel 发送一个值x = &lt;‐ch // 从 channel 接收值&lt;‐ch // 从 channel 接收值，但丢弃close(ch) // 关闭 channel 为了防止 channel 被乱用，Go语言还提供了单方向的 channel 类型，即只发送或只接收的channel。 12345// 只发送和只接受的 channel 类型chan&lt;‐ int // 只发送int的channel，不能接收&lt;‐chan int // 只接收int的channel，不能发送func squarer(out chan&lt;‐ int, in &lt;‐chan int) &#123;&#125; 任何双向channel向单向channel变量的赋值操作都将导致该隐式转换。但是没有反向转换的语法，即不能将类似 chan&lt;‐ int类型的单向型的channel转换为 chan int类型的双向型的channel。 因为关闭操作只用于断言不再向channel发送新的数据，所以只有在发送者所在的 goroutine 才会调用close函数，因此对一个只接收的channel调用 close 将是一个编译错误。 3.3 关闭channel还支持close操作，用于关闭channel，对于接收方和发送方，关闭channel之后的操作是不同的: 发送方: 对一个关闭的 channel 的任何发送操作都将导致panic异常，因此关闭操作只能由发送方执行 接收方: 在 channel 关闭之后依然可以接受到之前已经成功发送的数据；如果channel中已经没有数据，后续的接收操作也不会再阻塞，而是立即返回一个零值。稍后我们就会利用这个特性，通过关闭 channel实现一种广播机制。 所以对于下面这个例子，即使 naturals变量对应的channel 被关闭，循环也不会终止，它依然会收到一个永无休止的零值序列。 1234567// Squarergo func() &#123; for &#123; x := &lt;‐naturals squares &lt;‐ x * x &#125;&#125;() 没有办法直接测试一个channel是否被关闭，但是接收操作有一个变体形式：它多接收一个结果，多接收的第二个结果是一个布尔值ok，ture表示成功从channels接收到值，false表示channels已经被关闭并且里面没有值可接收。range 可以简化对 channels 的读取和关闭测试，下面是一些代码示例: 12345678910111213141516171819// 通过可选的第二个参数，在接收方判断 channel 是否关闭go func() &#123; for &#123; x, ok := &lt;‐naturals if !ok &#123; break // channel was closed and drained &#125; squares &lt;‐ x * x &#125; close(squares)&#125;()// range循环可直接在channels上迭代，当channel被关闭并且没有值可接收时跳出循环go func() &#123; for x := range naturals &#123; squares &lt;‐ x * x &#125; close(squares)&#125;() 最后，试图关闭一个nil值的channel也将导致panic异常。 4. select 多路复用有些时候，我们需要同时监听多个 channel 的接收和发送操作，并选择第一个可执行 channel 进行操作。此时我们就需要 select 多路复用。select 与 和 switch 语句稍微有点相似，select 也会有几个 case和最后的default选择分支。每一个case代表一个通信操作(在某个channel上进行发送或者接收)并且会包含一些语句组成的一个语句块。 12345678910select &#123;case &lt;‐ch1: // ...case x := &lt;‐ch2: // ...use x...case ch3 &lt;‐ y: // ...default: // ...&#125; select会等待case中的 channel 操作，直至出现一个可通信的 channel 时，执行通信并选择对应的 case 执行；这时候其它通信是不会执行的。一个没有任何case的select语句写作select{}，会永远地等待下去。如果多个case同时就绪时，select会随机地选择一个执行，这样来保证每一个channel都有平等的被select的机会。 对一个nil的channel发送和接收操作会永远阻塞，在select语句中操作nil的channel永远都不会被select到。这使得我们可以用nil来激活或者禁用case，来达成处理其它输入或输出事件时超时和取消的逻辑。 5. goroutine 的中断有了上面的铺垫，我们回头来看如何中断一个 goroutine 的执行。现在我们知道，当一个被关闭的 channel 被消费掉了所有已发送的值之后，对channel 的任何操作会立即被执行，并且产生零值。我们将代表取消操作的 channel 作为 select 的一个分支，一个立刻返回的分支；通过关闭 channel 让所有操作该 channel 的代码都可以立马执行，从而 select 会选择退出分支，让 goroutine 立刻终止。通过 channel 的取消操作，我们实现了一种广播机制。下面是一个简单的代码示例: 123456789101112131415161718192021222324252627# 广播机制var done = make(chan struct&#123;&#125;)func cancelled() bool &#123; select &#123; case &lt;‐done: // channel 被关闭后，立马就会执行 return true default: return false &#125;&#125;# 监听用户的取消操作go func() &#123; os.Stdin.Read(make([]byte, 1)) // read a single byte close(done) // 通过关闭 channel，进行消息广播&#125;()func walkDir(dir string, n *sync.WaitGroup, fileSizes chan&lt;‐ int64) &#123; defer n.Done() if cancelled() &#123; // 发现用户取消，立刻终止 return &#125; for _, entry := range dirents(dir) &#123; // ... &#125;&#125; 6. 使用示例接下来，我们将探究一个生成缩略图的问题来作为 goroutine 和 channel 的使用示例。下面是一个顺序执行的版本。 123456789// makeThumbnails makes thumbnails of the specified files.func makeThumbnails(filenames []string) &#123; for _, f := range filenames &#123; # 缩略图执行的函数，具体代码省略 if _, err := thumbnail.ImageFile(f); err != nil &#123; log.Println(err) &#125; &#125;&#125; 显然，我们可以使用并发来加快程序的执行速度。 123456// NOTE: incorrect!func makeThumbnails2(filenames []string) &#123; for _, f := range filenames &#123; go thumbnail.ImageFile(f) // NOTE: ignoring errors &#125;&#125; 然而上面面的程序是有问题的，makeThumbnails(下称主函数)在 go 创建的 goroutine(下称 work goroutine) 还没有完成工作之前就已经返回了。我们需要主函数等待 work goroutine 完成。我们可以使用 channel 进行同步。 123456789101112131415func makeThumbnails4(filenames []string) error &#123; errors := make(chan error) for _, f := range filenames &#123; go func(f string) &#123; _, err := thumbnail.ImageFile(f) errors &lt;‐ err &#125;(f) &#125; for range filenames &#123; if err := &lt;‐errors; err != nil &#123; return err // NOTE: incorrect: goroutine leak! &#125; &#125; return nil&#125; 这个程序有一个微秒的bug。当它遇到第一个非nil的error时会直接将error返回到调用方，使得没有一个goroutine去排空errors channel。这样剩下的worker goroutine在向这个channel中发送值时，都会永远地阻塞下去，并且永远都不会退出。即出现goroutine泄露，可能会导致整个程序卡住或者跑出out of memory的错误。 最简单的解决办法就是用一个具有合适大小的buffered channel(c h := make(chan item, len(filenames)))，这样这些worker goroutine向channel中发送错误时就不会被阻塞。另一个可选的解决办法是创建一个另外的goroutine，当maingoroutine返回第一个错误的同时去排空channel。 此外，如果文件过多，程序可能会创建成百上千的 goroutine，我们需要用计数信号量来限制并发的数量。 12345678910// 限制并发数的信号量var sema = make(chan struct&#123;&#125;, 20)go func(f string) &#123; sema &lt;‐ struct&#123;&#125;&#123;&#125; // 执行前获取 token defer func() &#123; &lt;‐sema &#125;() // 执行结束后释放 token _, err := thumbnail.ImageFile(f) errors &lt;‐ err&#125;(f) 7. 使用局限至此，我们已经掌握了goroutine 和 channel的基本使用，但是还远远不够。我们无法解决像下面这些问题:","categories":[{"name":"Go","slug":"Go","permalink":"http://yoursite.com/categories/Go/"}],"tags":[{"name":"go语言入门","slug":"go语言入门","permalink":"http://yoursite.com/tags/go语言入门/"}]},{"title":"7 go 接口","slug":"go/go_grammar/go_7","date":"2019-01-06T16:00:00.000Z","updated":"2019-04-14T01:58:18.352Z","comments":true,"path":"2019/01/07/go/go_grammar/go_7/","link":"","permalink":"http://yoursite.com/2019/01/07/go/go_grammar/go_7/","excerpt":"Go 的泛型编程","text":"Go 的泛型编程 1. 接口概述接口是 Go 语言提供的泛型的核心概念。所谓泛型就是允许程序员在强类型程序设计语言中编写代码时使用一些以后才指定的类型，目的是增加函数的通用性。当然我们没必要去纠结概念，最重要的是搞明白，Go 如何通过接口来提高程序的灵活性。 在学习接口之前，我们需要对它有如下一个整体的认识，以把握住接口的整体脉络: Go 的接口类型是抽象类型，与 Python 中鸭子类型类似，通过类型支持的方法来约束对象的适用范围。我们将学些如何在 Go 定义接口，如何判断一个具体类型实现了哪些接口。 接口不仅是对类型的抽象和限定，也代表了将接口作为参数的函数和函数调用者之间的一个约定(正是通过这种约定提高了函数的可用性): 调用者需要提供符合接口的具体类型作为参数 函数在接受任何满足接口的值时都可以工作，函数不会调用接口限定之外的任何其他方法 有了上面的铺垫，我们将按照下面的顺序介绍接口的相关内容: 接口类型 接口的定义 接口归属判断 接口的约定 接口值 类型断言 2. 接口类型2.1 接口定义接口类型是一种抽象的类型，与字符串，整数这些具体类型相比，我们并不知道接口类型代表的具体值；它只包含方法声明，描述了一系列方法的集合。下面是 Go 接口类型的定义示例: 1234567891011121314151617package iotype Reader interface &#123; Read(p []byte) (n int, err error)&#125;type Closer interface &#123; Close() error&#125;type Writer interface &#123; Write(p []byte) (n int, err error)&#125;type ReadWriter interface &#123; Reader Writer&#125; 与结构体嵌入类似，我们也可以通过类似的方式进行接口内嵌，实现接口组合。在接口的定义中方法的定义顺序没有影响，唯一重要的是接口内的方法集合。 2.2 接口归属判断如果一个类型拥有一个接口需要的所有方法，那么这个类型就实现了这个接口，我们称这个具体类型是这个接口类型的实例。正如我们在 go 方法一章所描述的，一个自定义数据类型的方法集合中仅会包含它的所有值方法，而该类型的指针类型的方法集合却囊括了所有值方法和所有指针方法。因此对于一个自定义类型，他的类型和他的指针类型实现的接口并不相同。 1234567891011121314// 1. 表达一个类型属于某个接口只要这个类型实现这个接口var rwc io.ReadWriteCloserrwc = os.Stdout // OK: *os.File has Read, Write, Close methodsrwc = new(bytes.Buffer) // compile error: *bytes.Buffer lacks Close method// 2. 接口归属的判断同样适合接口之间w = rwc // OK: io.ReadWriteCloser has Write methodrwc = w // compile error: io.Writer lacks Close method// 3. 类型 与 类型的指针类型，实现的接口并不相同，后者可能实现了更多的接口type IntSet struct &#123; /* ... */ &#125;func (*IntSet) String() stringvar _ fmt.Stringer = &amp;s // OKvar _ fmt.Stringer = s // compile error: IntSet lacks String method 每一个具体类型的组基于它们相同的行为可以表示成一个接口类型。接口不止是一种有用的方式来分组相关的具体类型和表示他们之间的共同特定。在Go语言中我们可以在需要的时候定义一个新的抽象或者特定特点的组，而不需要修改具体类型的定义。 2.3 接口的约定正如我们开篇所说的，接口类型不仅是对类型的约束，也代表着函数和调用者之间的约定。 1234567891011121314151617181920type Writer interface &#123; Write(p []byte) (n int, err error)&#125;func Fprintf(w io.Writer, format string, args ...interface&#123;&#125;) (int, error)type ByteCounter intfunc (c *ByteCounter) Write(p []byte) (int, error) &#123; *c += ByteCounter(len(p)) // convert int to ByteCounter return len(p), nil&#125;var c ByteCounterc.Write([]byte(\"hello\"))fmt.Println(c) // \"5\", = len(\"hello\")c = 0 // reset the countervar name = \"Dolly\"fmt.Fprintf(&amp;c, \"hello, %s\", name)fmt.Println(c) // \"12\", = len(\"hello, Dolly\") 如上例所述，io.Writer 接口约定了，函数调用者必须提供实现了 io.Writer 接口的具体类型作为函数参数，而 Fprintf 函数只能调用 io.Writer 接口暴露出来的方法，即使具体类型有其它的方法也不能调用。 2.4 空接口interface{}被称为空接口，空接口类型是不可或缺的。因为空接口类型对实现它的类型没有要求，所以我们可以将任意一个值赋给空接口类型。当然我们不能直接对它持有的值做操作，因为interface{}没有任何方法。我们会在稍后介绍一种用类型断言来获取interface{}中值的方法。 123456var any interface&#123;&#125;any = trueany = 12.34any = \"hello\"any = map[string]int&#123;\"one\": 1&#125;any = new(bytes.Buffer) 3. 接口的值3.1 接口赋值概念上讲一个接口的值，由两个部分组成，一个具体的类型和那个类型的值。它们被称为接口的动态类型和动态值。对于像Go语言这种静态类型的语言，类型是编译期的概念；因此一个类型不是一个值。在我们的概念模型中，一些提供每个类型信息的值被称为类型描述符，比如类型的名称和方法。在一个接口值中，类型部分代表与之相关类型的描述符。 我们通过下面一个赋值的示例来了解接口的值 1234var w io.Writerw = os.Stdoutw = new(bytes.Buffer)w = nil var w io.Writer定义了变量w，变量总是被一个定义明确的值初始化，即使接口类型也不例外。对于一个接口的零值就是它的类型和值的部分都是nil。一个接口值仅基于它的动态类型被描述为空或非空，因此一个不包含任何值的nil接口值和一个刚好包含nil指针的接口值是不同的，后者不为 nil。 你可以通过使用w==nil或者w!=nil来判读接口值是否为空。调用一个空接口值上的任意方法都会产生panic。调用一个包含 nil 指针的接口上的方法是否会报错，取决于接口内包含的动态类型。 123456789101112// w，f 都是特定类型的空值，将他们赋值给 w 都将得到一个 包含nil指针的接口值var w io.Writervar f *os.Filevar buf *bytes.Buffer// 对 *os.File的类型，nil是一个有效的接收者，所以不会报错w = f w.Writer()// (*bytes.Buffer).Write方法的接收者必须非空，调用会报错w = bufbuf，Writer() w = os.Stdout这个赋值过程调用了一个具体类型到接口类型的隐式转换，这和显式的使用io.Writer(os.Stdout)是等价的。此时这个接口值的动态类型被设为*os.File指针的类型描述符，它的动态值持有os.Stdout的拷贝； w = nil这个重置将它所有的部分都设为nil值，把变量w恢复到和它之前定义时的状态。 一个接口值可以持有任意大的动态值。从概念上讲，不论接口值多大，动态值总是可以容下它。（这只是一个概念上的模型；具体的实现可能会非常不同） 3.2 接口比较接口值可以使用==和!＝来进行比较。两个接口值相等仅当它们都是nil值或者它们的动态类型相同并且动态值也根据这个动态类型的==操作相等。因为接口值是可比较的，所以它们可以用在map的键或者作为switch语句的操作数。 然而，如果两个接口值的动态类型相同，但是这个动态类型是不可比较的（比如切片），将它们进行比较就会失败并且panic: 12var x interface&#123;&#125; = []int&#123;1, 2, 3&#125;fmt.Println(x == x) // panic: comparing uncomparable type []int 考虑到这点，接口类型是非常与众不同的。其它类型要么是安全的可比较类型（如基本类型和指针）要么是完全不可比较的类型（如切片，映射类型，和函数），但接口的可比性取决接口包含的动态类型。但是在比较接口值或者包含了接口值的聚合类型时，我们必须要意识到潜在的panic。同样的风险也存在于使用接口作为map的键或者switch的操作数。只能比较你非常确定它们的动态值是可比较类型的接口值。 通过 fmt包的%T 动作，我们可以获取接口值的动态类型，在fmt包内部，使用反射来获取接口动态类型的名称。关于反射，我们后面在详述。 123456var w io.Writerfmt.Printf(\"%T\\n\", w) // \"&lt;nil&gt;\"w = os.Stdoutfmt.Printf(\"%T\\n\", w) // \"*os.File\"w = new(bytes.Buffer)fmt.Printf(\"%T\\n\", w) // \"*bytes.Buffer\" 4. 类型断言类型断言是我们使用 Go 语言中接口的另一种方式。前面的第一个方式中，一个接口的方法表达了实现这个接口的具体类型间的相似性，但是隐藏了代表的细节和这些具体类型本身的操作。重点在于方法上，而不是具体的类型上。 第二种使用方式利用了一个接口值可以持有各种具体类型值的能力并且将这个接口认为是这些类型的 union（联合）。类型断言用来动态地区别出接口包含的每一个类型，做不同处理。在这个方式中，重点在于具体的类型满足这个接口，而不是在于接口的方法（如果它确实有一些的话），并且没有任何的信息隐藏。我们将以这种方式使用的接口描述为discriminated unions（可辨识联合）。 通过类型断言，我们至少可以实现下面这些目标: 区别错误类型 判断对象是否支持特定的方法 4.1 语法x.(T): x - 表示待判断的接口类型，T - 表示断言的类型 如果 T 是一个具体类型，类型断言检查 x 的动态类型是否和T相同，相同，返回 x 的动态值 如果 T 是一个接口类型，类型断言检查 x 的动态类型是否满足T，满足，返回包含 x 动态类型和动态值的接口 T 的值 12345678910111213141516// 具体类型断言var w io.Writerw = os.Stdoutrw := w.(io.ReadWriter) // success: *os.File has both Read and Writew = new(ByteCounter) rw = w.(io.ReadWriter) // panic: *ByteCounter has no Read method， 断言失败触发 panic// 接口类型断言var w io.Writer = os.Stdoutf, ok := w.(*os.File) // success: ok, f == os.Stdoutb, ok := w.(*bytes.Buffer) // failure: !ok, b == nil// 通过第二个变量接受断言是否成功，替代断言失败时的异常if w, ok := w.(*os.File); ok &#123; // if 引出了新的作用域，因此这里发生的是对变量名的重新，发生了变量的覆盖，不是变量的重新赋值。// ...use w...&#125; 换句话说，对一个接口类型的断言改变了类型的表述方式，改变了可以获取的方法集合（通常更大），我们几乎不需要对一个更少限制性的接口类型（更少的方法集合）做断言，因为它表现的就像赋值操作一样，除了对于nil接口值的情况。如果断言操作的对象是一个nil接口值，那么不论被断言的类型是什么这个类型断言都会失败。 123// 对更小的接口无需断言，可直接赋值w = rw // io.ReadWriter is assignable to io.Writerw = rw.(io.Writer) // fails only if rw == nil 4.2 类型开关类型断言有一个 Switch 的便捷语法，称为类型开关，一个类型开关像普通的switch语句一样，它的运算对象是x.(type)－它使用了关键词字面量type－并且每个case有一到多个类型。一个类型开关基于这个接口值的动态类型使一个多路分支有效。一个使用示例如下所示: 1234567891011121314151617func sqlQuote(x interface&#123;&#125;) string &#123; switch x := x.(type) &#123; case nil: return \"NULL\" case int, uint: return fmt.Sprintf(\"%d\", x) // x has type interface&#123;&#125; here. case bool: if x &#123; return \"TRUE\" &#125; return \"FALSE\" case string: return sqlQuoteString(x) // (not shown) default: panic(fmt.Sprintf(\"unexpected type %T: %v\", x, x)) &#125;&#125; 这个示例还展示了，类型开关语句的一个扩展的形式，它可以将提取的值绑定到一个在每个case范围内的新变量 switch x := x.(type) { /* ... */ }。在这个版本的函数中，在每个单一类型的case内部，变量x和这个case的类型相同。例如: 变量 x 在bool的case中是bool类型和string的case中是string类型 在所有其它的情况中，变量x是 switch 运算对象的类型（接口）；在这个例子中运算对象是一个interface{} 当多个 case 需要相同的操作时，比如int和uint的情况，类型开关可以很容易的合并这些情况","categories":[{"name":"Go","slug":"Go","permalink":"http://yoursite.com/categories/Go/"}],"tags":[{"name":"go语言入门","slug":"go语言入门","permalink":"http://yoursite.com/tags/go语言入门/"}]},{"title":"6 go 方法","slug":"go/go_grammar/go_6","date":"2019-01-05T16:00:00.000Z","updated":"2019-04-14T01:55:16.588Z","comments":true,"path":"2019/01/06/go/go_grammar/go_6/","link":"","permalink":"http://yoursite.com/2019/01/06/go/go_grammar/go_6/","excerpt":"Go 的对象组合技术","text":"Go 的对象组合技术 1. 内容概要方法是面向对象编程(OOP)中的概念。有关 OOP 的定义我也说不清楚。但是与概念相比，更重要的是OOP的两个关键点:封装和组合。我们的目的是看看 Go 语言如何通过结构体嵌入等技术实现这两个关键点。 Go 语言中的方法和接口密切相关，接口是 Go 语言提供的用来支持泛型编程的核心组件，我们会在下一章详细讨论。现在我们只需要明白: 方法是与特定类型关联的函数，可以被声明到任意命名类型，包括 Go 的内置类型;但不能是一个指针或者一个接口类型 方法分为值方法和指针方法两类，这会影响到类型是否属于特定接口的判断 2. 方法2.1 方法声明在函数声明时，在其名字之前放上一个变量，即是一个方法。这个附加的参数会将该函数附加到这种类型上，即相当于为这种类型定义了一个独占的方法。 123456789101112type Point struct&#123; X, Y float64 &#125;// 1. 为 Point 定义一个值方法// 参数p，叫做方法的接收器(receiver)func (p Point) Distance(q Point) float64 &#123; return math.Hypot(q.X‐p.X, q.Y‐p.Y)&#125;// 2. 调用方法p := Point&#123;1, 2&#125;q := Point&#123;4, 6&#125;fmt.Println(p.Distance(q)) // \"5\", method call 从上面的示例可以看出来，在方法的定义和调用等行为上，Go 与 Python 并没有什么太大差别。有一点不同的是，当出现命名冲突时，Python 的默认行为是覆盖，而 Go 在编译阶段就直接失败。此外需要注意的是方法和属性在同一命名空间，因此它们之间的命名冲突也是不允许的。 2.2 值方法与指针方法前面函数的部分我们说过，Go 中实参通过值的方式传递。类似的，传递给方法接收器的对象也是按值传递。在上面的 Distance 内接收器 p 是外部 p 对象的拷贝。相对应的我们可以像下面这样，用其指针而不是对象来声明方法。 1234func (p *Point) ScaleBy(factor float64) &#123; p.X *= factor p.Y *= factor&#125; 2.3 接收器限制只有类型(Point)和指向他们的指针(*Point)，才是可能会出现在接收器声明里的两种接收器。为了避免歧义，在声明方法时，如果一个类型名本身是一个指针的话，是不允许其出现在接收器中的，比如下面这个例子。即我们不能为指针定义方法。 12type P *intfunc (P) f() &#123; /* ... */ &#125; // compile error: invalid receiver type 2.4 方法调用中的隐式转换原则上，类型 Point只能调用其值方法，*Point只能调用其指针方法。这样在方法的调用中会有很多转换操作。幸运的是，Go 为我们提供了隐示的转换，就像我们直接通过指针去访问结构的成员变量一样。 12345678910p := Point&#123;1, 2&#125;pptr := &amp;p// type --&gt; *typep.ScaleBy(2) // 等同于(&amp;p).ScaleBy(2)// *type --&gt; typepptr.Distance(q) // 等同于(*pptr).Distance(q) 需要特别注意的是 type --&gt; *type 转换的前提是对象是可取址的。我们不能通过一个无法取到地址的接收器来调用指针方法，比如临时变量： 1Point&#123;1, 2&#125;.ScaleBy(2) // compile error: can't take address of Point literal 2.5 类型的方法集合如上所述，正因为我们总是可以通过对一个地址解引用(*)来获取变量，但是却不一定能获取一个对象的地址(临时对象)，所以一个自定义数据类型的方法集合中仅会包含它的所有值方法，而该类型的指针类型的方法集合却囊括了前者的所有方法，包括所有值方法和所有指针方法。 3. 结构体嵌入3.1 结构体嵌入与类的继承在结构体一节中，我们就已经提到了，结构体中通过匿名字段嵌入的不仅仅是结构体的成员还是其方法。以下面嵌入了 Point 的 ColoredPoint 为例，我们可以把ColoredPoint类型当作接收器来调用Point里的方法，即使ColoredPoint里没有声明这些方法。 12345678910111213141516import \"image/color\"type Point struct&#123; X, Y float64 &#125;type ColoredPoint struct &#123; Point Color color.RGBA&#125;red := color.RGBA&#123;255, 0, 0, 255&#125;blue := color.RGBA&#123;0, 0, 255, 255&#125;var p = ColoredPoint&#123;Point&#123;1, 1&#125;, red&#125;var q = ColoredPoint&#123;Point&#123;5, 4&#125;, blue&#125;fmt.Println(p.Distance(q.Point)) // \"5\"p.ScaleBy(2)q.ScaleBy(2)fmt.Println(p.Distance(q.Point)) // \"10\" 这种行为看起来跟 OOP 类的继承一样，但是有本质区别。最明显的地方是，在类的继承中，子类的实例也是基类的实例，但是在结构体嵌入中，ColoredPoint 类型的”实例”，并不是 Point 的”实例”。 请注意上面例子中对Distance方法的调用。尽管q有着Point这个内嵌类型，但是q并不是一个Point类，我们必须要显式地选择它。 12p.Distance(q.Point) // rightp.Distance(q) // compile error: cannot use q (ColoredPoint) as Point 在 Go 的结构体嵌入中，我们只能说 ColoredPoint has a Point 而不能说 ColoredPoint 继承自 Point。内嵌可以使我们将复杂类型的定义拆分，将字段先按小类型分组，然后定义小类型的方法，之后再把它们组合起来。 3.2 嵌入命名类型的指针在类型中内嵌的匿名字段也可能是一个命名类型的指针，添加这一层间接关系让我们可以共享通用的结构并动态地改变对象之间的关系。 12345678910111213type ColoredPoint struct &#123; *Point Color color.RGBA&#125;p := ColoredPoint&#123;&amp;Point&#123;1, 1&#125;, red&#125;q := ColoredPoint&#123;&amp;Point&#123;5, 4&#125;, blue&#125;// 注意访问 *q.Point 的区别fmt.Println(p.Distance(*q.Point)) // \"5\"q.Point = p.Point // p and q now share the same Pointp.ScaleBy(2)fmt.Println(*p.Point, *q.Point) // \"&#123;2 2&#125; &#123;2 2&#125;\" 3.3 多匿名字段的查找顺序如果结构体中嵌入了多个匿名字段，将遵循下面的字段和方法查找顺序: 直接定义在类型里方法 内嵌字段引入的方法 内嵌字段的内嵌字段引入的方法，然后一直递归向下找 如果在同一级里有两个同名的方法，编译器会报错 上面说的同一级可以理解为，由内嵌所构成的树的同一层。 12345678910111213141516171819202122232425262728293031type A struct &#123; A1&#125;type A1 struct &#123;&#125;type B struct &#123; B1&#125;type B1 struct&#123;&#125;func (a A1) name() &#123; fmt.Println(\"a1\")&#125;func (b B1) name() &#123; fmt.Println(\"b1\")&#125;type C struct &#123; A B&#125;c := C&#123;&#125;// 同一级的 A1，B1 的 同名 name 方法导致编译错误c.name() // ambiguous selector c.name 4. 封装一个对象的变量或者方法如果对调用方是不可见的话，一般就被定义为“封装”。封装有时候也被叫做信息隐藏，同时也是面向对象编程最关键的一个方面。 Go语言只有一种控制可见性的手段：大写首字母的标识符会从定义它们的包中被导出，小写字母的则不会。这种限制包内成员的方式同样适用于struct或者一个类型的方法。因而如果我们想要封装一个对象，我们必须将其定义为一个struct。 这种基于名字的手段使得在语言中最小的封装单元是package。一个struct类型的字段对同一个包的所有代码都有可见性，无论你的代码是写在一个函数还是一个方法里。","categories":[{"name":"Go","slug":"Go","permalink":"http://yoursite.com/categories/Go/"}],"tags":[{"name":"go语言入门","slug":"go语言入门","permalink":"http://yoursite.com/tags/go语言入门/"}]},{"title":"5 go 函数","slug":"go/go_grammar/go_5","date":"2019-01-04T16:00:00.000Z","updated":"2019-04-14T01:55:16.587Z","comments":true,"path":"2019/01/05/go/go_grammar/go_5/","link":"","permalink":"http://yoursite.com/2019/01/05/go/go_grammar/go_5/","excerpt":"函数，代码封装的基本单元","text":"函数，代码封装的基本单元 1. 函数函数通常使用起来并不复杂，定义或声明函数后，直接使用即可。但是为了函数更加易用，编程语言会为函数添加很多特性。在 Python 和 Go 中，函数都是一等”公民”，即函数可以用在任何变量可以使用的地方，并且具有类型。因此接下来我们按照下面的顺序来讲解 Go 函数的相关内容: 第一部分: Go 函数作为基础数据类型的特性: 函数声明 函数的类型 函数的零值 第二部分: Go 函数语言层的特性 匿名函数与闭包 异常处理 Deferred 2. 函数2.1 函数声明Go 函数声明包括函数名、形式参数列表、返回值列表（可省略）以及函数体。函数的参数，返回值以及函数调用时的传值方式是函数的核心。 123func name(parameter‐list) (result‐list) &#123; body&#125; 下面是几个函数声明的示例:1234567891011121314151617func hypot(x, y float64) float64 &#123; return math.Sqrt(x*x + y*y)&#125;// 参数类型相同时，可以合并func f(i, j, k int, s, t string) &#123; /* ... */ &#125;func f(i int, j int, k int, s string, t string) &#123; /* ... */ &#125;//func add(x int, y int) int &#123;return x + y&#125;func sub(x, y int) (z int) &#123; z = x ‐ y; return&#125;func first(x int, _ int) int &#123; return x &#125; // _ 可以强调某个参数未被使用func zero(int, int) int &#123; return 0 &#125;// 在返回值的类型都相同时， 返回值变量名可以传达函数返回值的含义func Size(rect image.Rectangle) (width, height int)func Split(path string) (dir, file string) 返回值与 Python 默认返回 None 不同，Go 有返回值列表，但是没有默认的返回值，返回值列表就是对函数返回值的约束: 返回值列表描述了函数返回值的变量名以及类型 如果没有返回值列表，函数不能返回任何值 如果包含返回值列表，函数必须返回与返回值列表类型相符的值 返回值可以被命名，此时每个返回值被声明成一个局部变量，并根据返回值的类型，被其初始化为 0 当如果函数返回一个无名变量或者没有返回值，返回值列表的括号可以省略。 Go 的函数返回值符合 Go 强变量类型的约束。 参数Go 函数参数没有默认值，也不能通过参数名指定行参。每一次函数调用都必须按照声明顺序为所有参数提供实参（参数值）。因此形参和返回值的变量名对于函数调用者而言没有意义。 为了让函数更加通用，Go 和 Python 都提供了可变参数的特性。在 Go 中声明可变参数时，需要在参数列表的最后一个参数类型之前加上省略符号“…”，这表示该函数会接收任意数量的该类型参数。 123456789101112func sum(vals...int) int &#123;total := 0for _, val := range vals &#123; total += val &#125; return total&#125;//fmt.Println(sum()) // \"0\"fmt.Println(sum(3)) // \"3\"fmt.Println(sum(1, 2, 3, 4)) // \"10\" 在上面的代码中，调用者隐式的创建一个数组，并将原始参数复制到数组中，再把数组的一个切片作为参数传给被调函数。如果原始参数已经是切片类型，可以像下面这样向函数传递参数。 123//values := []int&#123;1, 2, 3, 4&#125;fmt.Println(sum(values...)) // \"10\" 2.2 函数类型与值Go 中函数的类型被称为函数的标识符，函数的取决于参数和返回值的类型: 如果两个函数形式参数列表和返回值列表中的变量类型一一对应，那么它们有相同的类型和标识符 形参和返回值的变量名不不会影响函数标识符 函数类型的零值是 nil。调用值为nil的函数值会引起panic错误。函数值可以与nil比较，但是函数值之间是不可比较的，也不能用函数值作为map的key。函数之间之所以不可比，是因为函数闭包，函数会保留定义函数时，存在的自由变量的绑定。我们会在下面讲解。 123456789// 此处f的值为nil, 会引起panic错误var f func(int) intf(3)// 函数与 nil 比较var f func(int) intif f != nil &#123; f(3)&#125; 2.3 函数调用的传值方式我们把调用函数时传递给函数的值称为实参，函数接收参数值的变量称为行参。 Go 中实参通过值的方式传递，因此函数的形参是实参的拷贝。对形参进行修改不会影响实参。但是，如果实参包括引用类型，如指针，slice(切片)、map、function、channel等类型，实参可能会由于函数的间接引用被修改。 在函数体中，函数的形参作为局部变量，被初始化为调用者提供的值。函数的形参和有名返回值作为函数最外层的局部变量，被存储在相同的词法块中。我们甚至可以直接修返回值变量，来修改函数的返回值。我们会在讲解 Deffer 时详述。 说完了函数作为基本类型的特性，我们再来看为了方便编程，Go 为函数提供的语言层特性。 3. 函数特性3.1 函数闭包Go 里面一个有意思的地方是拥有函数名的函数只能在包级语法块中被声明。即我们不能在函数内部使用，使用 func name(parameter‐list) (result‐list) 方式定义函数，但不带 name 的 func (parameter‐list) (result‐list) 匿名函数可以。func (parameter‐list) (result‐list) 是 Go 函数的函数字面量。函数值字面量是一种表达式，它的值被成为匿名函数（anonymousfunction） 说起来比较绕，即如果我们想在函数内定义命名函数必须使用下面这种方式；或者直接使用匿名函数。 123456789101112131415161718192021// 1. 函数内定义命名函数func f1(a, b int) (r int) &#123; v := func() &#123; r += b &#125; defer v() return a + b&#125;// 2. 直接使用匿名函数func squares() func() int &#123; var x int return func() int &#123; x++ return x * x &#125;&#125;f := squares()fmt.Println(f()) // \"1\"fmt.Println(f()) // \"4\" 注意在上面第二个示例中，squares中定义的匿名内部函数可以访问和更新squares中的局部变量，这意味着匿名函数和squares中存在变量引用。这就是函数闭包，也是函数值属于引用类型和函数值不可比较的原因。 需要注意的是函数闭包内保存的是变量的引用而不是变量的值。我们来看下面删除临时文件的示例: 12345678var rmdirs []func()for _, dir := range tempDirs() &#123; // dir := d // NOTE: necessary! os.MkdirAll(dir, 0755) rmdirs = append(rmdirs, func() &#123; os.RemoveAll(dir) // NOTE: incorrect! &#125;)&#125; 在上面的程序中，for循环语句引入了新的词法块，循环变量dir在这个词法块中被声明。在该循环中生成的所有函数值都共享相同的循环变量。需要注意，函数值中记录的是循环变量的内存地址，而不是循环变量某一时刻的值。以dir为例，后续的迭代会不断更新dir的值，当删除操作执行时，for循环已完成，dir中存储的值等于最后一次迭代的值。这意味着，每次对os.RemoveAll的调用删除的都是相同的目录。 如果你使用go语句或者defer语句会经常遇到此类问题。这不是go或defer本身导致的，而是因为它们都会等待循环结束后，再执行函数值。 3.2 Defer 机制Go 的 Defer 机制与 Python 的上下文管理器有点类似，都是为了保证某些代码一定要执行，无论代码是否出现了异常。 defer 的语法很简单，只需要在调用普通函数或方法前加上关键字defer。 当defer语句被执行时，跟在defer后面的函数会被延迟执行。 直到包含该defer语句的函数执行完毕时，defer后的函数才会被执行，不论包含defer语句的函数是通过return正常结束，还是由于panic导致的异常结束。 可以在一个函数中执行多条defer语句，它们的执行顺序与声明顺序相反。 通过defer机制，不论函数逻辑多复杂，都能保证在任何执行路径下，资源被释放。释放资源的defer应该直接跟在请求资源的语句后。需要注意的是跟在 defer 之后的是函数调用，而不是函数本身。 12345678910111213141516171819// defer 关闭文件package ioutilfunc ReadFile(filename string) ([]byte, error) &#123; f, err := os.Open(filename) if err != nil &#123; return nil, err &#125; defer f.Close() return ReadAll(f)&#125;// 释放锁var mu sync.Mutexvar m = make(map[string]int)func lookup(key string) int &#123; mu.Lock() defer mu.Unlock() return m[key]&#125; 利用 defer中的函数会在return语句更新返回值变量后再执行，以及在函数中定义的匿名函数可以访问该函数包括返回值变量在内的所有变量，我们就可以上面说到的改变函数返回值的目的。 123456func triple(x int) (result int) &#123; defer func() &#123; result += x &#125;() return double(x)&#125;fmt.Println(triple(4)) // \"12 3.3 错误与异常处理严格的区分错误和异常，应该是 Go 编码风格一个最大的特点。在Go中，错误是程序运行的几个预期的结果之一。而异常是未被预料到的错误，即bug，而不是那些在健壮程序中应该被避免的程序错误。正因为如此，在 Go 的代码中你会看到很多类似下面的条件判断。Go 将对错误的处理放在了代码的逻辑控制中，让程序员更多的关注错误。 1234567891011// 导致失败的原因只有一个，额外的返回值可以是一个布尔值，通常被命名为okvalue, ok := cache.Lookup(key) if !ok &#123; // ...cache[key] does not exist…&#125;// 导致失败的原因不止一种时，额外的返回值是error类型，resp, err := http.Get(url)if err != nil&#123; return nill, err&#125; 错误处理对于那些将运行失败看作是预期结果的函数，它们会返回一个额外的返回值，通常是最后一个，来传递错误信息。调用者需要处理程序出现的潜在错误。因此Go中大部分函数的代码结构几乎相同，首先是一系列的初始检查，防止错误发生，之后是函数的实际逻辑。 对于函数返回的错误，通常有以下五种处理方式: 传播错误 重新尝试失败的操作 输出错误信息并结束程序 有时，只输出错误信息就足够了，不需要中断程序的运行 直接忽略掉错误 需要注意的是，输出错误信息并结束程序只应在main中执行。对库函数而言，应仅向上传播错误，除非该错误意味着程序内部包含不一致性，即遇到了bug，才能在库函数中结束程序。 异常处理Go 中的异常称为 Panic。一般而言，当panic异常发生时，程序会中断运行，并立即执行在该goroutine 中被延迟的函数（defer 机制），在Go的panic机制中，延迟函数的调用在释放堆栈信息之前。直接调用内置的panic函数也会引发panic异常，panic函数接受任何值作为参数。 通常来说，不应该对panic异常做任何处理，但有时候我们需要从异常中恢复，此时就需要 Go 的 Recover 机制来捕获异常。 12345678func Parse(input string) (s *Syntax, err error) &#123; defer func() &#123; if p := recover(); p != nil &#123; err = fmt.Errorf(\"internal error: %v\", p) &#125; &#125;()// ...parser...&#125; 如上所示，如果在deferred函数中调用了内置函数recover，并且定义该defer语句的函数发生了panic异常，recover会使程序从panic中恢复，并返回panic value。导致panic异常的函数不会继续运行，但能正常返回。在未发生panic时调用recover，recover会返回nil。 通常我们不应该不加区分的恢复所有的panic异常，同时作为被广泛遵守的规范，也不应该试图去恢复其他包引起的panic。安全的做法是有选择性的recover。 为了标识某个panic是否应该被恢复，我们可以将panic value设置成特殊类型。在recover时对panic value进行检查，如果发现panic value是特殊类型，就将这个panic作为errror处理，如果不是，则按照正常的panic进行处理。 123456789101112func soleTitle(doc *html.Node) (title string, err error) &#123; type bailout struct&#123;&#125; defer func() &#123; switch p := recover(); p &#123; case nil: // no panic case bailout&#123;&#125;: // \"expected\" panic err = fmt.Errorf(\"multiple title elements\") default: panic(p) // unexpected panic; carry on panicking &#125; &#125;()&#125; 最后某些致命错误会导致Go在运行时终止程序，无法恢复，比如内存不足。","categories":[{"name":"Go","slug":"Go","permalink":"http://yoursite.com/categories/Go/"}],"tags":[{"name":"go语言入门","slug":"go语言入门","permalink":"http://yoursite.com/tags/go语言入门/"}]},{"title":"4 go 复合数据类型","slug":"go/go_grammar/go_4","date":"2019-01-03T16:00:00.000Z","updated":"2019-04-14T01:55:16.587Z","comments":true,"path":"2019/01/04/go/go_grammar/go_4/","link":"","permalink":"http://yoursite.com/2019/01/04/go/go_grammar/go_4/","excerpt":"Go 的类型系统","text":"Go 的类型系统 1. Go 的复合数据类型接着上一篇，我们来继续讨论 Go 里面的复合数据类型，包括数组、slice、map和结构体。数组和结构体是聚合类型；它们的值由许多元素或成员字段的值组成。slice,map 分别与 Python 中的 array.Array,dict 相对应，它们是 Go 提供给我们容器数据类型。 编程语言提供的复合数据类型应该是数据结构与算法的基础内容，如果你熟悉常用的数据结构，对复合类型的特性和支持的操作应该很容易就能理解。因此接下来的内容，我们会先简单说一说数据结构的特点，然后在介绍它们在 Go 中的实现和支持的操作。 2. 数组数组应该是最基本的数据结构，简单来说，数组具有如下特性: 数组是一段连续的内存空间，用来存储一组具有相同类型的数据 数组一经创建，大小便不能更改，连续的内存要求元素之间不能出现空洞 数组的特性决定了数组天然支持基于下标的“随机访问”(索引)。Go 中的数组我们需要关注以下几个知识点: 数组的长度是数组类型的一个组成部分，[3]int和[4]int是两种不同的数组类型 数组的长度必须是常量表达式，因为数组的长度需要在编译阶段确定 数组的可比性取决于数组的类型是否相同以及数组元素是否可比，只有当两个数组类型相同并且所有元素都是相等的时候数组才相等 下面是数组常用操作的代码示例: 1234567891011121314151617181920212223242526// 1. 数组字面量var q [3]int = [3]int&#123;1, 2, 3&#125;q := [...]int&#123;1, 2, 3&#125; // “...”省略号，表示数组的长度是根据初始化值的个数来计算r := [...]int&#123;99: ‐1&#125; // 直接按位置初始化，未初始化的为对应类型的零值// 2. 索引和切片fmt.Println(q[0]) // print the first elementfmt.Println(q[len(q)‐1]) // print the last element, q[2]e := [3]int&#123;1, 2, 3&#125;ff := e[0:2] // 对数组切片返回的是 slice 而不是原数组类型if ff == e &#123; // missmatch type []int and [3]int&#125;// 3. for 循环迭代for i, v := range q &#123; fmt.Printf(\"%d %d\\n\", i, v)&#125;// 4. 数组可比性a := [2]int&#123;1, 2&#125;d := [3]int&#123;1, 2&#125;fmt.Println(a == d) // compile error: cannot compare [2]int == [3]int 3. slice 切片因为数组的大小固定，类型限定严格，我们通常很少直接使用数组，使用更多的是数组的容器，Go 中数组的容器类型就是 slice (切片)。容器的最主要作用是能够根据元素大小对数组进行扩缩容。因此我们可以从 slice 的组成和扩缩容两个方面去理解 slice。 3.1 slice 组成Go 的 slice由三个部分构成： 指针: 指针指向第一个slice元素对应的底层数组元素的地址 容量: 容量一般是从 slice 的开始位置到底层数据的结尾位置 长度: 对应slice中元素的数目，长度不能超过容量 需要注意的是，因为 slice 底层数组是可以共享(通常是由于切片行为引起的)，因此slice 指针指向的第一个元素并不一定就是数组的第一个元素。内置的len和cap函数分别返回slice的长度和容量。下面是一个 slice 结构示意图: 1234months := [...]string&#123;1: \"January\", /* ... */, 12: \"December\"&#125;Q2 := months[4:7]summer := months[6:9] 对数组 months 的切片操作返回的是 slice []int，Q2和summer 共用了底层的 months 数组。 3.2 slice 扩缩容slice 扩缩容策略由 append 函数实现，但 append 只能向slice追加元素，Go 并没有删除 slice 中元素的函数。append扩容的过程大体是这样的: 在每次向 slice 添加时，append 会判断当前数组的大小是否足以容纳新增元素，足够则直接插入 如果数组容量不够，append 将创建一个原有数组两倍大小的新数组，并将原数组中的元素拷贝到新数组中去 最后将 slice 中的指针的指向新的底层数组 append 函数可以向 slice 追加多个元素，甚至追加一个slice: 123456var x []intx = append(x, 1)x = append(x, 2, 3)x = append(x, 4, 5, 6)x = append(x, x...) // append the slice xfmt.Println(x) // \"[1 2 3 4 5 6 1 2 3 4 5 6]\" 需要注意的是，通常我们要将 append 的返回值直接赋值给输入的slice变量，这么做与 Go 中函数的参数传值方式有关: Go 中的函数参数是按值传递的，因此传入 append 的是 slice 的副本，但是它们的指针指向了相同的底层数组 如果 append 函数发生了扩容，函数内的 slice 副本将指向新的内存数组，此时 append 函数将不会影响到传入的 slice 变量，为了达到修改 slice 的目的，通常要对输入的slice变量重新赋值 3.3 slice 操作说完了 slice 的实现，我们再来看看 slice 支持的操作: slice 的字面量与数组类似，只是去掉长度声明 对 slice 的切片操作如果超出cap(s)的上限将导致一个panic异常，但是超出len(s)则是意味着扩展了slice，新slice的长度会变长 为了避免创建 slice 多次内存分配，内置的 make 函数可以创建指定长度和容量的 slice slice之间不能比较，我们不能使用==操作符来判断两个slice是否含有全部相等元素，slice唯一合法的比较操作是和nil比较 因为 Go 没有提供删除 slice 元素的函数，只能采用覆盖的方式进行 slice 元素删除 下面是 slice 常用操作的代码示例: 12345678910111213141516171819202122232425262728293031323334353637383940414243// 1. slice 字面量var m = []int&#123;3: 10&#125;// 2. slice 创建函数// make创建了一个匿名的数组变量，然后返回一个slicemake([]T, len)make([]T, len, cap) // same as make([]T, cap)[:len]// 3. slice 与 nil 的比较和转换if summer == nil &#123; /* ... */ &#125;var s []int // len(s) == 0, s == nils = nil // len(s) == 0, s == nils = []int(nil) // len(s) == 0, s == nil，类型转换s = []int&#123;&#125; // len(s) == 0, s != nil// 4. slice 为空测试，不应该使用 s == nilif len(s) == 0&#123;&#125;// 5. slice 复制// copy函数可以方便地将一个slice复制另一个相同类型的slice// copy函数将返回成功复制的元素的个数，等于两个slice中较小的长度copy(m, s) // 将 s 复制到 m// 6. slice 元素删除//如果要保持 slice 原来顺序func remove(slice []int, i int) []int &#123; copy(slice[i:], slice[i+1:]) return slice[:len(slice)‐1]&#125;//如果不用保持原来顺序的话，使用最后元素覆盖删除元素func remove(slice []int, i int) []int &#123; slice[i] = slice[len(slice)‐1] return slice[:len(slice)‐1]&#125;// 7. slice 模拟栈操作stack = append(stack, v) // push vtop := stack[len(stack)‐1] // top of stackstack = stack[:len(stack)‐1] // pop 4. Map 散列表在Go语言中，一个map就是一个散列表的引用，散列表是映射的一种实现方式，因此要想理清楚散列表，我们要从映射入手。所谓映射就是支持以下方法的键值对: M[k]: 返回键 k 对应的值，对应 Python __getitem__ M[k]=v: 对应 Python __setitem__ del M[k]: 对应 Python __delitem__ len(M): 对应 Python __len__ iter(M): 迭代映射 M 中的所有键，对应 Python __iter__ 我列出了 Python 中与之对应的方法，但是 Go 中实现方式有所不同，我们会在下面讲解。散列表是映射高效的实现方式，可以实现 O(1) 时间复杂度的元素查找。那散列表是如何实现的呢？ 4.1 散列表的实现散列表是数组的一种扩展，利用的是数组支持按照下标随机访问的特性，通过散列函数把元素的键映射为数组的下标来实现在数组中保存和查询元素。在整个散列表的实现中，有三个核心问题： 散列函数设计 散列冲突的解决 装载因子以及散列表的动态扩容 下面是散列表实现映射的示意图: 限于篇幅的原因，有关散列表的实现，我就不过多解释，不了解的同学可以看看这篇文章散列表实现。这里我们需要关注的是散列表在使用上的限制。 首先，由于映射过程以及散列冲突的存在，所有的编程语言的散列表都会有以下两点要求: key 不可变，如果key 可变，元素的哈希值就会变化，查找就会失败 key 之间可比，当发生散列冲突时，要通过比较进行二次查找 而 Go 对散列表使用更加严格: 散列表中所有的key必须是相同的类型，所有的value也必须是相同的类型，但是 key 和 value 的类型可以不同 因为 Go 中可变的元素都是不可比的，所以上面的条件就退化成 key 必须是支持==比较运算符的数据类型,例如整数、数组或结构体等 虽然浮点数类型也是支持相等运算符比较的，但是将浮点数用做key类型则是一个坏的想法，最坏的情况是可能出现的NaN和任何浮点数都不相等 4.2 map 操作说完了散列表的实现，接下来我们看看 Go map 支持的操作。在Go语言中，一个map就是一个哈希表的引用，map类型可以写为map[K]V，其中K和V分别对应key和value。与 slice 类似，我们可以使用字面量和 make 来创建 map。 map 支持上面所说的映射操作，但是与 Python 相比 Go map 有以下两个鲜明特点: key 不存在时，执行 M[key]，不会触发异常，而是返回 value 类型对应的零值 map类型的零值是nil，也就是没有引用任何哈希表，map上的查找、删除、len和range循环都可以安全工作在nil值的map上，它们的行为和一个空的map类似。但是向一个nil值的map存入元素将导致一个panic异常 此外和slice一样，map之间也不能进行相等比较；唯一的例外是和nil进行比较。要判断两个map是否包含相同的key和value，我们必须通过一个循环实现。下面 map 操作的代码示例: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// 1. 字面量ages := map[string]int&#123; \"alice\": 31, \"charlie\": 34,&#125;// 2. 初始化函数 makeages := make(map[string]int)ages[\"alice\"] = 31ages[\"charlie\"] = 34// 3. 元素访问与删除ages[\"alice\"] = 32fmt.Println(ages[\"alice\"]) // \"32delete(ages, \"alice\") // remove element ages[\"alice\"]// 元素不存在的判断if age, ok := ages[\"bob\"]; !ok &#123; /* ... */ &#125; // 判断元素是否存在// 4. 迭代和遍历，迭代总是随机和无序的for name, age := range ages &#123; fmt.Printf(\"%s\\t%d\\n\", name, age)&#125;// 有序遍历import \"sort\"var names []stringfor name := range ages &#123; names = append(names, name)&#125;sort.Strings(names)for _, name := range names &#123; fmt.Printf(\"%s\\t%d\\n\", name, ages[name])&#125;// 5. 零值，以及是否为空的比较var ages map[string]intfmt.Println(ages == nil) // \"true\"fmt.Println(len(ages) == 0) // \"true\"// 6. 两个相同 map 判等func equal(x, y map[string]int) bool &#123; if len(x) != len(y) &#123; return false&#125; for k, xv := range x &#123; // 注意必须先判断，元素是否存在 if yv, ok := y[k]; !ok || yv != xv &#123; return false &#125; &#125; return true&#125; 最后，Go语言中并没有提供一个set类型，可以通过 map 实现类似set的功能，常用的 map 类型就是map[string]bool。 5. 结构体结构体是一种聚合的数据类型由零个或多个任意类型的值聚合成的实体。每个值称为结构体的成员。结构体是 Go 提供给我们创建自定义类型的载体，下面是一个创建示例: 12345678910type Employee struct &#123; ID int Name, Address string DoB time.Time Position string Salary int ManagerID int&#125;var dilbert Employee struct 定义了一个结构体，type 为这个结构体定义类型别名，便于引用，这种定义方式与 C 很接近。 在结构体的定义上，Go 中还有下面一些特性: 结构体成员的输入顺序也有重要的意义，拥有相同成员但是成员顺序不同的结构体属于不同的结构体类型 如果结构体成员名字是以大写字母开头的，那么该成员就是导出的；这是Go语言导出规则决定的。一个结构体可能同时包含导出和未导出的成员。 结构体的操作稍显复杂，我们分成下面两块来讲解 结构体通用操作，包括成员变量的引用，结构体的创建和比较 结构体的嵌入和匿名变量，这个是 Go 语言的特性，需要重点关注 5.1 结构体通用操作成员引用结构体是一个变量，它所有的成员也同样是变量，可以赋值或者取址，然后通过指针访问。结构体变量的成员可以通过点操作符访问，点操作符也可以和指向结构体的指针一起工作： 123456789101112// 通过点操作直接访问var dilbert Employeedilbert.Salary ‐= 5000// 可以对成员变量取址，然后访问position := &amp;dilbert.Position*position = \"Senior \" + *position // promoted, for outsourcing to Elbonia// 点操作也可以直接用在结构体指针上var employeeOfTheMonth *Employee = &amp;dilbertemployeeOfTheMonth.Position += \" (proactive team player)\" // 等同于(*employeeOfTheMonth).Position += \" (proactive team player)\" 结构体字面量结构体字面值有两种语法格式: 以结构体成员定义的顺序为每个结构体成员指定一个面值，这种方式在结构定义发生变化时就会导致编译错误，因此这种方式只在定义结构体的包内部使用，或者是在较小的结构体中使用，这些结构体的成员排列比较规则 以成员名字和相应的值来初始化，可以包含部分或全部的成员,如果成员被忽略的话将默认用零值 需要注意的是两种不同形式的写法不能混合使用。而且，你不能企图在外部包中用第一种顺序赋值的技巧来偷偷地初始化结构体中未导出的成员。 123456789101112131415// 方式一: 按照成员定义顺序，依次赋值type Point struct&#123; X, Y int &#125;p := Point&#123;1, 2&#125;// 方式二: 以成员名字和相应的值来初始化f := Point&#123;X: 1, Y: 2&#125;// 未导出变量，无法赋值package ptype T struct&#123; a, b int &#125; // a and b are not exportedpackage qimport \"p\"var _ = p.T&#123;a: 1, b: 2&#125; // compile error: can't reference a, bvar _ = p.T&#123;1, 2&#125; // compile error: can't reference a, b 除了字面量外，我们还可以用前面介绍的 new 函数来创建结构体变量 1234pp := &amp;Point&#123;1, 2&#125;pp := new(Point)*pp = Point&#123;1, 2&#125; 结构体的零值与比较结构体类型的零值是每个成员都是零值。如果结构体没有任何成员的话就是空结构体，写作struct{}。它的大小为0，也不包含任何信息，通常用作占位。 如果结构体的全部成员都是可以比较的，那么结构体也是可以比较的。可比较的结构体类型和其他可比较的类型一样，可以用于map的key类型。 1234567type address struct &#123; hostname string port int&#125;hits := make(map[address]int)hits[address&#123;\"golang.org\", 443&#125;]++ 5.2 结构体的嵌入与匿名变量结构体嵌入结构体嵌入是 Go 语言提供的类似类继承机制，形式上是让一个命名的结构体包含另一个结构体类型的匿名成员，目的是实现通过简单的点运算符x.f来访问匿名成员链中嵌套的x.d.e.f成员的机制。说起来很复杂，举个例子。考虑一个图形系统，我们需要定义点，线，圆。显然圆可以在点即园心的基础上添加半径来表示。在 Go 中可以使用下面的结构体表示这样的结构。 1234567891011121314151617181920// 点type Point struct &#123; X, Y int&#125;// 圆type Circle struct &#123; Center Point Radius int&#125;type Wheel struct &#123; Circle Circle Spokes int&#125;// 创建圆var w Wheelw.Circle.Center.X = 8w.Circle.Center.Y = 8w.Circle.Radius = 5w.Spokes = 20 如上所示，现在想访问Wheel的结构体成员 X 将变的异常繁琐。而结构嵌入就是为了在满足上面结构不变的情况，实现 w.X 成员快速访问。结构体声明如下所示: 1234567891011121314151617181920type Point struct &#123; X, Y int&#125;type Circle struct &#123; Point // 匿名成员 Radius int&#125;type Wheel struct &#123; Circle // 匿名成员 Spokes int&#125;var w Wheelw.X = 8 // equivalent to w.Circle.Point.X = 8w.Y = 8 // equivalent to w.Circle.Point.Y = 8w.Radius = 5 // equivalent to w.Circle.Radius = 5w.Spokes = 20 Point，Circle 此时为匿名成员。所谓匿名成员，就是只声明一个成员对应的数据类型而不指名成员的名字。匿名成员并不是没有名字，其名字就是命名的类型名字，但是这些名字在点操作符中是可选的。上面 w.Circle.Point.X = 8 这样的访问方式依旧是合法的。 不幸的是，结构体字面值并没有简短表示匿名成员的语法， 因此下面的语句都不能编译通过。结构体字面值必须遵循形状类型声明时的结构 12345678910111213// 错误w = Wheel&#123;8, 8, 5, 20&#125; // compile error: unknown fieldsw = Wheel&#123;X: 8, Y: 8, Radius: 5, Spokes: 20&#125; // compile error: unknown fields// 正确w = Wheel&#123;Circle&#123;Point&#123;8, 8&#125;, 5&#125;, 20&#125;w = Wheel&#123; Circle: Circle&#123; Point: Point&#123;X: 8, Y: 8&#125;, Radius: 5, &#125;, Spokes: 20, // NOTE: trailing comma necessary here (and at Radius)&#125; 匿名变量的使用要求需要注意的是 Go 对匿名成员的使用存在一些约束: 匿名成员的数据类型必须是命名的类型或指向一个命名的类型的指针 因为匿名成员也有一个隐式的名字，因此不能同时包含两个类型相同的匿名成员，这会导致名字冲突 因为成员的名字是由其类型隐式地决定的，所有匿名成员也有可见性的规则约束 比如将上面改成小写字母开头的point和circle），此时在包内依旧可以使用 w.X = 8；但是在包外部，因为circle和point没有导出不能访问它们的成员，因此简短的匿名成员访问语法也是禁止的。 最后匿名成员并不要求是结构体类型；其实任何命名的类型都可以作为结构体的匿名成员。但是为什么要嵌入一个没有任何子成员类型的匿名成员类型呢？答案是匿名类型的方法集。 简短的点运算符语法可以用于选择匿名成员嵌套的成员，也可以用于访问它们的方法。实际上，外层的结构体不仅仅是获得了匿名成员类型的所有成员，而且也获得了该类型导出的全部的方法。 这个机制可以用于将一个有简单行为的对象组合成有复杂行为的对象。组合是Go语言中面向对象编程的核心。我们在下一章将方法时会再来讨论。","categories":[{"name":"Go","slug":"Go","permalink":"http://yoursite.com/categories/Go/"}],"tags":[{"name":"go语言入门","slug":"go语言入门","permalink":"http://yoursite.com/tags/go语言入门/"}]},{"title":"3 go 基础数据类型","slug":"go/go_grammar/go_3","date":"2019-01-02T16:00:00.000Z","updated":"2019-04-14T01:55:16.587Z","comments":true,"path":"2019/01/03/go/go_grammar/go_3/","link":"","permalink":"http://yoursite.com/2019/01/03/go/go_grammar/go_3/","excerpt":"Go 的类型系统","text":"Go 的类型系统 1. Go 中的数据类型Go语言将数据类型分为四类：基础类型、复合类型、引用类型和接口类型。基础类型，包括：数字、字符串和布尔型。复合数据类型包括数组和结构体(通过组合简单类型，来表达更加复杂的数据结构)。引用类型包括指针、切片、字典、函数、通道，虽然数据种类很多，但它们都是对程序中一个变量或状态的间接引用。函数和通道并不属于我们通常所说的数据类型，我们放在后面相关章节来介绍。 对于大多数编程语言来说，基础类型以及它们之上的可用运算符都是类似，更加需要我们注意的是，编程语言提供给我们的数据容器以及操作它们的方式。因此我们分成以下几个部分来讲解 Go 的类型系统。 数值与布尔型 字符串与编码 数组与结构体 切片 字典 本节我们先来介绍 Go 中的基本数据类型，即数值，布尔值和字符串。在介绍这些数据类型之前，我们先来谈谈变量类型的含义，这有助于加深我们对编程语言本身的理解。 1.1 变量的类型无论什么数据，在存储器内都是 0-1，那数据是数值还是字符完全取决于我们对这些二进制数据的解释。变量的类型就是用来定义对应存储值的属性特征，即它们在内部是如何表示的，支持的操作符，以及关联的方法集等。 而在一个编程语言类型系统中，除了内置的变量类型外，还有如下一些问题: 自定义类型 定义新的类型名称(类型重命名) 类型转换 1.2 自定义类型自定义类型允许我们在编程语言底层类型的基础上定义更加复杂的类型，它是面向对象编程的基础。在 Go 中自定义类型就是使用结构体。 1.2 类型重命名在任何程序中都会存在一些变量有着相同的内部结构，但是却表示完全不同的概念。例如，一个int类型的变量可以用来表示一个循环的迭代索引、或者一个时间戳、或者一个文件描述符。类型重命名就是为分隔不同概念的类型。新的类型名称使用类型声明语句创建。Go 的类型声明语法如下所示: 1type 类型名字 底层类型 新的类型和底层类型具有相同的底层结构，支持和底层类型相同的运算符。但是新类型与底层类型以及基于相同底层类型的不同新类型，是完全不不同的数据类型。 12345678import \"fmt\"type Celsius float64 // 摄氏温度type Fahrenheit float64 // 华氏温度// 因为 Fahrenheit，float64，Celsius 是完全不同的类型，所以它们不能直接比较// compile error: type mismatchfmt.Println(Fahrenheit(1.0) == float64(1.0))fmt.Println(Fahrenheit(1.0) == Celsius(1.0)) 1.2 类型转换对于每一个类型T，都有一个对应的类型转换操作T(x)，用于将x转为T类型。如果T是指针类型，可能会需要用小括弧包装T，比如 (*int)(0)。 在编程语言中，不同类型的变量之间是不能进行直接赋值和比较的，要这样做就需要显示或隐式的类型转换。对于不同编程语言而言，有不同的类型转换规则，但大多数规则都是类似。在 Go 中: 数值之间的转类型转换有一套特定规则，这个规则在不同的编程语言中是一样的，比如将浮点数转换为整数会损失小数部分 显示的类型转换T(x)要求 T 和 x 具有相同的底层基础类型或指向相同底层结构的指针类型；对于数据容器而言需要它们有类似的实现，比如可以将一个字符串转为 []byte类型 自定义的新类型名称，不会自动应用底层类型的隐式类型转换规则，一个命名类型的变量只能和另一个有相同类型的变量，或有着相同底层类型的未命名类型的值之间做比较；依赖相同底层类型的不同自定义类型之间想要进行比较或赋值必须进行显示的类型转换。 1234567891011121314import \"fmt\"// 自定义类型与其底层类型不可比较type tt intfmt.Println(tt(1) &gt; int(0)） // compile error: type mismatchvar c Celsiusvar f Fahrenheitfmt.Println(c == 0) // \"true\"fmt.Println(f &gt;= 0) // \"true\"// 依赖相同底层类型的不同自定义类型不可比较fmt.Println(c == f) // compile error: type mismatchfmt.Println(c == Celsius(f)) // \"true\"! 说了这么多，接下来我们开始正式讲解 Go 中的数据类型。 2. 数值Go语言的数值类型包括几种不同大小的整数、浮点数和复数，还有一些为特定用途定义的类型别名。 2.1 整数整数包括如下几种类型及类型别名： 类型 大小 含义 uint8 8 无符号 8 位整型 uint16 16 无符号 16 位整型 uint32 32 无符号 32 位整型 uint64 64 无符号 64 位整型 uint 32 或 64位 平台相关，取决于CPU平台机器字大小 int 32 或 64位 平台相关，取决于CPU平台机器字大小 int8 8 有符号 8 位整型 int16 16 有符号 16 位整型 int32 32 有符号 32 位整型 int64 64 有符号 64 位整型 byte 8, int8的别名 表示原始的二进制数据 rune 32, int32的别名 Unicode字符，表示一个Unicode码点 uintptr 无符号整数，没有明确指定大小 用于存放一个指针，GO 底层使用 其中int是应用最广泛的数值类型。内置的len函数返回一个有符号的int，虽然使用uint无符号类型似乎是一个更合理的选择。len函数返回有符号 int ，可以使我们像下面这样处理逆序循环。 1234medals := []string&#123;\"gold\", \"silver\", \"bronze\"&#125; for i := len(medals) ‐ 1; i &gt;= 0; i‐‐ &#123; fmt.Println(medals[i]) // \"bronze\", \"silver\", \"gold\"&#125; 所以尽管Go语言提供了无符号数和运算，并且在数值本身不可能出现负数的情况下，我们还是倾向于使用有符号的int类型。出于这个原因，无符号数往往只有在位运算或其它特殊的运算场景才会使用，就像bit集合、分析二进制文件格式或者是哈希和加密操作等。它们通常并不用于仅仅是表达非负数量的场合。 2.2 整数的运算符Go 的整数支持如下操作符号，其中大多数与其他语言类似，只有一个比较特殊x &amp;^ y，它表示将 x 中与 y 对应的且 y 中等于 1 的位置为 0，即位清空(AND NOT) 1234567# 优先级递减* / % # 算数运算符&lt;&lt; &gt;&gt; &amp; &amp;^ # 位运算符+ ‐ # 算数运算符| ^ # 位运算符== != &lt; &lt;= &gt; &gt;= # 比较运算符&amp;&amp;(AND) ||(or) # 逻辑运算符 2.3 浮点数Go语言提供了两种精度的浮点数，float32 和 float64 。浮点数的范围极限值可以在math包找到。常量math.MaxFloat32表示float32能表示的最大数值，对应的 float64 为 math.MaxFloat64。 一个float32类型的浮点数可以提供大约6个十进制数的精度，而float64则可以提供约15个十进制数的精度；通常应该优先使用float64类型。小数点前面或后面的数字都可能被省略（例如.707或1.）。很小或很大的数最好用科学计数法书写，通过e或E来指定指数部分。 1234567// float32的有效bit位只有23个，整数大于23bit表示范围时，将出现误差var f float32 = 16777216 // 1 &lt;&lt; 24fmt.Println(f == f+1) // \"true\"!const a = .909const Avogadro = 6.02214129e23 // 阿伏伽德罗常数const Planck = 6.62606957e‐34 // 普朗克常数 math包中除了提供大量常用的数学函数外，还提供了IEEE754浮点数标准中定义的特殊值的创建和测试。 123456789v := math.Inf(1) // 返回正无穷p := math.Inf(-1) // 返回负无穷n := math.NaN() // 返回 NaN 非数，一般用于表示无效的除法操作结果0/0或Sqrt(­1).t ：= math.IsNaN(n) // 测试是否为 NaN// NaN和任何数都是不相等的nan := math.NaN()fmt.Println(nan == nan, nan &lt; nan, nan &gt; nan) // \"false false false\" 2.4 复数Go语言提供了两种精度的复数类型：complex64 和 complex128，分别对应 float32 和 float64 两种浮点数精度。内置的complex函数用于构建复数，内建的real和imag函数分别返回复数的实部和虚部。复数的字面量使用 i 后缀。 123456789var x complex128 = complex(1, 2) // 1+2ivar y complex128 = complex(3, 4) // 3+4ifmt.Println(x*y) // \"(‐5+10i)\"fmt.Println(real(x*y)) // \"‐5\"fmt.Println(imag(x*y)) // \"10\"// 复数的字面量x := 1 + 2iy := 3 + 4i 3. 布尔值Go 布尔类型的值只有两种：true 和 false，if 和 for 语句的条件部分都是布尔值。需要特别注意的是 在 Go 中布尔之值不会与其他任何类型作隐式转换，将其他类型的值用在 if 或 for 中作为条件判断时，必须作显示的类型转换。 1234567func itob(i int) bool &#123; return i != 0 &#125;b := 0i := 0if itob(b) &#123; i = 1&#125; 4. 字符串4.1 字符串操作创建字符串最简单的方式是字符串字面量。在 Go 中，单个字符的字面量使用单引号，字符串字面量使用双引号，原生字符串使用反引号。所谓原生字符类似于 Python 中的 r&quot;&quot; 用于消除字符串中的所有转义操作。Go 的原生字符甚至可以消除换行，实现跨行，所以原生字符广泛使用再正则表达式，HTML模板、JSON面值以及命令行提示信息中。 与 Python 将大多数字符串操作作为字符串对象的方法不同，Go 大多数的字符串操作都在 strings 包，我们将这部分内容放在后面专门介绍，先来看看Go 提供的字符串基础操作。下面是一些代码示例: 123456789101112131415161718192021// 原生字符串const GoUsage = `Go is a tool for managing Go source code.Usage:go command [arguments]`s := \"hello, world\"// 1. len 函数获取字符串长度fmt.Println(len(s)) // \"12\"// 2. 索引fmt.Println(s[0], s[7]) // \"104 119\" ('h' and 'w')// 3. 切片fmt.Println(s[0:5]) // \"hello// 4. + 拼接fmt.Println(\"goodbye\" + s[5:]) // \"goodbye, world\"// 5. 不可修改s[0] = 'L' // compile error: cannot assign to s[0] 虽然字符串作为一个基本的数据类型被几乎所有的编程语言所支持，但是字符串本身确是很复杂。而复杂的地方至少有如下两点: 字符串的实现 字符的编码问题 4.1 字符串的实现 上面是字符串以及切片操作结果的示意图，在 Go 中，字符串是一个不可改变的字节序列，底层是一个字符数组，一个字符串可认为由两个部分构成:指针、长度 指针指向第一个字符对应的底层数组元素的地址 长度对应字符串中字符的个数 字符串的底层数组位于受保护的内存中，不能被修改，因此字符串是不可变的 对字符串变量进行重新赋值，不会改变字符串的底层数组，而只是改变了字符串中的指针的指向 不变性意味两个字符串可以安全的共享相同的底层数据，这使得字符串复制和切片不会发生实际的复制行为，而是直接共享原有的底层字符数组，因此操作非常迅速。 4.3 字符集在上面关于字符串的实现中，我们忽略了一个问题，即如何把字符串中的字符保存在一个数组中。我们知道在计算机上保存的数据只有二进制的 0 和 1，显然计算机没办法直接保存每个字符，于是就有了字符集的概念。 对于字符集以及字符的编码和解码，我是这样理解的: 字符集中最重要的概念就是码表，其作用是将每个字符与一个特定的数字对应起来，用特定的数字(又称码点)来表示特定的字符，因此码表就是字符集能表示的字符范围 有了码表，并没有解决保存字符的问题，显然就算是数字也要保存为整数的二进制格式。对于不同字符集而言，码点到特定的二进制也有一套特定的转换规则 因此，字符集实现了字符 --&gt; 码点 ---&gt; 码点二进制值的转换过程，码点 ---&gt; 码点二进制值被称为编码，反过来就是解码 有了上面的说明，就能解释清楚下面两个问题: ASCII 字符集 与 Unicode 字符集区别: ASCII字符集使用7bit来表示一个码点，而 Unicode 使用32bit表示一个 Unicode 码点，Unicode 显然能表示更大的字符范围 UTF8 编码与 UTF32 编码的区别: UTF32 编码直接将每个 Unicode 码点保存为 int32 的整数，而UTF8 会根据Unicode码点变长编码成二进制，它们都表示 Unicode 字符集，但是编码规则不同 4.4 字符串和 []runeGo语言的源文件采用UTF8编码，因此程序运行之后，保存在字符数组内的是 UTF8 编码的二进制值。因此前面我们所讲的字符串基础操作，操作的其实是UTF8 编码的每个字节，并不是我们理解的字符。为了处理真实的字符，我们需要对字符串进行解码。Go 将 Unicode 码点表示为 rune 整数类型，因此字符串解码后的类型就是 []rune。下面就是Go 中字符编码解码的一些代码示例: 1234567891011121314151617181920212223242526272829303132// 1. 字符串基础操作操作的是 UTF8 中的字节import \"unicode/utf8\"s := \"Hello, 世界\"fmt.Println(len(s)) // \"13\"fmt.Println(utf8.RuneCountInString(s)) // \"9\"// 2. unicode 提供了 UTF8 的解码函数for i := 0; i &lt; len(s); &#123; r, size := utf8.DecodeRuneInString(s[i:]) fmt.Printf(\"%d\\t%c\\n\", i, r) i += size&#125;// 3. range 会自动对字符串解码for i, r := range \"Hello, 世界\" &#123; fmt.Printf(\"%d\\t%q\\t%d\\n\", i, r, r)&#125;// 4. []rune 字符串的类型转换s := \"プログラム\"fmt.Printf(\"% x\\n\", s) // \"e3 83 97 e3 83 ad e3 82 b0 e3 83 a9 e3 83 a0\"// 字符串 --&gt; []runer := []rune(s)fmt.Printf(\"%x\\n\", r) // \"[30d7 30ed 30b0 30e9 30e0]// string 函数： []rune ---&gt; 字符串fmt.Println(string(r)) // \"プログラム// 5. 生成Unicode码点字符的UTF8字符串fmt.Println(string(65)) // \"A\", not \"65\"fmt.Println(string(0x4eac)) // \"京\" 4.5 字符串和 []byte一个字符串是包含的只读字节数组，一旦创建，是不可变的。相比之下，一个字节slice(即 []byte，下一节我们会详述)的元素则可以自由地修改。字符串和字节slice之间可以相互转换： 123s := \"abc\"b := []byte(s)s2 := string(b) 4.6 字符串相关类型的包标准库中有四个包对字符串处理尤为重要：bytes、strings、strconv和unicode包 strings包提供了许多如字符串的查询、替换、比较、截断、拆分和合并等功能。 bytes包也提供了很多类似功能的函数，但是针对和字符串有着相同结构的[]byte类型 strconv包提供了布尔型、整型数、浮点数和对应字符串的相互转换，还提供了双引号转义相关的转换 unicode包提供了IsDigit、IsLetter、IsUpper和IsLower等类似功能，它们用于给字符分类，每个函数有一个单一的rune类型的参数，然后返回一个布尔值 下面是字符串与数值转换的代码示例，我们会在后面专门讲解这些包的实现和使用。 123456789101112// 数值转字符串x := 123y := fmt.Sprintf(\"%d\", x)fmt.Println(y, strconv.Itoa(x)) // \"123 123\"// 数值的进制转换fmt.Println(strconv.FormatInt(int64(x), 2)) // \"1111011\"s := fmt.Sprintf(\"x=%b\", x) // \"x=1111011// 字符串转数值x, err := strconv.Atoi(\"123\") // x is an inty, err := strconv.ParseInt(\"123\", 10, 64) // base 10, up to 64 bits 5. 常量5.1 常量的类型在讲常量之前，先问大家一个问题，你知道字面量，常量，变量，字面量类型之间的区别么？ 字面量是编程语言提供的用来创建特定值的快捷方式，因此字面量也有类型，特定的字面量代表什么类型，完全有编程语言决定。因此对于像下面的赋值语句来说，在字面量类型和变量类型之间发生了类型转换。 1var f float64 = 3 常量和变量都是变量，但是相比与变量，常量有以下特点: 常量的值不可变，并且常量的类型只能是基础类型：boolean、string或数字 常量表达式的值在编译期计算，而不是在运行期，因此常量可以是构成类型的一部分，例如用于指定数组类型的长度 因为常量也是变量，所以常量通常有确定的类型，但Go语言的常量有个不同寻常之处， Go 中的常量可以没有一个明确的基础类型。 首先在 Go 中，有六种无类型的字面量，分别是无类型的布尔型、无类型的整数、无类型的字符、无类型的浮点数、无类型的复数、无类型的字符串。例如0、0.0、0i和’\\u0000’分别对应无类型的整数、无类型的浮点数、无类型的复数和无类型的字符。 其次在如下不带类型声明的常量声明语句中，不会发生隐式类型转换，常量的类型依旧为无类型的整数。 1const deadbeef = 0xdeadbeef // untyped int with value 3735928559 为了便于描述下面我们将无类型的字面量和常量统称为无类型常量，这些无类型常量有诸多好处。 编译器为这些无类型常量提供了比基础类型更高精度的算术运算。通过延迟明确常量的具体类型，无类型的常量不仅可以提供更高的运算精度，而且可以直接用于更多的表达式而不需要显式的类型转换。 只有常量可以是无类型的。当一个无类型的常量被赋值给一个变量的时候，或者出现在有明确类型的变量声明的右边，无类型的常量将会被隐式转换为对应的类型，如果转换合法的话。对于一个没有显式类型的变量声明（包括简短变量声明），字面量的形式将隐式决定变量的默认类型，Go 有一个明确的转换规则。如果要给变量一个不同的类型，我们必须显式地将无类型的常量转化为所需的类型，或给声明的变量指定明确的类型。 123456789101112131415161718192021222324252627282930// 1. 常量可以无类型，无类型常量可以提供更高的精度const ( deadbeef = 0xdeadbeef // untyped int with value 3735928559 a = uint32(deadbeef) // uint32 with value 3735928559 b = float32(deadbeef) // float32 with value 3735928576 (rounded up) c = float64(deadbeef) // float64 with value 3735928559 (exact) d = int32(deadbeef) // compile error: constant overflows int32 e = float64(1e309) // compile error: constant overflows float64 f = uint(‐1) // compile error: constant underflows uint)// 2. 无类型常量，可以直接应用在更多的表达式中，无需显示类型转换var f float64 = 3 + 0i // untyped complex ‐&gt; float64f = 2 // untyped integer ‐&gt; float64f = 1e123 // untyped floating‐point ‐&gt; float64f = 'a' // untyped rune ‐&gt; float64// 3. 有类型声明时，无类型常量将根据类型隐式类性转换var x float32 = math.Pivar y float64 = math.Pivar z complex128 = math.Pi// 4. 无类型声明时，根据字面量形式，决定变量类型i := 0 // untyped integer; implicit int(0) r := '\\000' // untyped rune; implicit rune('\\000')f := 0.0 // untyped floating‐point; implicit float64(0.0)c := 0i // untyped complex; implicit complex128(0i)var i = int8(0)var i int8 = 0 5.2 常量批量声明最后，Go 为常量的批量声明提供了一些便捷方式，下面是代码示例: 123456789101112131415161718192021222324252627282930313233343536// 1. 批量声明多个常量const ( e = 2.71828182845904523536028747135266249775724709369995957496696763 pi = 3.14159265358979323846264338327950288419716939937510582097494459)const ( a = 1 b // 省略初始化表达式，表示使用前面常量的初始化表达式写法， b=1 c = 2 d // d=2)// 2. iota常量生成器初始化，用于生成一组以相似规则初始化的常量type Weekday int const ( Sunday Weekday = iota // 在第一个声明的常量所在的行，iota将会被置为0， Monday // 然后在每一个有常量声明的行加一， 1 Tuesday // 2 Wednesday // 3 Thursday Friday Saturday)const ( _ = 1 &lt;&lt; (10 * iota) KiB // 1024 MiB // 1048576 GiB // 1073741824 TiB // 1099511627776 (exceeds 1 &lt;&lt; 32) PiB // 1125899906842624 EiB // 1152921504606846976 ZiB // 1180591620717411303424 (exceeds 1 &lt;&lt; 64) YiB // 1208925819614629174706176)","categories":[{"name":"Go","slug":"Go","permalink":"http://yoursite.com/categories/Go/"}],"tags":[{"name":"go语言入门","slug":"go语言入门","permalink":"http://yoursite.com/tags/go语言入门/"}]},{"title":"2 go 变量及流程控制","slug":"go/go_grammar/go_2","date":"2019-01-01T16:00:00.000Z","updated":"2019-04-14T01:55:16.586Z","comments":true,"path":"2019/01/02/go/go_grammar/go_2/","link":"","permalink":"http://yoursite.com/2019/01/02/go/go_grammar/go_2/","excerpt":"Hello World!","text":"Hello World! 1. Hello World抛开数据结构，代码封装和复杂的库文件，我们接触一门新语言的第一步可能就是学会这门语言的基础语法。下面是我写的 go 的一个 “Hello World” 程序。在这个简单的代码中包含了很多 Go 基础语法的内容: 变量及常量的命名，声明和创建 条件判断和循环 变量的生命周期与作用域 下面我们就分成这几块来讲讲 Go 的基础语法。 123456789101112131415161718192021package mainimport \"fmt\"const defaultUser = \"unsigned\"func main() &#123; name := \"A\" if name == defaultUser &#123; fmt.Println(\"Helll man\") &#125; else &#123; fmt.Println(\"Hey it is you\") &#125; num := 100 r := 0 for i := 0; i &lt;= num; i++ &#123; r += i &#125; fmt.Println(r)&#125; 2. 变量及常量的命名，声明和创建2.1 命名规则几乎所有的编程语言变量，常量，函数以及类型的命名规则都是相同的，即一个名字必须以一个字母或下划线开头，后面可以跟任意数量的字母、数字或下划线。 Go 与众不同的是名称中可以包含Unicode字母(不建议使用)，并且使用名字的开头字母的大小写决定了名字在包外的可见性。关于变量的导出我们会在模块的相关内容详述。 习惯上，Go语言程序员推荐使用驼峰式命名。 2.2 声明和创建Go语言主要有四种类型的声明语句：var、const、type和func，分别对应变量、常量、类型和函数实体对象的声明。我们先说变量以及常量。 与 Python 这种动态语言不同的是，Go 是静态语言，变量必须先声名才能使用。Go 中变量可以看成一个“容器”，一个变量对应一个保存了变量对应类型值的内存空间；变量一经声明，其类型就不能再改变。下面是 Go 中声明和创建变量的几种方式: 123456789101112//方式一: var 声明语句var name string = \"abc\"var i, j, k intvar b, f, s = true, 2.3, \"four\"//方式二: 函数内的短变量声明，用于局部变量的声明和初始化t := 10i, j := 0, 1//方式三: new 函数，创建变量，并返回对应变量的指针p := new(int) // 此处创建了两个变量: new 函数创建的匿名变量，以及指向匿名变量的指针变量 p*p = 2 varvar声明语句可以创建一个特定类型的变量，然后给变量附加一个名字，并且设置变量的初始值。对于 var 变量名字 类型 = 表达式，“类型”或“= 表达式”两个部分可以省略其中的一个。 如果省略的是类型信息，那么将根据初始化表达式来推导变量的类型信息。 如果初始化表达式被省略，那么将用零值初始化该变量，规则如下 数值类型变量对应的零值是0 布尔类型变量对应的零值是false 字符串类型对应的零值是空字符串 接口或引用类型（包括slice、指针、map、chan和函数）变量对应的零值是nil 数组或结构体等聚合类型对应的零值是每个元素或字段都是对应该类型的零值 常量的声明和创建使用 const 声明语句，用法与 var 类似。 短变量声明短变量声明语句用在函数内，用于声名和初始化局部变量，语法为变量名:=表达式，变量的类型根据表达式自动推导。Go 的短变量声明有一些微妙之处: 首先“:=”是一个变量声明语句，而“=”是一个变量赋值操作 其次，简短变量声明左边的变量可以包含已经声明过的变量，对于这些变量将只是赋值，而不是再声明 最后，简短变量声明语句中必须至少要声明一个新的变量否则无法通过编译 new 函数new 是 Go 预定义的一个函数，new(T)将创建一个T类型的匿名变量，初始化为T类型的零值，然后返回变量地址。 用new创建变量和普通变量声明语句方式创建变量没有什么区别，除了不需要声明一个临时变量的名字外。因为 new 只是一个普通函数，因此可以使用在任何函数可用的地方，甚至new名字可以被重定义其他类型。 3. 条件判断和循环看完了变量创建，我们再来看看 Go 为我们提供的逻辑控制语句: if, switch, for。Go 没有 while 语句，但是 for 语句包含了 while 语句的功能。除了 if 外，switch 和 for 的用法都不简单。 除了这些基础的逻辑控制语句外，Go 还有一个特殊的与 Go 高并发相关的多路复用器 select。 3.1 ifGo 应该是类 C 风格的语言，使用 {} 来分隔代码块。一个完整的 if 语句如下所示:1234567if r == 0 &#123; fmt.Println(\"aaa\")&#125; else if r == 1 &#123; fmt.Println(\"bbbb\")&#125; else &#123; fmt.Println(\"cccc\")&#125; 3.2 switchswitch 是多分支条件判断的便捷语法，用于基于不同条件执行不同动作，Go 的 switch 有如下三种使用方式。 123456789101112131415161718192021222324252627282930//方式一: 变量值判断switch var1 &#123; case v1: // var1 变量与 case 语句中的值类型必须相同 ... case v2,v3: // 逗号分隔表示可匹配多个值 ... default: ...&#125;// 方式二: 条件判断的变形switch &#123; case condition1: ... case condition2, condition3: // 逗号分隔表示可匹配多个条件 ... default: ...&#125;// 方式三: type-switch 用来判断某个 interface 变量中实际存储的变量类型// 我们会在后面讲接口类型时详述switch x.(type) &#123; case type1: .... case type2: .... default: ....&#125; 不同语言的 switch 语句差异很大，Go 的 switch 有如下特点: switch 语句的执行过程是从上直下逐一测试，直到匹配就会停止 每个 case 分支的最后不需要再加break，即默认只会执行第一个匹配到的 case 分支 Python 中没有 switch 语句，shell 脚本则必须在每个 case 分支之后添加 break，否则第一次匹配成功后后，会继续匹配之后的 case 分支。 3.3 selectselect 类似于用于通信的switch语句，它的一个使用示例如下所示:1234567891011func main() &#123; var c1, c2 chan int var i1, i2 int select &#123; case i1 = &lt;-c1: fmt.Printf(\"received \", i1, \" from c1\\n\") case c2 &lt;- i2: fmt.Printf(\"sent \", i2, \" to c2\\n\") default: fmt.Printf(\"no communication\\n\") &#125; 在 select 中: 每个case必须是一个通信操作，要么是发送要么是接收 所有channel表达式都会被求值，如果有多个 case 可以运行，select会随机执行一个可运行的case 如果没有case可运行，此时 如果有default子句，则执行该语句，defalut 子句应该总是可运行的 如果没有default字句，select将阻塞，直到某个通信可以运行；Go不会重新对channel或值进行求值 3.3 forGo 的 for 循环有四种常见的使用方式，如下所示。最特殊的是第四种 for 循环的 range 格式，它可以对 slice、map、数组、字符串等进行迭代循环。 12345678910111213141516171819202122232425//方式一: 典型的类 C for 循环for init; condition; post &#123;&#125;//方式二: 类 while 循环for condition &#123;&#125;//方式三: 无限循环for &#123;&#125;//无限循环的另一种方式for true &#123;&#125;//方式四: 类Python 的迭代循环for index, value := range oldMap &#123; // index: 索引 // value: 索引对应的值&#125; 4. 变量的生命周期与作用域变量的生命周期指的是在程序运行期间变量有效存在的时间间隔，变量作用域是指源代码中可以有效使用这个名字的范围。虽然我将变量的生命周期与作用域放在一起，但是其实它们之间并没有什么联系。声明语句的作用域对应的是一个源代码的文本区域；它是一个编译时的属性。一个变量的生命周期是指程序运行时变量存在的有效时间段，在此时间区域内它可以被程序的其他部分引用；是一个运行时的概念。 Go 与 Python 类似，通过引用计数的方式，解释器会自动实现对象内存的分配和释放。变量的生命周期取决于变量是否可达，即其引用计数是否为 0，而与变量的作用域无关。虽然大多数位于函数内的局部变量的生命周期都是函数调用的存续区间，但是函数内的局部变量可以”逃逸”成为全局变量，或者从函数返回，从而延长生命周期。 变量的作用域取决于变量声明语句所在的语法块(又称词法域)，语法块通常由花括号显示限定，除此之外还有一些特殊的语法块。对于 Go 作用域从大到小依次是: 整个源代码，称为全局语法块 每个包的包语法块 每个源文件的源文件级的语法块 由显示花括号限定的由外而内的语法块 对于 if,for,switch,select 还有隐式的语法块 一个程序可能包含多个同名的声明，只要它们在不同的作用域。位于内部作用域的变量声明显然会覆盖外部的同名变量。对于大多数程序的作用于而言，都有类似规则。而 Go 比较特殊的是 if,for,switch,select引入的隐式作用域。 if, for 等的隐式作用域12345678if x := f(); x == 0 &#123; fmt.Println(x)&#125; else if y := g(x); x == y &#123; fmt.Println(x, y)&#125; else &#123; fmt.Println(x, y)&#125;fmt.Println(x, y) // compile error: x and y are not visible here 在上面的示例中存在多个作用域，从大到小依次是: 全局作用域 外层 if 语句条件部分创建隐式词法域 外层 if 语句花括弧包含的显式作用域 内层 if 语句条件部分创建隐式词法域 ….. 因此内层 if 语句的条件测试部分，能访问到外层 if 语句条件部分声明的变量 x。for 语句循环的初始化部分，switch 语句的条件测试部分都会引入类似的隐式作用域。 变量的作用域问题说起来比较复杂，但是大多数情况下，只要我们不在不同的作用域内声明同名变量，导致变量覆盖，基本上都不会现问题。但是在 Go 中要特别注意短变量声明语句的作用域。 在下面的示例中，虽然cwd在外部已经声明过，但是 := 语句还是将cwd和err重新声明为新的局部变量。因为内部声明的cwd将屏蔽外部的声明，因此上面的代码并不会正确更新包级声明的cwd变量。 1234567891011121314151617var cwd stringfunc init() &#123; cwd, err := os.Getwd() // compile error: unused: cwd if err != nil &#123; log.Fatalf(\"os.Getwd failed: %v\", err) &#125;&#125;// 正确做法var cwd stringfunc init() &#123; var err error # 单独声明 err 变量 cwd, err = os.Getwd() if err != nil &#123; log.Fatalf(\"os.Getwd failed: %v\", err) &#125;&#125; 最后，Go 变量遵循先声明后使用的规则，但是在包级别，声明的顺序并不会影响作用域范围，因此一个先声明的可以引用它自身或者是引用后面的一个声明，这可以让我们定义一些相互嵌套或递归的类型或函数。","categories":[{"name":"Go","slug":"Go","permalink":"http://yoursite.com/categories/Go/"}],"tags":[{"name":"go语言入门","slug":"go语言入门","permalink":"http://yoursite.com/tags/go语言入门/"}]},{"title":"1 go 入门开篇","slug":"go/go_grammar/go_1","date":"2018-12-31T16:00:00.000Z","updated":"2020-05-24T15:03:11.003Z","comments":true,"path":"2019/01/01/go/go_grammar/go_1/","link":"","permalink":"http://yoursite.com/2019/01/01/go/go_grammar/go_1/","excerpt":"如果编程的世界是海贼王里的”大航海时代”, go 语言可能就是”草帽海贼团”","text":"如果编程的世界是海贼王里的”大航海时代”, go 语言可能就是”草帽海贼团” 1. 要去学 Go 了学习和使用 Python 有三四年,好想学一门新语言,打算学 Go。为什么是 Go，其实更想学 Rust。但是 Go 有谷歌这个大佬，背靠k8s，显然学 Go 好处大大的。其实也无所谓，哪天想学 Rust，就拿来看看对比着学可能更快。当然学 Go 还有另一个重要原因，想转运维开发。 2. 怎么学 Go因为已经不是第一次学编程了，之前也看过一段时间 C，想看看在学习了编程这么长时间之后，在编程领域的学习能力相比于一开始有没有提升。所以这次打算从语言特性的角度出发，有目的性的对比学习，看看能不能以更快的速度学好 Go。下面是我能想到知识面: 基础语法，包括变量，循环，判断以及运算符 Go 语言提供的基本数据结构 异常处理 函数，类与泛型 并发编程 3. 学习资料书选的《Go程序设计语言》，在写博客之前已经翻过一遍，的确是一本可以拿来入门的好书。 4. 环境搭建在学习 Go 语言之前，最重要的是搭建一个 Go 的开发环境。为了对 Go 有一个更好的整体把握，对于这个开发环境我们至少完成下面这些任务。下面涉及的 Go 专业术语，后面会详细解释，为了便于理解，我简单的跟 Python作了一个对比 安装 Go，搭建基本的go开发环境 – python 安装 Go 语言工具箱，特别是 go 程序包的查询，下载和管理 – pip 的使用 Go 语言的工作目录 – 模块的搜索路径 IDE 编程环境 我们主要讲解 Linux 下的环境搭建，Windows 的搭建类似。我们使用 VScode 作为我们的IDE，没其他原因，因为大佬们都推荐。 4.1 Go 安装Go 语言官方文档有完整的安装文档,Linux 下可直接运行下面的 bash 脚本，而唯一需要修改的是最后三个环境变量的配置。其中 PATH: 用于将 go 命令添加到环境变量的命令搜索路径中，便于直接使用 go 命令 GOPATH: 用于指定 go 的工作区，可以是单个目录路径，也可以是冒号分割的多个路径 GOBIN: 用于指定 GO 程序生成的可执行文件（executable file）的存放路径 先让你的 Go 可以运行起来，别的不用着急，马上我们就会讲解环境变量的作用，在你理解这些环境变量的含义之后就可以按需修改。 1234567891011121314151617go_vsersion=go1.12.4.linux-amd64.tar.gz# 1. 下载安装包wget https://studygolang.com/dl/golang/$&#123;go_vsersion&#125;.tar.gz# 2. 解压到指定目录tar -C /usr/local -xzf $&#123;go_version&#125;.tar.gz# 3. 配置相关环境变量# 将 go 命令添加到 PATH 环境变量中，以便直接使用，PATH 环境变量与 GO 本身无关echo 'export PATH=/usr/local/go/bin:$PATH' &gt; /etc/profile.d/go.sh# 添加 Go的工作区，下面默认为 $HOME/goecho 'export GOPATH=$(go env GOPATH)' &gt;&gt; /etc/profile.d/go.shecho 'export GOBIN=$GOPATH/bin' &gt;&gt; /etc/profile.d/go.# 4. 并通过在命令行中输入go version来验证是否安装成功。go version 4.2 Go语言工具箱在 go 安装完毕之后，在 go 安装目录的 bin 子目录下会有一个 go 命令(默认为/usr/local/go/bin)，这就是 go 语言提供给我们的管理工具箱，它是一系列功能的集合: 首先它是一个构建系统，计算文件的依赖关系，然后调用编译器、汇编器和连接器构建程序 其次它是一个包管理器（类似于python pip），用于包的查询、下载、依赖关系解决。 最后它是一个单元测试和基准测试的驱动程序 go 命令的执行依赖很多环境变量，使用 go env 可以查看所有的环境变量，大多数环境变量在 go 语言正确安装之后(主要是选择与操作系统匹配的安装包)会自动配置，唯一需要用户配置是GOPATH，用于指定go 语言的工作区，工作区是 go 语言中的一个核心概念，Go 语言项目在其生命周期内的所有操作（编码、依赖管理、构建、测试、安装等）基本上都是围绕着 GOPATH 和工作区进行的。 4.3 Go 工作区GOPATH对应的工作区目录有三个子目录: src 子目录用于存储源代码，使用 go get 下载的 go 包和自定义的 go 程序源代码都存在此目录中，同时也是代码包搜索和导入的启始根目录 pkg子目录用于保存编译后的包的目标文件 bin子目录用于保存编译后的可执行程序 go build命令编译命令行参数指定的每个包。如果 src使用命令 go get可以下载一个单一的包或者用 …下载整个子目录里面的每个包。go get 会自动下载所依赖的每个包 #### #### 4.3 GOROOT环境变量GOROOT用来指定Go的安装目录，还有它自带的标准库包的位置。GOROOT的目录结构和GOPATH类似，因此存放fmt包的源代码对应目录应该为$GOROOT/src/fmt。用户一般不需要设置GOROOT，默认情况下Go语言安装工具会将其设置为安装的目录路径。 下面是我当前工作区目录的示例:123456789101112131415161718192021$ tree -L 2 /home/tao/go/home/tao/go├── bin│ ├── a│ ├── dlv│ ├── gocode│ ├── godef│ ├── go-outline│ ├── gopkgs│ ├── goreturns│ └── helloworld├── pkg│ └── linux_amd64└── src ├── algo ├── blog ├── github.com ├── golang.org ├── gopl.io ├── sourcegraph.com └── test 你可以运行go或go help命令查看内置的帮助文档，为了查询方便，我们列出了最常用的命令 12345678910111213141516171819202122232425262728$ goGo is a tool for managing Go source code.Usage: go &lt;command&gt; [arguments]The commands are: bug start a bug report build compile packages and dependencies clean remove object files and cached files doc show documentation for package or symbol env print Go environment information fix update packages to use new APIs fmt gofmt (reformat) package sources generate generate Go files by processing source get download and install packages and dependencies install compile and install packages and dependencies list list packages or modules mod module maintenance run compile and run Go program test test packages tool run specified go tool version print Go version vet report likely mistakes in packagesUse \"go help &lt;command&gt;\" for more information about a command. 4.3 Go 环境变量GOPATH对应的工作区目录有三个子目录。 与 Python 不同的是，Go 的包不是通过镜像的方式，而是直接从远程版本控制系统(eg: githup)直接下载的，因此当我们使用标准的 go get 下载Go包时，可能会由于不可描述的原因失败。因此我们必须手动解决一些包的安装问题。 4.2 Vscode 安装在Vscode官网 下载与你系统时配的安装包，安装即可。安装完成后在 VScode Extension 安装与 go 相关的扩展，如下图所示:","categories":[{"name":"Go","slug":"Go","permalink":"http://yoursite.com/categories/Go/"}],"tags":[{"name":"go语言入门","slug":"go语言入门","permalink":"http://yoursite.com/tags/go语言入门/"}]},{"title":"39 最小生成树","slug":"alog/graph_use4","date":"2018-11-20T16:00:00.000Z","updated":"2019-04-14T01:55:16.574Z","comments":true,"path":"2018/11/21/alog/graph_use4/","link":"","permalink":"http://yoursite.com/2018/11/21/alog/graph_use4/","excerpt":"最小生成树","text":"最小生成树 1. 最小生成树所谓最小生成树(简称MST)就是在一个无向，有权图G中，找到一颗连接所有顶点的树，并且树包含的边的权重总和最低。最小生成树有两种常见解法: Prim-Jarnik 算法: 从单个根节点生成 MST，它和 Dijkstra有很多相似之处 Kruskal 算法: 通过按照边的权重的非递减去考虑边来成群的生成 MST 无论是哪种算法，它们都基于最小生成树这样一个事实。即 G 是一个有权重的连通图，另 V1 和 V2 是两个不相交的非空集合G的顶点的一部份。此外另 e 是那些一个顶点在 V1 一个顶点在V2的有最小权重的G的边，则 e 是最小生成树的一条边。 1.1 Prim-JarnikPrim-Jarnik 算法，我们以一某一顶点 s 开始，定义初始集合 C，然后每次迭代中，我们选择一个最小权重的边 e，将 C 中的顶点连接到 C 之外的顶点 v，之后在将 v 加入 C 中。此时 e 就是最小生成树的一条边。 1.2 KruskalKruskal 算法，首先每个顶点本身是单元素集合集权。算法按照权重增加的顺序轮流考察每条边。如果一条边连接了两个不同的集群，那么 e 就是最小生成树的一条边。 2. 实现2.1 Prim-Jarnik12345678910111213141516171819202122232425262728def MST_Prim_Jarnik(g): d = &#123;&#125; tree = [] pq = AdaptableHeapPriorityQueue() # 优先队列 pdlocator = &#123;&#125; # pdlocator 此处还起到判断顶点是否已经迭代过的作用 # 初始化 for v in g.vertices(): if len(d) == 0: d[v] = 0 else: d[v] = float('inf') pdlocator[v] = pq.add(d[v], (v, None)) while not pq.is_empty(): key, value = pq.remove_min() u, edge = value if edge is not None: tree.append(edge) del pdlocator[u] for e in g.incident_edge(u): v = e.opposite(u) if v in pdlocator: wgt = e.element() if wgt &lt; d[v]: d[v] = wgt pq.update(pdlocator[v], (v, e)) return tree Prim-Jarnik Dijkstra类似，时间复杂度分析也类似。 2.2 Kruskal12345678910111213141516171819202122def MST_Kruskal(g): tree = [] pq = AdaptableHeapPriorityQueue() # 优先队列 forest = Partition() position = &#123;&#125; for v in g.vertices(): position[v] = forest.make_group(v) for e in g.edges(): pq.add(e.element, e) size = g.vertice_count() while len(tree) != size - 1 and pq.is_empty(): wgt, edge = pq.remove_min() u, v = edge.endpoints() a = forest.find(position[u]) b = forest.find(position[v]) if a != b: tree.append(edge) forest.union(a, b) return tree Partition 是一个不相交集合和联合查找结构的实现。 2.3 不相交集合和联合查找结构1234567891011121314151617181920212223242526272829303132class Partition(object): __slots__ = '_container', '_element', '_size', '_parent' class Position(object): def __init__(self, container, e): self._container = container self._element = e self._size = 1 self._parent = self def element(self): return self._element def make_group(self, e): return self.Position(self, e) def find(self, p): if p._parent != p: p._parenet = self.find(p._parent) return p._parent def union(self, p, q): a = self.find(p) b = self.find(q) if a is not b: if a._size &gt; b._size: b._parent = a a._size += b._size else: a._parent = b._parent b._size += a._size","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"38 最短路经","slug":"alog/graph_use3","date":"2018-11-19T16:00:00.000Z","updated":"2019-04-14T01:55:16.573Z","comments":true,"path":"2018/11/20/alog/graph_use3/","link":"","permalink":"http://yoursite.com/2018/11/20/alog/graph_use3/","excerpt":"最短路经","text":"最短路经 1. 最短路径广度优先算法可以计算连通图中，从一个顶点到另一顶点的最短路径，但前提是图上的每条边的权重相同。那如何计算全重不同的图的最短路径呢？最出名的莫过于 Dijkstra 算法。 1.1 DijkstraDijkstra 算法是贪心算法。贪心算法的递归过程差不多是这样:假设我们计算图 G 上顶点 u 到顶点 v 的最短距离；对于到顶点 v 的所有输入边的顶点集合 S，如果我们知道 u 到 S 中每个顶点的最短距离，那我们就能计算出 u 到 v 的最短距离。整个 Dijkstra 算法计算过程比较复杂，我们结合代码来看。 2. 实现2.1 Dijkstra12345678910111213141516171819202122232425262728293031323334def shortest_search(g, src): d = &#123;&#125; # 从 src 到 顶点的最短距离 cloud = &#123;&#125; # 收集已经计算得到最短距离的所有顶点 pre = &#123;&#125; # 还原最短路径的路径 pdlocator = &#123;&#125; # 定位顶点在优先队列中位置 pq = AdaptableHeapPriorityQueue() # 优先队列 # 初始化 for u in g.vertices(): d[u] = float('inf') d[src] = 0 pre[src] = None pdlocator[src] = pq.add(0, src) # 迭代优先队列，不断从中取出距离最小的顶点 while not pq.is_empty(): k, u = pq.remove_min() # 删除堆顶元素 cloud[u] = k del pdlocator[u] for e in g.incident_edge(u): v = e.opposite(u) n = k + e.element() if v not in cloud: if v not in pdlocator: d[v] = n # 插入堆 pdlocator[v] = pq.add(d[v], v) src[v] = u else: if n &lt; d[v]: d[v] = n # 更新堆 pq.update(pdlocator[v], n, v) src[v] = u return cloud, pre AdaptableHeapPriorityQueue 是我们在堆中实现的优先队列。之所以使用这个优先队列，是因为我们要不断的在队列中更新顶点的距离，以保证从优先队列取出的是当前距离最小的顶点。 整个代码的时间负载度分成两个部分: 一是 while + for 内对顶点和边的迭代，因为每个顶点和每条边最多被迭代一次，所以时间负载度是O(n+m); 二是对优先队列的操作，包括: add remove_min update 在堆一节中AdaptableHeapPriorityQueue被实现为一个堆，上述所有操作的时间复杂度都是 logn，因此总的时间复杂度是 O((n+m)logn)。 AdaptableHeapPriorityQueue 还有其他实现方式，比如一个未排序的数组，此时 remove_min 为 O(n)，其他两个操作的时间复杂度都是O(1)，此时总体的时间复杂度就是 O(n*n + m)。因此使用哪种实现方式更优取决于图的稀疏程度。 需要注意的是与前面类似，对于 d，pre， pdlocator，cloud 如果顶点可以用 0 到 n-1 进行编号， 它们都可以用数组代替，或者将作为顶点属性来记录。 2.2 重建最短路径树上面我们计算出从 src 到各个顶点的最短距离，但是并没有明确计算出获取最短剧路的路径。最短路径的重建有两种方式: 向上面代码中那样，使用 pre 记录到达每个顶点的前一个顶点。 是直接从 cloud 的返回值进行重建。 1234567891011121314151617181920212223242526272829# 重建最短路径树def shortest_path_tree(g, s, d): \"\"\" :param g: :param s: src 顶点 :param d: cloud 的返回值 :return: \"\"\" tree = &#123;&#125; for v in d: if v is not s: for e in g.incident_edge(v, False): u = e.opposite(v) wgt = e.element() if d[v] == d[u] + wgt: tree[v] = e return tree # 计算到顶点 v 的最短路径def shortest_path(pre, v): \"\"\" :param pre: pre :return: \"\"\" p = [v] while v in pre and pre[v] is not None: v = pre[v] p.append(v) return p.reverse()","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"37 拓扑排序","slug":"alog/graph_use2","date":"2018-11-18T16:00:00.000Z","updated":"2019-04-14T01:55:16.573Z","comments":true,"path":"2018/11/19/alog/graph_use2/","link":"","permalink":"http://yoursite.com/2018/11/19/alog/graph_use2/","excerpt":"拓扑排序","text":"拓扑排序 1. 拓扑排序的背景拓扑排序是一种排序，假设完成一项任务需要 n 个步骤，这 n 个步骤之间存在依赖关系，拓扑排序就是确定一个满足依赖关系的执行步骤。典型的拓扑排序用于解决如下问题: 大学课程之间的选修课的顺序 面向对象编程的类之间的继承 编译器在编译项目，按照编译的依赖关系确定编译顺序 拓扑排序是有向无环图的经典应用，解决的问题的模型也非常一致。凡是需要通过局部顺序来推导全局顺序的，一般都能用拓扑排序来解决。其有两种实现方法，分别是Kahn 算法 和 DFS 深度优先搜索算法。 1.1 Kahn 算法Kahn 算法实际上用的是贪心算法思想。 定义数据结构的时候，如果 s 需要先于 t 执行，那就添加一条 s 指向 t 的边。所以，如果某个顶点入度为 0， 也就表示，没有任何顶点必须先于这个顶点执行，那么这个顶点就可以执行了。 我们先从图中，找出一个入度为 0 的顶点，将其输出到拓扑排序的结果序列中，并且把这个顶点从图中删除（也就是把这个顶点可达的顶点的入度都减 1）。我们循环执行上面的过程，直到所有的顶点都被输出。最后输出的序列，就是满足局部依赖关系的拓扑排序。 Kahn 算法还能检测图是否存在环，如果最后输出出来的顶点个数，少于图中顶点个数，图中还有入度不是 0 的顶点，那就说明，图中存在环 1.2 DFS 深度优先搜索算法使用 DF 实现拓扑排序的方法不好理解。假设图上的一个路径是A--&gt;B--&gt;C--&gt;D，如果我们按照输入边进行 DFS，那么顶点 A 一定在其他顶点之前输出。即所有入度为 0 的顶点一定在其他顶点之前输出，而递归调用的返回相当于对于顶点的入度减 1。最终的结果就是按照输入边对图的 DFS 和Kahn 算法一致。文字描述并不是很清楚，请结合代码查看。 2. 实现2.1 Kahn 算法123456789101112131415161718192021def topologic_sort(g): topo = [] # 拓扑排序的结果 ready = [] # 入度为 0 待加入 topo 的顶点 incount = &#123;&#125; # 记录每个顶点的入度 for u in g.vertices(): c = g.degree(u, False) if c == 0: ready.append(u) else: incount[u] = c while ready: u = ready.pop() topo.append(u) # 获取 u 的输出边，减少对应顶点的入度 for e in g.incident_edge(u): # 迭代所有顶点的传出边 v = e.opposite(u) incount[v] -= 1 if incount[v] == 0: incount.pop(v) ready.append(v) return topo 和图的遍历一样如果顶点可以用 0 到 n-1 进行编号，我们可以用数组代替 incount，或者将入度的计数作为顶点属性来记录。显然整个算法的时间复杂度为 O(n + m) 2.2 DFS 深度优先搜索算法123456789101112131415161718def DFS_income(g, u, discovered, topo): for e in g.incident_edge(u, False): v = e.opposite(u) if v not in discovered: discovered[v] = e DFS_income(g, v, discovered, topo) # 类似于后序遍历, 顶点 A 会优先加入 topo topo.append(u)def topologic_dfs(g): discovered = &#123;&#125; topo = [] for u in g.vertices: if u not in discovered: discovered[u] = None DFS_income(g, u, discovered, topo) return topo","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"36 图的传递闭包","slug":"alog/graph_use1","date":"2018-11-17T16:00:00.000Z","updated":"2019-04-14T01:55:16.573Z","comments":true,"path":"2018/11/18/alog/graph_use1/","link":"","permalink":"http://yoursite.com/2018/11/18/alog/graph_use1/","excerpt":"解决图可达性的传递闭包","text":"解决图可达性的传递闭包 1. 场景通过图上的深度和广度优先搜索算法，我们可以知道顶点 u 到顶点 v 的可达性问题，但是在某些应用中，我们可能希望更高校的回答很多可达性问题。此时对图预计算一个更高效的表示方式是非常值得的，图的传递闭包就是用来解决这个问题。 有向图 G 的传递闭包是有向 G1 使得 G1 顶点与 G 的顶点一样，并且对于所有顶点对 (u, v) 能直接表示是否有从 u 到 v 的一条路径。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"35 深度和广度优先搜索","slug":"alog/graph_search","date":"2018-11-15T16:00:00.000Z","updated":"2019-04-14T01:55:16.572Z","comments":true,"path":"2018/11/16/alog/graph_search/","link":"","permalink":"http://yoursite.com/2018/11/16/alog/graph_search/","excerpt":"图的深度和广度优先搜索","text":"图的深度和广度优先搜索 1. 特性上一节我们讲解了图的存储和表示，这一节我们来介绍图上的搜索算法。图的搜索方法有很多，最常见的就是深度和广度优先搜索，除此之外还有 A、IDA 等启发式搜索算法。因为邻接表更加常用，我们就以邻接表作为图的存储方式来讲解最基础的深度和广度优先算法。 形式上，遍历是通过检查所有的边和顶点来探索图的系统化的步骤。图的遍历算法是回答许多涉及可达性概念的有关图的问题的关键，即在图中决定如何从一个顶点到达另一个顶点。 在无向图中处理可达性的问题包括: 计算从顶点 u 到顶点 v 的路经，或者报告这样的路经不存在 已知 G 的开始顶点，对每个 G 的顶点 v 计算 s 和 v 之间的边的最小数目的路经，或者报告有没有这样的路经 测试 G 是否是连通的 如果 G 是连通的，计算 G 的生成树 计算 G 的连通分支 计算 G 中的循环，或者报告 G 没有循环 在有向图中处理可达性的问题包括: 计算从顶点 u 到顶点 v 的有向路经，或者报告这样的路经不存在 找出 G 中从已知顶点 s 可达的顶点 判断 G 是否是非循环的 判断 G 是否是强连通的 1.1 深度优先搜索深度优先搜索（Depth-First-Search），简称 DFS。最直观的例子就是“走迷宫”，每次迭代时任意选择一个分岔的”顶点”进行搜索，直至没有顶点时退回到上一个顶点重新选择新的顶点继续遍历，直到所有顶点都被遍历结束。下面是一个深度优先搜索的示意图 深度优先搜索对是否从一个顶点到另一个顶点有路径和是否该图是一个连通图非常有用。 1.2 广度优先搜索广度优先搜索（Breadth-First-Search），简称为 BFS，就是一种“地毯式”层层推进的搜索策略，即先查找离起始顶点最近的，然后是次近的，依次往外搜索。所有的顶点按照从左往右，从上往下的顺序依次迭代。为了保证迭代的次序需要用到队列，整个过程就是从顶点入队开始，将队首元素出队，并将出队顶点的下一层顶点依次入队，迭代直至队列为空的过程。为了防止顶点被重复遍历，需要对已经遍历的顶点进行表识。 2. 应用2.1 深度优先搜索12345678910111213141516def DFS(g, u, discovered): \"\"\" :param g: 图 :param u: 开始顶点 :param discovered: 将图的顶点映射到用于发现那个顶点的边 :return: \"\"\" for e in g.incident_edge(u): v = e.opposite(u) if v not in discovered: discovered[v] = e DFS(g, v, discovered)# u 为开始顶点，值为 None，用于标识其为开始顶点result = &#123;u: None&#125;DFS(g, u, result) discovered 字典这里为两个目的服务，一是提供了用于判断顶点是否已被访问的机制，二是字典内保存的边就是DFS树的边。如果假设顶点可以用 0 到 n-1 进行编号，discovered可以用基于这些数子的数组替代。或者可以直接将所有顶点的发现状态以及顶点的发现边作为顶点的属性，成为顶点的一部分。 顶点 u 到 v 的可达路经基于discovered 字典我们可以很容易基于这个字典来提供从顶点 u 到达顶点 v 的可达路经的顶点列表。 1234567891011def construct_path(u, v, discovered): path = [] if v in discovered: path.append(v) walk = v while walk is not u: parent = discovered[walk].opposite[walk] path.append(parent) walk = parent path.reverse() return path 连通性测试基于 DFS 函数，我们可以很容判断图是否是连通的。在无向图的情况下，我们在任意顶点简单的开始深度有限搜索，然后测试 len(discovered) 和图的顶点数是否相同。如果相等无向图就是连通。 对于有向图，我们可能想测试它是否是强连通的。我们可以对任意顶点 s 执行深度优先搜索。注意在我们的 DFS 实现中，我们是以顶点的输出边为基础的，我们可以重新实现一个深度优先搜索函数 DFS_IN，这次以输入边作为遍历图的基础。对顶点 s 重新执行 DFS_IN。如果两次 DFS 遍历，所有顶点都是可达的则图是强连通的。 12345678910111213def DFS_IN(g, u, discovered): \"\"\" :param g: 图 :param u: 开始顶点 :param discovered: 将图的顶点映射到用于发现那个顶点的边 :return: \"\"\" # 以输入边执行反向的深度优先搜索 for e in g.incident_edge(u, outgoing=False): v = e.opposite(u) if v not in discovered: discovered[v] = e DFS(g, v, discovered) 计算所有的连通分支当图是不连通的时候，我们的下一个目标是识别无向图的所有连通分支，或有向图的强连通分支。我们首先来看无向图。 1234567def DFS_complete(g): forest = &#123;&#125; for u in g.vertices(): if u not in forest: forest[u] = None DFS(g, u, forest) return forest DFS_complete 函数返回的发现字典代表了整个图的 DFS 森林。连通分支数可以通过发现字典值为 None 的键的个数来判定。 找到有向图的强连通分支的情况更复杂，存在在 O(n+m)时间内计算这些连通分支的方法，使用两次单独的深度优先搜索遍历，细节我们之后在详述。 判断图是否存在循环循环的存在当且仅当和 DFS 遍历相关的 back 边存在。无向图搜索 back 边是容易的，因为所有的边不是树的边就是 back 边。而无向图比较困难。代码实现如下 12345def is_cycle(): passdef is_cycle_directed(): pass 2.1 广度优先搜索如下两个版本的广度有限搜索代码都是正确的，都是我们常用的形式。 123456789101112131415161718192021222324def BFS(g, s, discovered): queue = deque() queue.append(s) discovered[s] = None while queue: u = queue.popleft() for e in g.incident_edge(u): v = e.opposite(u) if v not in discovered: discovered[v] = e queue.append(v)def BFS_1(g, s, discovered): level = [] while level: next_level = [] for u in level: for e in g.incident_edge(u): v = e.opposite(u) if v not in discovered: discovered[v] = e next_level.append(v) level = next_level 广度优先搜索的应用BFS 可以遍历 s 所有的可达顶点，要探索整个图，可以从另一顶点重新开始，和代码 DFS_complete 类似。同样从顶点 s 到顶点 v 的实际路经可以使用代码段 construct_path 函数重建。 2.3 对比DFS 和 BFS 都能很高效的找到从给定源可达顶点的集合，然后判定到这些顶点的路经。然而 BFS 可保证这些路经尽可能少的使用边。对于无向图，两个算法都能用来测试连通性，识别连通分支或找出循环。对于有向图而言，DFS 可能更适合一些任务，比如在图中寻找有向循环，或识别强连通分支。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"34 图的表示","slug":"alog/graph","date":"2018-11-14T16:00:00.000Z","updated":"2019-04-14T01:55:16.572Z","comments":true,"path":"2018/11/15/alog/graph/","link":"","permalink":"http://yoursite.com/2018/11/15/alog/graph/","excerpt":"如何表示一个图","text":"如何表示一个图 1. 特性从本节开始，我们将进入图的学习。图是一种比树更复杂的非线性结构，图中有以下一些专用术语: 顶点: 图中的节点被称为顶点 边: 顶点之间存在链接关系，可以有方向，也可以有权重 有向图: 边有方向的图 带权图: 边存在权重的图 度: 顶点包含的边数，在有向图中，度分为出度和入度 出度表示以顶点作为起点的边，该边也称为顶点的输出边 入度表示以顶点作为终点的边，该边也称为顶点的入射边 很显然在表示和存储一个图时，我们需要保存图的顶点，边，以及边的方向和权重。而图的存储有两个常见方法: 邻接矩阵和邻接表 1.1 邻接矩阵邻接矩阵的底层是一个二维数组，A[i][j] 表示从节点 i 指向节点 j 的一条边，A[i][j]元素的值表示是否存在这条边或者在带权图中表示边的权重。 邻接矩阵的存储方式简单、直接，基于数组，在获取两个顶点的关系时，非常高效；可以将很多图的运算转换成矩阵之间的运算，计算方便。但是最大的缺点是浪费空间，在无向图中，有一半的空间是浪费的。如果我们存储的是稀疏图,也就是说，顶点很多，但每个顶点的边并不多，那邻接矩阵就更加浪费空间。通常我们遇到的都是稀疏图，所以邻接矩阵的存储方法并不常用。 1.2 邻接表 如上图，在邻接表中每个顶点对应一条链表，链表中存储的是与此顶点直接先连的其他顶点。与邻接矩阵相比，邻接表更加节省空间，但是使用起来就比较耗时，如果我们想确定是否存在从 i 指向 j 的边，我们必需遍历顶点 i 上的整个链表。 为了提高查找效率，我们可以将邻接表中的链表改成红黑树、跳表、散列表，甚至将链表改成有序动态数组，通过二分查找的方法来快速定位两个顶点之间否是存在边。至于如何选择，还需要看具体的业务场景。 1.3 应用示例我们以微博的用户关系为例，假设我们需要支持下面这样几个操作： 判断用户 A 是否关注了用户 B； 判断用户 A 是否是用户 B 的粉丝； 根据用户名称的首字母排序，分页获取用户的粉丝列表； 根据用户名称的首字母排序，分页获取用户的关注列表。 社交网络是一张稀疏图，更适合使用邻接表来存储。不过，此处我们需要两个图: 邻接表和逆邻接表。邻接表中存储了用户的关注关系，逆邻接表中存储的是用户的被关注关系，分别用于关注和粉丝两种关系的判断。因为我们有排序需求，而跳表存储的数据本身就是有序的，所以我们选择用跳表来替代链表。 但是对于拥有亿级别用户的微博，显然我们没法将图存在一台机器的内存上。我们可以通过哈希算法等数据分片方式，通过对顶点的哈希然后分片，将邻接表存储在不同的机器上。当要查询顶点与顶点关系的时候，我们就利用同样的哈希算法，先定位顶点所在的机器，然后再在相应的机器上查找。 此外借助于 mysql 这样的外部存储，我们可以将 (user_id, follower_id) 这样的关注关系存储在 mysql 中。相比于图这可能是更好的解决方案。 2. 实现图是顶点和边的集合，我们将图的抽象模型定义为三种数据类型的组合: Vertex,Edge 和 Graph。 VertexVertex ADT 用来表示顶点对象，有一个用来检索所存储元素的方法 element() EdgeEdge ADT 用来表示边，并具有如下方法: element(): 返回保存的边的值 endpoint(): 返回边对应的(u, v)，u为边起点，v为边的终点 opposite(u): 传入边的一个端点，返回边的另一个端点 GraphGraph ADT 表示图，包含如下方法: vertex_count(): 返回图的顶点数量 vertices(): 迭代返回图中的所有顶点 edge_count(): 返回图的边的数量 edges(): 迭代返回图中的所有边 get_edge(u, v): 返回从顶点 u 到顶点 v 的边，不存在返回 None，对于无向图 get_edge(u, v)，get_edge(v, u) 没有区别 degree(v, out=True): 返回顶点的出度，out=False 返回顶点的出度 incident_edges(v, out=True): 迭代返回顶点 v 的输出边，out=False 迭代返回顶点的输入边 insert_vertex(v=None): 创建并返回一个顶点的 Vertex 对象 insert_edge(u, v, x=None): 创建一个从顶点u 到顶点 v，存储元素 x 的 Edge 边对象 remove_vertex(v): 删除顶点及与顶点关联的边 remove_edge(e): 删除边 e 我们接下来就以邻接表，并使用哈希表代替链表的方式实现上述的抽象数据结构。 2.1 图的邻接表实现Vertext 和 Edge 类1234567891011121314151617181920212223242526272829303132class Vertex(object): __slots__ = '_element' def __init__(self, x): self._element = x def element(self): return self._element def __hash__(self): return hash(id(self))class Edge(object): __slots__ = '_origin', '_destination', '_element' def __init__(self, u, v, x): self._origin = u self._destination = v self._element = x def endpoints(self): return self._origin, self._destination def opposite(self, v): return self._destination if v is self._origin else self._origin def element(self): return self._element def __hash__(self): return hash((self._origin, self._destination)) Graph 类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071class Graph(object): def __init__(self, directed=False): \"\"\" :param directed: 是否创建有向图，默认为 False 表示创建无向图 \"\"\" self._outgoing = &#123;&#125; # key 为起点，value 为终点的 # 设计要点: key 为终点，value 为起点的，无向图_incoming 只是 _outgoing 的别名 self._incoming = &#123;&#125; if directed else self._outgoing def is_directed(self): return self._incoming is not self._outgoing def vertex_count(self): return len(self._outgoing) def vertices(self): return self._outgoing.keys() def edge_count(self): total = sum(len(self._outgoing[u]) for u in self._outgoing) if not self.is_directed(): total /= 2 return total def edges(self): result = set() # 对于无向图，需要去重 for u in self._outgoing: result.update(u.values()) return result def get_edge(self, u, v): return self._outgoing[u].get(v) def degree(self, v, outgoing=True): adj = self._outgoing if outgoing else self._incoming return len(adj[v]) def incident_edge(self, v, outgoing=True): adj = self._outgoing if outgoing else self._incoming for edge in adj[v].values: yield edge def insert_vertex(self, x=None): v = Vertex(x=x) self._outgoing[v] = &#123;&#125; if self.is_directed(): self._incoming[v] = &#123;&#125; return v def insert_edge(self, u, v, x): e = Edge(u, v, x) self._outgoing[u][v] = e self._incoming[v][u] = e def remove_vertex(self, v): # 有向图: u --&gt; v ---&gt; v1 # 无向图: u---&gt; v ---- u # 删除以 v 为起点的所有边 for v1 in self._outgoing[v].keys(): del self._incoming[v1][v] del self._outgoing[v] # 删除以 v 为终点的所有边 if self.is_directed(): for u in self._incoming[v].keys(): del self._outgoing[u][v] del self._incoming[v] def remove_edge(self, e): u, v = e.endpoints() del self._outgoing[u][v] del self._incoming[v][u]","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"33 动态规划实战","slug":"alog/dp_3","date":"2018-11-11T16:00:00.000Z","updated":"2019-04-14T01:55:16.572Z","comments":true,"path":"2018/11/12/alog/dp_3/","link":"","permalink":"http://yoursite.com/2018/11/12/alog/dp_3/","excerpt":"编程思想之动态规划实战","text":"编程思想之动态规划实战 1. 动态规划总结上一篇，我们总结了动态规划的使用场景，以及如何利用动态规划去解决问题了，总结了: 一个模型三个特征: 多阶段决策最优解模型，最优子结构，无后效性，重复子问题 状态转移表法 状态转移方程法 并总结对比了四中编程思想之间的区别。这些东西都非常理论，需要慢慢消化。本文是动态规划的实战篇，也是编程思想系列的最后一篇。 2.应用本节我们核心要解决的问题是如何量化两个字符串之间的相似程度呢？有一个非常著名的量化方法，那就是编辑距离（Edit Distance）。 编辑距离指的就是，将一个字符串转化成另一个字符串，需要的最少编辑操作次数（比如增加一个字符、删除一个字符、替换一个字符）。编辑距离越大，说明两个字符串的相似程度越小。 根据所包含的编辑操作种类的不同，编辑距离有多种不同的计算方式，比较著名的有莱文斯坦距离（Levenshtein distance）和最长公共子串长度（Longest common substring length）。其中，莱文斯坦距离允许增加、删除、替换字符这三个编辑操作，最长公共子串长度只允许增加、删除字符这两个编辑操作。莱文斯坦距离的大小，表示两个字符串差异的大小；而最长公共子串的大小，表示两个字符串相似程度的大小。 下面是两个方法的操作示例，我们的问题是如何计算两个字符串的莱文斯坦距离和最长公共子串长度。 2.1 计算莱文斯坦距离首先我们来看回溯的处理过程。如果 a[i] 与 b[j] 匹配，我们递归考察 a[i+1] 和 b[j+1]。如果 a[i] 与 b[j] 不匹配，那我们有多种处理方式可选： 可以删除 a[i]，然后递归考察 a[i+1] 和 b[j]； 可以删除 b[j]，然后递归考察 a[i] 和 b[j+1]； 可以在 a[i] 前面添加一个跟 b[j] 相同的字符，然后递归考察 a[i] 和 b[j+1]; 可以在 b[j] 前面添加一个跟 a[i] 相同的字符，然后递归考察 a[i+1] 和 b[j]； 可以将 a[i] 替换成 b[j]，或者将 b[j] 替换成 a[i]，然后递归考察 a[i+1] 和 b[j+1]。 反过来看状态 (i, j) 可能从 (i-1, j)，(i, j-1)，(i-1, j-1) 三个状态中的任意一个转移过来。我们可以尝试着将把状态转移的过程，用公式写出来。这就是我们前面讲的状态转移方程 1234567如果：a[i]!=b[j]，那么：min_edist(i, j) 就等于：min(min_edist(i-1,j)+1, min_edist(i,j-1)+1, min_edist(i-1,j-1)+1)如果：a[i]==b[j]，那么：min_edist(i, j) 就等于：min(min_edist(i-1,j)+1, min_edist(i,j-1)+1，min_edist(i-1,j-1))其中，min 表示求三数中的最小值。 2.2 计算最长公共子串长度首先我们先来看回溯的处理思路。我们从 a[0] 和 b[0] 开始，依次考察两个字符串中的字符是否匹配。 如果 a[i] 与 b[j] 互相匹配，我们将最大公共子串长度加一，并且继续考察 a[i+1] 和 b[j+1]。 如果 a[i] 与 b[j] 不匹配，最长公共子串长度不变，这个时候，有两个不同的决策路线： 删除 a[i]，或者在 b[j] 前面加上一个字符 a[i]，然后继续考察 a[i+1] 和 b[j]； 删除 b[j]，或者在 a[i] 前面加上一个字符 b[j]，然后继续考察 a[i] 和 b[j+1]。 反过来也就是说，如果我们要求 a[0…i] 和 b[0…j] 的最长公共长度 max_lcs(i, j)，我们只有可能通过下面三个状态转移过来： (i-1, j-1, max_lcs)，其中 max_lcs 表示 a[0…i-1] 和 b[0…j-1] 的最长公共子串长度； (i-1, j, max_lcs)，其中 max_lcs 表示 a[0…i-1] 和 b[0…j] 的最长公共子串长度； (i, j-1, max_lcs)，其中 max_lcs 表示 a[0…i] 和 b[0…j-1] 的最长公共子串长度。 如果我们把这个转移过程，用状态转移方程写出来，就是下面这个样子： 1234567如果：a[i]==b[j]，那么：max_lcs(i, j) 就等于：max(max_lcs(i-1,j-1)+1, max_lcs(i-1, j), max_lcs(i, j-1))；如果：a[i]!=b[j]，那么：max_lcs(i, j) 就等于：max(max_lcs(i-1,j-1), max_lcs(i-1, j), max_lcs(i, j-1))；其中 max 表示求三数中的最大值。 3. 练习3.1 最长递增子序列我们有一个数字序列包含 n 个不同的数字，如何求出这个序列中的最长递增子序列长度？比如 2, 9, 3, 6, 5, 1, 7 这样一组数字序列，它的最长递增子序列就是 2, 3, 5, 7，所以最长递增子序列的长度是 4。 12345678910111213141516171819202122232425262728293031323334class Solution(object): def lengthOfLIS(self, nums): \"\"\" :type nums: List[int] :rtype: int \"\"\" if not nums: return 0 n = len(nums) status = [None] * (n) status[0] = nums[0] end = 0 for i in range(1, n): end = max(end, self.binary_search(nums[i], status, end)) return end + 1 def binary_search(self, v, status, end, start=0): m = start if status[end] &lt; v: end += 1 status[end] = v return end while start &lt;= end: mid = start + ((end - start) &gt; 1) if status[mid] == v: return end elif status[mid] &lt; v: start = mid + 1 else: if mid == m or status[mid-1] &lt; v: status[mid] = v return end else: end = mid - 1","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"32 动态规划理论","slug":"alog/dp_2","date":"2018-11-10T16:00:00.000Z","updated":"2019-04-14T01:55:16.571Z","comments":true,"path":"2018/11/11/alog/dp_2/","link":"","permalink":"http://yoursite.com/2018/11/11/alog/dp_2/","excerpt":"编程思想之动态规划理论","text":"编程思想之动态规划理论 1. 再论动态规划动态规划比起其三个算法思想更难懂。上一篇文章我们从实践角度介绍了如何利用动态规划解决问题。有了这个基础，接下来我们来解决如下几个问题: 什么样的问题可以用动态规划解决？ 解决动态规划问题的一般思考过程是什么样的？ 贪心、分治、回溯、动态规划这四种算法思想又有什么区别和联系？ 1.1 适用场景动态规划适合解决的问题可以概括为“一个模型三个特征”。 一个模型: 多阶段决策最优解模型。动态规划通常被用来解决最优问题，而解决问题的过程，需要经历多个决策阶段。每个决策阶段都对应着一组状态。然后我们寻找一组决策序列，经过这组决策序列，能够产生最终期望求解的最优值。 三个特征: 最优子结构 无后效性 重复子问题 最优子结构最优子结构指的是，问题的最优解包含子问题的最优解。反过来说就是，我们可以通过子问题的最优解，推导出问题的最优解。如果我们把最优子结构，对应到我们前面定义的动态规划问题模型上，那我们也可以理解为，后面阶段的状态可以通过前面阶段的状态推导出来。 无后效性无后效性有两层含义，第一层含义是，在推导后面阶段的状态的时候，我们只关心前面阶段的状态值，不关心这个状态是怎么一步一步推导出来的。第二层含义是，某阶段状态一旦确定，就不受之后阶段的决策影响。无后效性是一个非常“宽松”的要求。只要满足前面提到的动态规划问题模型，其实基本上都会满足无后效性。 重复子问题这个概念比较好理解。前面一节，我已经多次提过。如果用一句话概括一下，那就是，不同的决策序列，到达某个相同的阶段时，可能会产生重复的状态。 1.2 解题思路解决动态规划问题，一般有两种思路。我把它们分别叫作，状态转移表法和状态转移方程法。 状态转移表法一般能用动态规划解决的问题，都可以使用回溯算法的暴力搜索解决。所以，这种方法与回溯算法相关，通常我们需要进行如下几步: 使用回溯算法，定义状态，画出递归树；判断是否存在重复子问题，看是否能用动态规划解决 画出状态转移表，根据递推关系，分阶段填充状态表中的每个状态 最后，将递推填表的过程，翻译成代码，就是动态规划代码了 状态表一般都是二维的，所以可以把它想象成二维数组。其中，每个状态包含三个变量，行、列、数组值。尽管大部分状态表都是二维的，但是如果问题的状态比较复杂，需要很多变量来表示，那对应的状态表可能就是高维的，比如三维、四维。那这个时候，我们就不适合用状态转移表法来解决了。一方面是因为高维状态转移表不好画图表示，另一方面是因为人脑确实很不擅长思考高维的东西。 状态转移方程法状态转移方程法有点类似递归的解题思路。状态转移方程法的大致思路可以概括为，找最优子结构 - 写状态转移方程 - 将状态转移方程翻译成代码。我们需要分析，某个问题如何通过子问题来递归求解，也就是所谓的最优子结构。根据最优子结构，写出递归公式，也就是所谓的状态转移方程。有了状态转移方程，代码实现就非常简单了。一般情况下，我们有两种代码实现方法，一种是递归加“备忘录”，另一种是迭代递推。 状态转移方程是解决动态规划的关键。如果我们能写出状态转移方程，那动态规划问题基本上就解决一大半了，而翻译成代码非常简单。但是很多动态规划问题的状态本身就不好定义，状态转移方程也就更不好想到。 1.3 四种算法思想比较如果我们将这四种算法思想分一下类，那贪心、回溯、动态规划可以归为一类，而分治单独可以作为一类。前三个算法解决问题的模型，都可以抽象成我们今天讲的那个多阶段决策最优解模型，而分治算法解决的问题尽管大部分也是最优解问题，但是，大部分都不能抽象成多阶段决策模型。 回溯算法是个“万金油”。基本上能用的动态规划、贪心解决的问题，我们都可以用回溯算法解决。回溯算法相当于穷举搜索。穷举所有的情况，然后对比得到最优解。不过，回溯算法的时间复杂度非常高，是指数级别的，只能用来解决小规模数据的问题。对于大规模数据的问题，用回溯算法解决的执行效率就很低了。 尽管动态规划比回溯算法高效，但是，并不是所有问题，都可以用动态规划来解决。能用动态规划解决的问题，需要满足三个特征，最优子结构、无后效性和重复子问题。在重复子问题这一点上，动态规划和分治算法的区分非常明显。分治算法要求分割成的子问题，不能有重复子问题，而动态规划正好相反，动态规划之所以高效，就是因为回溯算法实现中存在大量的重复子问题。 贪心算法实际上是动态规划算法的一种特殊情况。它解决问题起来更加高效，代码实现也更加简洁。不过，它可以解决的问题也更加有限。它能解决的问题需要满足三个条件，最优子结构、无后效性和贪心选择性（这里我们不怎么强调重复子问题）。其中，最优子结构、无后效性跟动态规划中的无异。“贪心选择性”的意思是，通过局部最优的选择，能产生全局的最优选择。每一个阶段，我们都选择当前看起来最优的决策，所有阶段的决策完成之后，最终由这些局部最优解构成全局最优解。 2. 应用有了上面的论述，接下来我们看看如何利用我们所说的动态规划的理论和方法来解决实际问题。 2.1 最小路经假设我们有一个 n 乘以 n 的矩阵 w[n][n]。矩阵存储的都是正整数。棋子起始位置在左上角，终止位置在右下角。我们将棋子从左上角移动到右下角。每次只能向右或者向下移动一位。从左上角到右下角，会有很多不同的路径可以走。我们把每条路径经过的数字加起来看作路径的长度。那从左上角移动到右下角的最短路径长度是多少呢？ 套用上面所讲的一个模型三个特征理论，我们来看看这个是否可以用动态规划来解: 一个模型: 从左上角到右下角可以分成多个步骤移动，显然这是一个多阶段决策问题 三个特征: 首先位置(i, j) 只能由 (i, j-1),(i-1, j) 移动得来，位置(i, j)的最短距离可以从这两个位置的最短距离得来，符合最优子结构， 其次位置(i, j)之后得如何选择与位置(i, j)之前无任何关系符合无后效性特征 最后，一个位置可以由两个位置移动得来，回溯求解中肯定会产生重复子问题因此这个问题能用动态规划解决。 123456789101112131415161718192021222324252627def min_path_in_matrix(matrix): row = len(matrix) column = len(matrix[0]) status = [[0] * column for i in range(row)] s = 0 for c in range(column): s += matrix[0][c] status[0][c] = s s = 0 for r in range(column): s += matrix[r][0] status[r][0] = s for i in range(1, row): for j in range(1, column): status[i][j] = min(status[i][j - 1], status[i - 1][j]) + matrix[i][j] print status return status[-1][-1]ss = [ [1,2,3], [4,5,6], [7,8,9]]min_path_in_matrix(ss) 2.2 硬币找零我们今天来看一个新的硬币找零问题。假设我们有几种不同币值的硬币 v1，v2，……，vn（单位是元）。如果我们要支付 w 元，求最少需要多少个硬币。比如，我们有 3 种不同的硬币，1 元、3 元、5 元，我们要支付 9 元，最少需要 3 个硬币（3 个 3 元的硬币）。 1pass 参考: 王争老师专栏-数据结构与算法之美","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"31 初识动态规划","slug":"alog/dp_1","date":"2018-11-09T16:00:00.000Z","updated":"2019-04-14T01:55:16.571Z","comments":true,"path":"2018/11/10/alog/dp_1/","link":"","permalink":"http://yoursite.com/2018/11/10/alog/dp_1/","excerpt":"编程思想之动态规划初识","text":"编程思想之动态规划初识 1. 动态规划动态规划是几个编程思想中最难的一个，它与回溯密切相关。回溯问题是在一组可能的解中，搜索满足期望的解；采用的方法类似枚举，找出所有解，筛选符合要求的解；而动态规划比较适合用来求解最优问题，比如求最大值、最小值等等。基本上所有的动态规划问题都能用回溯算法解决，但是动态规划能有效避免回溯算法中的重复计算，提高代码执行效率。 1.1 解决思路上一节我们用回溯算法解决了0-1背包问题，并阐述了回溯算法中可能存在重复计算的问题，借助于对子问题的缓存，我们能有效避免重复计算。但是需要注意的是这种方法并不是总是有效。 与回溯算法类似，动态规划中，我们同样把问题分解为多个阶段，每个阶段对应一个决策。我们记录每一个阶段可达的状态集合并去重，然后通过当前阶段的状态集合，来推导下一个阶段的状态集合，直至达到最终状态，并从中选择一个最优解。通过记录每个阶段的所有可达状态并去重来避免重复计算。 尽管动态规划的执行效率提高了，但是动态规划的空间复杂度也提高了，所以，很多时候，我们会说，动态规划是一种空间换时间的算法思想。 2.1 应用2.1 动态规划解0-1背包问题现在我们用动态规划来解决上一节的0-1背包问题，我们把整个求解过程分为 n 个阶段，每个阶段会决策一个物品是否放到背包中。每个物品决策（放入或者不放入背包）完之后，背包中的物品的重量会有多种情况，也就是说，会达到多种不同的状态，对应到递归树中，就是有很多不同的节点。 我们把每一层重复的状态（节点）合并，只记录不同的状态，然后基于上一层的状态集合，来推导下一层的状态集合。我们可以通过合并每一层重复的状态，这样就保证每一层不同状态的个数都不会超过 w 个（w 表示背包的承载重量），也就是例子中的 9。于是，我们就成功避免了每层状态个数的指数级增长。 我们用一个二维数组 states[n][w+1]，来记录每层可以达到的不同状态。n表示第n个物品，w+1 表示当前背包的重量。 12345678910111213141516171819202122232425262728def rucksack_hold(items, weight): status = [[0] * (weight + 1) for i in range(len(items))] status[0][0] = 1 status[0][items[0]] = 1 for i in range(1, len(items)): for j in range(weight + 1): if status[i - 1][j]: status[i][j] = status[i - 1][j] if j + items[i] &lt;= weight: status[i][j + items[i]] = 1 for l in status: print l # 判断可放置的最大重量 j = weight n = len(items) - 1 while j &gt;= 0: if status[n][j]: break print j # 打印最大重量，放置的物品 for i in range(n, 1, -1): if j - items[i] &gt;= 0 and status[i - 1][j - items[i]]: print i, items[i] j -= items[i]rucksack_hold([2, 2, 4, 6, 3], 9) 实际上我们可以有一个比上面空间复杂度更小的解法，代码如下:12345678910111213def rucksack_hold_2(items, weight): status = [0] * (weight + 1) status[0] = 1 status[items[0]] = 1 print status for i in range(1, len(items)): for j in range(weight - items[i], -1, -1): if status[j]: status[j + items[i]] = 1 print statusrucksack_hold_2([2, 2, 4, 6, 3], 9) 2.2 升级的 0-1 背包问题这次我们引入物品价值，要求计算在满足背包最大重量限制的前提下，背包中可装入物品的最大总价值。 使用动态规划的求解过程与上面类似，只不过现在 status 数组记录的不再是0或1，而是当前状态对应的最大总价值。我们把每一层中 (i, cw) 重复的状态（节点）合并，只记录 cv 值最大的那个状态，然后基于这些状态来推导下一层的状态。如果用回溯算法，这个问题就没法再用“备忘录”解决了。 12345678910111213141516171819 def rucksack_hold_3(items, weight, values): status = [[None] * (weight + 1) for i in range(len(items))] status[0][0] = 0 status[0][items[0]] = values[0] for i in range(1, len(items)): for j in range(weight + 1): if status[i - 1][j] &gt;= 0: status[i][j] = status[i - 1][j] if j + items[i] &lt;= weight: v = status[i - 1][j] + values[i] if status[i][j + items[i]] &lt; v: status[i][j + items[i]] = v for l in status: print lprint '------------------'a = [3, 4, 8, 9, 6]# a = [1, 1, 1, 1, 1]rucksack_hold_3([2, 2, 4, 6, 3], 9, a) 3. 练习3.1 练习一杨辉三角我们对杨辉三角进行一些改造。每个位置的数字可以随意填写，经过某个数字只能到达下面一层相邻的两个数字。假设你站在第一层，往下移动，我们把移动到最底层所经过的所有数字之和，定义为路径的长度。请你编程求出从最高层移动到最底层的最短路径长度。 1234567891011121314151617181920212223242526272829303132333435def path_pascal_triangle(pt): \"\"\" :param pt: :return: 计算杨辉三角的最短路径 \"\"\" n = len(pt) status = [] for i in range(0, n): s = [float('inf')] * (i + 1) row = pt[i] if i == 0: s[0] = row[0] s[-1] = row[-1] else: s[0] = row[0] + status[i - 1][0] s[-1] = row[-1] + status[i - 1][-1] status.append(s) print status for i in range(2, n): for j in range(1, i): left = j - 1 right = j status[i][j] = min(status[i - 1][left], status[i-1][right]) + pt[i][j] print status return min(status[-1])ss = [ [3], [1, 2], [5, 6, 7], [1, 1, 1, 1]]print path_pascal_triangle(ss) 参考: 王争老师专栏-数据结构与算法之美","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"30 回溯算法","slug":"alog/backtracking","date":"2018-11-06T16:00:00.000Z","updated":"2020-05-24T14:07:21.340Z","comments":true,"path":"2018/11/07/alog/backtracking/","link":"","permalink":"http://yoursite.com/2018/11/07/alog/backtracking/","excerpt":"编程思想之回溯算法","text":"编程思想之回溯算法 1. 回溯算法回溯算法很多时候都应用在“搜索”这类问题上。即在一组可能的解中，搜索满足期望的解。 回溯的处理思想，有点类似枚举搜索。我们枚举所有的解，找到满足期望的解。为了有规律地枚举所有可能的解，避免遗漏和重复，我们把问题求解的过程分为多个阶段。每个阶段，我们都会面对一个岔路口，我们先随意选一条路走，当发现这条路走不通的时候（不符合期望的解），就回退到上一个岔路口，另选一种走法继续走。 很多经典的数学问题都可以用回溯算法解决，比如数独、八皇后、0-1 背包、图的着色、旅行商问题、全排列等等。我们将以其中的几个问题为例来讲解如何使用回溯算法解决问题。 1.1 解决步骤回溯算法非常适合用递归来实现，在实现的过程中，剪枝操作是提高回溯效率的一种技巧。利用剪枝，我们并不需要穷举搜索所有的情况，从而提高搜索效率。 与递归算法一样，回溯算法容易理解，但是写起来丝毫不容易。个人觉得，相比于找到递归终止条件和递推公式，更难的是确定递归函数的变量和函数的返回值。关于函数变量的选择有一个可参考的经验，就是始终关注的是在计算中会使用到的随着计算不断变动的量；对于函数返回值，回溯算法是枚举所有的解，期望的解通常不是通过函数直接返回，而通常位于递归终止条件中。 2. 应用2.1 八皇后问题所谓八皇后问题是这样的，我们往一个 8x8 的棋盘中放 8 个棋子（皇后），每个棋子所在的行、列、对角线都不能有另一个棋子，找出所有满足要求的摆放方式。下面是一个满足条件和不满足条件的示例。 我们把这个问题划分成 8 个阶段，依次将 8 个棋子放到第一行、第二行、第三行……第八行。在放置的过程中，我们不停地检查当前的方法，是否满足要求。如果满足，则跳到下一行继续放置棋子；如果不满足，那就再换一种方法，继续尝试。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253def queens_eight(num=8): def cal_queens(row): if row == num: output_chessboard(chessboard, num) return for column in range(num): if is_ok(chessboard, row, column, num): # print chessboard, row, column chessboard[row] = column cal_queens(row + 1) # 下标表示行，值表示列 chessboard = [0] * num cal_queens(0) return chessboarddef is_ok(chessboard, row, column, num): \"\"\" :param chessboard: :param row: :param column: :return: 检查最新的(row, column)摆放是否符合规则 \"\"\" left_up, right_up = column - 1, column + 1 last = row - 1 # 从最后一行往上检查 while last &gt;= 0: # 检查同列 if chessboard[last] == column: return False # 检查左上角对角线 if 0 &lt;= left_up == chessboard[last]: return False # 检查右上角对角线 if num &gt; right_up == chessboard[last]: return False last -= 1 left_up -= 1 right_up += 1 return Truedef output_chessboard(result, num): print result for i in range(num): column = result[i] c = ['*'] * num c[column] = '1' print ' '.join(c)queens_eight() 2.2 0-1 背包问题0-1 背包是非常经典的算法问题，这个问题的经典解法是动态规划，不过还有一种简单但没有那么高效的解法，那就是回溯算法。因此这个示例将是我们理解回溯算法和动态规划区别的很重要一个例子。 0-1 背包问题有很多变体，我这里介绍一种比较基础的。背包总的承载重量是 Wkg，有 n 个物品，每个物品的重量不等，并且不可分割。期望在不超过背包所能装载重量的前提下，让背包中物品的总重量最大。 对于每个物品来说，都有两种选择，装或者不装。n 个物品共有 2^n 种装法，去掉超过 Wkg，从剩下的选择种选择总重量最接近 Wkg 的。不过，我们如何才能不重复地穷举出这 2^n 种装法呢？ 我们可以把物品依次排列，整个问题就分解为了 n 个阶段，每个阶段对应一个物品怎么选择。先对第一个物品进行处理，选择装进去或者不装进去，然后再递归地处理剩下的物品。下面是代码实现: 1234567891011121314151617181920212223242526class RucksackHold(object): def __init__(self, weight, items): self.weight = weight self.items = items self.hold = 0 def _get_max_hold(self, i, cw): \"\"\" :param i: 考察的第 i 个物品 :param cw: 当前背包的总重量 :return: \"\"\" if i == len(self.items) or cw == self.weight: if cw &gt; self.hold: self.hold = cw return self._get_max_hold(i + 1, cw) if self.items[i] + cw &lt;= self.weight: self._get_max_hold(i + 1, cw + self.items[i]) def __call__(self, *args, **kwargs): self._get_max_hold(0, 0) return self.holdpk = RucksackHold(items=[1, 2, 4], weight=10)print pk() 回溯中的重复计算在回溯算法中，有些子问题的求解可能是重复的。假设背包的最大承载重量是 9，有 5 个不同的物品，重量分别是 2，2，4，6，3。如果我们把这个例子的回溯求解过程，用递归树画出来，就是下面这个样子： 递归树中的f(i, cw）表示一次函数调用。从递归树中可以发现，有些子问题的求解是重复的，比如图中的 f(2, 2) 和 f(3,4) 都被重复计算了两次。借助于对子问题结果的缓存，我们可以有效避免冗余计算提高计算效率。 2.3 正则表达式正则表达式中，最重要的就是通配符，简单期间，假设正表达式中只包含“*”和“\\?”这两种通配符，并且“*”匹配任意多个（大于等于 0 个）任意字符，“\\?”匹配零个或者一个任意字符。基于如上假设，如何用回溯算法，判断一个给定的文本，能否跟给定的正则表达式匹配？ 正则表达式中的特殊字符就是所谓的岔路口，比如“*”可以匹配任意个文本串中的字符，我们就先随意的选择一种匹配方案，然后继续考察剩下的字符。如果中途发现无法继续匹配下去了，我们就回到这个岔路口，重新选择一种匹配方案，然后再继续匹配剩下的字符。 12345678910class Pattern(): def __init__(self, pattern): self.pattern = pattern self.is_match = False def _match(self, S, i, j): pass def match(self, S): return self._match(S, 0, 0) 2.4 图的着色1234```### 2.5 旅行商问题```python 2.6 全排列1234```### 2.7 数独```python 参考: 王争老师专栏-数据结构与算法之美","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"29 分治算法","slug":"alog/divide","date":"2018-11-04T16:00:00.000Z","updated":"2019-04-14T01:55:16.570Z","comments":true,"path":"2018/11/05/alog/divide/","link":"","permalink":"http://yoursite.com/2018/11/05/alog/divide/","excerpt":"编程思想之分治算法","text":"编程思想之分治算法 1. 分治算法分治算法（divide and conquer）的核心思想其实就是四个字，分而治之 ，也就是将原问题划分成 n 个规模较小，并且结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。 这个定义看起来有点类似递归的定义。关于分治和递归的区别，我们在排序（下）的时候讲过，分治算法是一种处理问题的思想，递归是一种编程技巧。实际上，分治算法一般都比较适合用递归来实现。分治算法的递归实现中，每一层递归都会涉及这样三个操作： 分解：将原问题分解成一系列子问题； 解决：递归地求解各个子问题，若子问题足够小，则直接求解； 合并：将子问题的结果合并成原问题。 1.1 适用情景分治算法能解决的问题，一般需要满足下面这几个条件： 原问题与分解成的小问题具有相同的模式； 原问题分解成的子问题可以独立求解，子问题之间没有相关性，这一点是分治算法跟动态规划的明显区别，等我们讲到动态规划的时候，会详细对比这两种算法； 具有分解终止条件，也就是说，当问题足够小时，可以直接求解； 可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了。 1.2 分治在大数据中的应用我们前面讲的数据结构和算法，大部分都是基于内存存储和单机处理。但是，如果要处理的数据量非常大，没法一次性放到内存中，这个时候，这些数据结构和算法就无法工作了。 要解决这种数据量大到内存装不下的问题，我们就可以利用分治的思想。将海量的数据集合根据某种方法，划分为几个小的数据集合，每个小的数据集合单独加载到内存来解决，然后再将小数据集合合并成大数据集合。实际上，利用这种分治的处理思路，不仅仅能克服内存的限制，还能利用多线程或者多机处理，加快处理的速度。 2.应用归并排序和快速排序是分治算法的典型应用，这两个算法我们在之前的排序章节已经讲过了。所以我们以另一个例子: 如何编程求出一组数据的有序对个数或者逆序对个数，来讲解如何应用分治算法。 2.1 计算数据有序度我们套用分治的思想来求数组 A 的逆序对个数。我们可以将数组分成前后两半 A1 和 A2，分别计算 A1 和 A2 的逆序对个数 K1 和 K2，然后再计算 A1 与 A2 之间的逆序对个数 K3。那数组 A 的逆序对个数就等于 K1+K2+K3。 使用分治算法其中一个要求是，子问题合并的代价不能太大，否则就起不了降低时间复杂度的效果了。那如何快速计算出两个子问题 A1 与 A2 之间的逆序对个数呢？这里就要借助归并排序算法了。 1234567891011121314151617181920212223242526272829303132333435363738def reverse_count(A): \"\"\" :param A: :return: 计算数组 A 的逆序度 \"\"\" if len(A) &lt;= 1: return 0 mid = len(A) // 2 S1 = A[:mid] c1 = reverse_count(S1) S2 = A[mid:] c2 = reverse_count(S2) c3 = merge(S1, S2, A) return c1 + c2 + c3def merge(S1, S2, S): \"\"\" :param S1: :param S2: :param S: :return: 归并排序，并计算两个数组的逆序度 \"\"\" c = i = j = 0 while i + j &lt; len(S): if i == len(S1) or (j &lt; len(S2) and S1[i] &gt; S2[j]): S[i + j] = S2[j] j += 1 c += (len(S1) - i) else: S[i + j] = S1[i] i += 1 return cs = [1, 5, 6, 7] + [2, 3, 4]print reverse_count(s)print s 3.练习下面是分治算法的一些典型练习题 3.1 练习一二维平面上有 n 个点，如何快速计算出两个距离最近的点对？1pass 3.2 练习二有两个 n*n 的矩阵 A，B，如何快速求解两个矩阵的乘积 C=A*B？1pass 参考: 王争老师专栏-数据结构与算法之美","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"28 贪心算法","slug":"alog/greedy","date":"2018-11-03T16:00:00.000Z","updated":"2020-05-24T06:03:03.051Z","comments":true,"path":"2018/11/04/alog/greedy/","link":"","permalink":"http://yoursite.com/2018/11/04/alog/greedy/","excerpt":"编程思想之贪心算法","text":"编程思想之贪心算法 1. 贪心算法前面我们讲完了字符串匹配相关的算法，接下来的几章与编程思想有关，包括贪心，分治，回溯和动态规划。它们都非常抽象，但是理解透了可以帮我们解决很多问题。这些算法在一定程度上很相近，因此学习过程中，我们首先要搞清楚它们的适用场景，其次是掌握怎么运用它们去解决问题。今天，我们先来学习贪心算法。 1.1 适用场景贪心，回溯，动态规划都适合解决“分阶段决策问题”。而贪心算法不适合前面的选择，会影响后面的选择这类情景。贪心算法的求解过程中，只会保留每个阶段的最优解，不会保留其他非最优状态。对于后面的选择依赖前面选择的分阶段决策问题，如果考虑前面的选择，计算将无法回溯，不可行；如果只考虑每个阶段的最优，最后很可能无法得出最优解。 1.2 解决步骤利用贪心算法，我们可以按照如下步骤去解决问题: 第一步，当我们看到这类问题的时候，首先要联想到贪心算法，贪心算法格外适用于针对一组数据，我们定义了限制值和期望值，希望从中选出几个数据，在满足限制值的情况下，期望值最大。 第二步，我们尝试看下这个问题是否可以用贪心算法解决：每次选择当前情况下，在对限制值同等贡献量的情况下，对期望值贡献最大的数据。 第三步，我们举几个例子看下贪心算法产生的结果是否是最优的。大部分情况下，举几个例子验证一下就可以了。严格地证明贪心算法的正确性，是非常复杂的，需要涉及比较多的数学推理。而且，从实践的角度来说，大部分能用贪心算法解决的问题，贪心算法的正确性都是显而易见的，也不需要严格的数学推导证明。 2. 应用贪心算法有很多经典应用包括钱币找零，区间覆盖，霍夫曼编码等等。我们就以其中几个例子来实战贪心算法的应用。 2.1 钱币找零假设我们有 1 元、2 元、5 元、10 元、20 元、50 元、100 元面额的纸币， 张数分别是: c1、c2、c5、c10、c20、c50、c100。我们现在要用这些钱来支付 K 元，最少要用多少张纸币呢？ 思路: 在贡献相同期望值（纸币数目）的情况下，我们希望多贡献点金额，这样就可以让纸币数更少，这就是一种贪心算法的解决思路。 123456789101112def cion(k): coin = [100, 50, 20, 10, 5, 2, 1] coin_map = [(i, 20) for i in coin] i = 0 coin_count = 0 while i &lt; len(coin) and k &gt; 0: use = coin_map[i] c = min(use[1], k // use[0]) k -= use[0] * c coin_count += c i += 1 return coin_count 2.2 区间覆盖假设我们有 n 个区间，区间的起始端点和结束端点分别是 [l1, r1]，[l2, r2]，[l3, r3]，……，[ln, rn]。我们从这 n 个区间中选出一部分区间，这部分区间满足两两不相交（端点相交的情况不算相交），最多能选出多少个区间呢？lintcode-1242. 无重叠区间就是这个问题的变形。 思路：我们假设这 n 个区间中最左端点是 lmin，最右端点是 rmax。这个问题就相当于，我们选择几个不相交的区间，从左到右将 [lmin, rmax] 覆盖上。我们按照起始端点从小到大的顺序对这 n 个区间排序。我们每次选择的时候，左端点跟前面的已经覆盖的区间不重合的，右端点又尽量小的，这样可以让剩下的未覆盖区间尽可能的大，就可以放置更多的区间。这实际上就是一种贪心的选择方法。 123456789101112131415161718192021class Solution(object): def eraseOverlapIntervals(self, intervals): \"\"\" :type intervals: List[List[int]] :rtype: int \"\"\" if not intervals: return 0 intervals.sort(key=lambda x: x[0]) collect = [intervals[0]] rm = 0 for i in intervals[1:]: low, up = collect[-1] if i[0] &gt;= up: collect.append(i) elif i[1] &lt;= up: collect[-1] = i rm += 1 else: rm += 1 return rm 2.3 霍夫曼编码霍夫曼编码不仅会考察文本中有多少个不同字符，还会考察每个字符出现的频率，根据频率的不同，选择不同长度的编码。霍夫曼编码试图用这种不等长的编码方法，来进一步增加压缩的效率。如何给不同频率的字符选择不同长度的编码呢？根据贪心的思想，我们可以把出现频率比较多的字符，用稍微短一些的编码；出现频率比较少的字符，用稍微长一些的编码。为了避免解压缩过程中的歧义，霍夫曼编码要求各个字符的编码之间，不会出现某个编码是另一个编码前缀的情况。 12 3. 练习leetcode 上有很多贪心算法的练习题，下面是一些练习题以及它们的解答 3.1 练习一在一个非负整数 a 中，我们希望从中移除 k 个数字，让剩下的数字值最小，如何选择移除哪 k 个数字呢？ 123456789101112131415161718class Solution: def removeKdigits(self, num, k): numStack = [] # Construct a monotone increasing sequence of digits for digit in num: while k and numStack and numStack[-1] &gt; digit: numStack.pop() k -= 1 numStack.append(digit) # - Trunk the remaining K digits at the end # - in the case k==0: return the entire list finalStack = numStack[:-k] if k else numStack # trip the leading zeros return \"\".join(finalStack).lstrip('0') or \"0\" 参考: 王争老师专栏-数据结构与算法之美","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"27 AC 自动机","slug":"alog/ac","date":"2018-11-02T16:00:00.000Z","updated":"2019-04-14T01:55:16.568Z","comments":true,"path":"2018/11/03/alog/ac/","link":"","permalink":"http://yoursite.com/2018/11/03/alog/ac/","excerpt":"敏感词过滤","text":"敏感词过滤","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"26 Trie 树","slug":"alog/trie","date":"2018-11-01T16:00:00.000Z","updated":"2019-04-14T01:55:16.582Z","comments":true,"path":"2018/11/02/alog/trie/","link":"","permalink":"http://yoursite.com/2018/11/02/alog/trie/","excerpt":"一组字符串的快速匹配","text":"一组字符串的快速匹配","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"25 字符串匹配之 BM 算法","slug":"alog/str_match2","date":"2018-10-31T16:00:00.000Z","updated":"2019-04-14T01:55:16.581Z","comments":true,"path":"2018/11/01/alog/str_match2/","link":"","permalink":"http://yoursite.com/2018/11/01/alog/str_match2/","excerpt":"最优匹配的 BM 算法","text":"最优匹配的 BM 算法 1. BM 算法本节我们继续介绍另一个高效的字符串匹配算法 BM(Boyer-Moore)。BM 与 KMP 优化思路类似，都是希望尽可能增加发生不匹配时，模式串后移的位数来提高字符串的匹配效率。BM 要想达到更高的匹配效率，必需利用更多的已知信息。 这里依旧推荐阮一峰老师有关 BM算法的博客。因为阮一峰老师讲的已经不能在通俗易懂，这里我就简单总结一下 BM 算法所使用的匹配规则。示例采用博客中的示例，即在”HERE IS A SIMPLE EXAMPLE”，中搜索 “EXAMPLE”。 1.1 坏字符规则 开始匹配时，主串与模式串头部对齐，从尾部开始比较。此时”S”与”E”不匹配。我们称”S”为”坏字符”（bad character），即不匹配的字符。 利用坏字符以及坏字符是否出现在模式串中，BR 算法使用的第一个移位规则被称为坏字符规则: 后移位数 = 坏字符的位置 - 搜索词中的上一次出现位置。如果”坏字符”不包含在搜索词之中，则上一次出现位置为 -1。道理很显而易见，如果出现坏字符，我们就直接把模式串移动到能跟坏字符匹配的位置上来。 示例中”S”不包含在搜索词”EXAMPLE”之中，这意味着可以把搜索词直接移到”S”的后一位 1.2 好后缀规则借鉴 KMP 算法，利用已经匹配的字符串和已匹配部分是否出现在模式串中，BR 算法使用的第二个移位规则被称为 好后缀规则:后移位数 = 好后缀的位置 - 搜索词中的上一次出现位置 “好后缀”的位置以最后一个字符为准。假定”ABCDEF”的”EF”是好后缀，则它的位置以”F”为准，即5（从0开始计算）。 如果”好后缀”在搜索词中只出现一次，则它的上一次出现位置为 -1。比如，”EF”在”ABCDEF”之中只出现一次，则它的上一次出现位置为-1（即未出现）。 如果”好后缀”有多个，则除了最长的那个”好后缀”，其他”好后缀”的上一次出现位置必须在头部。比如，假定”BABCDAB”的”好后缀”是”DAB”、”AB”、”B”，请问这时”好后缀”的上一次出现位置是什么？回答是，此时采用的好后缀是”B”，它的上一次出现位置是头部，即第0位 道理也很显而易见，如果已经匹配的部分多次出现在模式串，当发生不匹配时，就直接把模式串移动到上一次匹配的位置上。显然 BM 与KMP 不同，BM 不要求后缀匹配的部分必需是模式串的前缀。 “MPLE”与”MPLE”匹配。我们把这种情况称为”好后缀”（good suffix），即所有尾部匹配的字符串。注意，”MPLE”、”PLE”、”LE”、”E”都是好后缀。此时，所有的”好后缀”（MPLE、PLE、LE、E）之中，只有”E”在”EXAMPLE”还出现在头部，所以后移 6 - 0 = 6位。 1.3 移位选择Boyer-Moore算法的基本思想是，每次后移这两个规则之中的较大值。更巧妙的是，这两个规则的移动位数，只与模式串有关，与主串无关。因此，可以预先计算生成《坏字符规则表》和《好后缀规则表》。使用时，只要查表比较一下就可以了。 2. 实现显然 BR 算法的核心是要先生成《坏字符规则表》和《好后缀规则表》，然后利用这两个规则表进行字符串匹配。 2.1 坏字符规则表2.2 好后缀规则表2.3 字符串匹配参考: 王争老师专栏-数据结构与算法之美 阮一峰-Boyer-Moore算法 《数据结构与算法：python语言实现》","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"24 字符串匹配之 KMP 算法","slug":"alog/str_match3","date":"2018-10-30T16:00:00.000Z","updated":"2019-04-14T01:55:16.581Z","comments":true,"path":"2018/10/31/alog/str_match3/","link":"","permalink":"http://yoursite.com/2018/10/31/alog/str_match3/","excerpt":"优雅的的 KMP 算法","text":"优雅的的 KMP 算法 1. KMP 算法BM（Boyer-Moore）和 KMP(Knuth-Morris-Pratt) 都是非常高效的字符串匹配算法。BM 比 KMP 更高效，有实验统计 BM 的性能 是 KMP 3-4 倍。但是他们都非常复杂难懂。除了专栏，我也非常推荐你看一看阮一峰老师有关 BM 和 KMP 算法的介绍。因为 BM 算法利用到了KMP的算法思想，本节我们就先来介绍 KMP 的实现。 1.1 优化思路KMP 算法基于这样一个实现思路: 如下图所示，对于字符串匹配过程中已经匹配的部分，我们是已知的；利用这个已知的信息，我们可以把模式串往后移动更多位，而不是 BR 算法中的一位。而最终移动的位数取决于已匹配部分的&quot;前缀&quot;和&quot;后缀&quot;的最长的共有元素的长度，我们将这个最长的公共子串称为最长可匹配(前缀/后缀)子串 在上面的图例中，已匹配部分是 ABCDAB，前后缀最长匹配的元素是 AB，因此前缀 AB就可以直接来到后缀AB的位置，直接向后移动 4 位继续匹配，如下图所示。 字符串已匹配部分永远是模式串的前缀子串，因此最长可匹配(前缀/后缀)子串我们可以提前计算出来，这个就是 KMP 中的 部分匹配表。因此，整个 KMP 的计算过程就分成了两步: 计算部分匹配表 根据部分匹配表计算每次不匹配时，模式串的移动位数，进行字符串匹配 1.2 部分匹配表部分匹配表，被称为失效函数，又称为 next 数组。在计算 next 数组之前，首先我们需要明确两个概念: “前缀”和”后缀”: “前缀”指除了最后一个字符以外，一个字符串的全部头部组合 “后缀”指除了第一个字符以外，一个字符串的全部尾部组合 12345678以&quot;ABCDAB&quot;为例0. &quot;A&quot;的前缀和后缀都为空集，共有元素的长度为0；1. &quot;AB&quot;的前缀为[A]，后缀为[B]，共有元素的长度为0；2. &quot;ABC&quot;的前缀为[A, AB]，后缀为[BC, C]，共有元素的长度0；3. &quot;ABCD&quot;的前缀为[A, AB, ABC]，后缀为[BCD, CD, D]，共有元素的长度为0；4. &quot;ABCDA&quot;的前缀为[A, AB, ABC, ABCD]，后缀为[BCDA, CDA, DA, A]，共有元素为&quot;A&quot;，长度为1；5. &quot;ABCDAB&quot;的前缀为[A, AB, ABC, ABCD, ABCDA]，后缀为[BCDAB, CDAB, DAB, AB, B]，共有元素为&quot;AB&quot;，长度为2；6. &quot;ABCDABD&quot;的前缀为[A, AB, ABC, ABCD, ABCDA, ABCDAB]，后缀为[BCDABD, CDABD, DABD, ABD, BD, D]，共有元素的长度为0。 即”ABCDAB” 的 next 数组为 [0, 0, 0, 0, 1, 2, 0]。其中 next 数组的下标对应每个前缀子串结尾字符的下标 next 数组的值则是最长可匹配子串的长度 1.3 KMP 复杂度分析KMP 的空间复杂度是 O(m)，时间复杂度为 O(m+n)。分析过程在我们讲解完 KMP 的实现之后再来讲解。 2. KMP 算法实现2.1 计算部分匹配表部分匹配表的计算非常巧妙，下面是代码： 123456789101112131415def kmp_next(P): m = len(P) fail = [0] * m j = 1 # 按照下标从小到大的子串 k = 0 # 上一个子串最长可匹配子串的长度 while j &lt; m: if P[j] == P[k]: fail[j] = k + 1 j += 1 k += 1 elif k &gt; 0: k = fail[k - 1] else: j += 1 return fail 我们以”ABCDAB”为例来讲解计算过程，大家需要牢记的是P[j] 表示当前子串的最后一个字符，P[k] 表示上一个子串的最长可匹配子串的下一个字符，此时分为三种情况: P[j] == P[k]: 对应ABCDAB，前一个子串是ABCDA，最长可匹配子串是 A，此时P[5]==P[1]==B，即AB=AB,所以 ABCDAB的最长可匹配子串长度就是 2 P[j] != P[k] and k &gt; 0: 对应ABCDABD，P[6]!=P[2]，即D!=C，此时可以确定的是ABCDABD的最长可匹配子串，只能从ABD进行匹配，进而问题转换为已知AB的最长可匹配子串，求ABD的最长可匹配子串问题。 P[j] != P[k] and k == 0: 显然此时就没有任何可匹配到的子串。 这个计算过程很巧妙，不多看几次很难明白。 在next 的计算过程中，使用了一个额外的数组，因此这一部份的空间复杂度是 O(m)。在 while 循环中 j 执行的次数一定不会超过 m，而 k 变量无论是增加累计的量，还是减少累计的量都不会超过 m，因此这一部分的时间复杂度为 O(m)。 2.2 KMP 字符串匹配过程字符串匹配的过程中，最重要的一步是确定不匹配时，后移的位数，代码如下: 12345678910111213141516def kmp_match(T, P): fail = kmp_next(P) n, m = len(T), len(P) k = 0 j = 0 while j &lt; n: if T[j] == P[k]: if k == m - 1: return j - (m - 1) k += 1 j += 1 elif k &gt; 0: k = fail[k - 1] # 后移表示为 k 索引的变化 else: j += 1 return -1 整个匹配过程中，j 变量的执行次数不会超过 n，而变量 k，无论是增加的累计量还是减少的累计量都不会超过 n，因此这一部分的时间复杂度不会超过 O(n)。因此总的时间复杂度不会超过O(m+n)。 参考: 王争老师专栏-数据结构与算法之美 阮一峰-KMP算法 《数据结构与算法：python语言实现》","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"23 字符串匹配之 BF & RK 算法","slug":"alog/str_match1","date":"2018-10-29T16:00:00.000Z","updated":"2019-04-14T01:55:16.580Z","comments":true,"path":"2018/10/30/alog/str_match1/","link":"","permalink":"http://yoursite.com/2018/10/30/alog/str_match1/","excerpt":"粗暴匹配的 BF 与 RK 算法","text":"粗暴匹配的 BF 与 RK 算法 1. 特性从本节开始我们将学习字符串匹配算法。字符串匹配算法有很多，大体可以分成两类: 单模式串匹配算法: 一个串跟一个串进行匹配，包括BF，RK，KMP，BM 算法 多字符串匹配算法: 一个串中同时查找多个串，包括 Trie 树和 AC 自动机 本节我们先来学习“最简单粗暴的” BF 和 RK 算法。为了便于描述，对于在字符串A中查找 B，我们将 A 称为主串，B 称为模式串，n=len(A), m=len(B)。 1.1 BF 算法BF(Brute Force) 暴力匹配算法，采用的就是我们最容易理解的穷举法。从主串的第一位开始检查主串中所有长度为 m 的子串看是否与模式串相等。主串中有(n-m+1)个长度为 m 的子串，因此总共需要比较(n-m+1)*m次 BF 算法的时间复杂度很高，为 O(m*n)，但却是一个比较常用的字符串匹配算法，而原因有两个: 大部分情况下，模式串和主串的长度都不会太长。而且每次模式串与主串中的子串匹配的时候，当中途遇到不能匹配的字符的时候，就可以就停止了，不需要把 m 个字符都比对一下。所以，尽管理论上的最坏情况时间复杂度是 O(n*m)，但是，统计意义上，大部分情况下，算法执行效率要比这个高很多。 算法简单，实现起来不容易出错 1.2 RK 算法RK 算法的全称叫 Rabin-Karp 算法，是由它的两位发明者 Rabin 和 Karp 的名字来命名的。RK 算法可以理解为引入了哈希算法的 BF。 RK 算法希望通过计算字符串的哈希值，并通过比较哈希值，而不是比较字符串，来缩小BF算法中主串的子串与模式串的比较时间。要想达到优化的目的，我们必需使得(字符串哈希+哈希值比较的时间) &lt; (m 次字符比较的时间)。因为哈希值是整数，单次整数的比较时间可以忽略不计。但是字符串哈希值的计算也需要遍历每个字符，因此想要优化，必需精心设计此处的哈希函数。 为了减少字串哈希值的计算量，在计算第 i 个字串的哈希值时，需要能用到已经计算的第 i-1 个字串的哈希值，并且平衡好计算过程中空间占用和哈希冲突的概率。因为 RK 算法并不常用，所以这里我不再过多讲述，有兴趣的同学可以自己查看专栏的介绍。 RK 算法的时间复杂度取决于哈希函数，理想情况下，RK 算法的时间复杂度是 O(n)，如果存在冲突的情况下，时间复杂度可能会退化。极端情况下，时间复杂度就退化为 O(n*m)。 2. 实现2.1 BF 算法123456789101112131415def find_brute(T, P): \"\"\" :param T: 主串 :param P: 模式串 :return: \"\"\" n, m = len(T), len(P) for i in range(n - m + 1): j = 0 while j &lt; m and T[i + j] == P[j]: j += 1 if j == m: return i return -1 3. 应用今天讲的一维字符串匹配可以应用到二维空间中。即如下图所示，在一个二维主串中搜索另一个二维模式串。 下面是代码实现:1pass 参考: 王争老师专栏-数据结构与算法之美 《数据结构与算法：python语言实现》","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"22 堆的应用","slug":"alog/heap_use","date":"2018-10-28T16:00:00.000Z","updated":"2019-04-14T01:55:16.576Z","comments":true,"path":"2018/10/29/alog/heap_use/","link":"","permalink":"http://yoursite.com/2018/10/29/alog/heap_use/","excerpt":"动态 topK","text":"动态 topK 1. 堆的应用2. 实现2.1 合并小文件2.2 高性能定时器2.3 动态 topK2.4 动态求中位数参考: 王争老师专栏-数据结构与算法之美 《数据结构与算法：python语言实现》","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"21 堆","slug":"alog/heap","date":"2018-10-27T16:00:00.000Z","updated":"2019-04-14T01:55:16.576Z","comments":true,"path":"2018/10/28/alog/heap/","link":"","permalink":"http://yoursite.com/2018/10/28/alog/heap/","excerpt":"能找到”最好学生”的堆","text":"能找到”最好学生”的堆 1. 特性堆是一种特殊的二叉树，它满足如下两个属性: 堆是一完全二叉树 堆中每个节点的值都必需大于等于(或小于等于)其子树中每个节点的值，下称为 Heap-Order 完全二叉树被定义为除了最后一层，其他层的节点个数都是满的，最后一层的节点都靠左排列。所以完全二叉树具有如下一些特性: 非常适合使用数组进行存储，不会出现空间浪费 如果下标从 1 开始，下标为 i 的节点的左右子节点的下标是 2*i，2*i+1； 对于一个有 n 个元素的完全二叉树，树的高度为 logn 为了维护堆的Heap-Order，当我们更改堆中的元素时，我们需要在堆中上下交换堆的元素，额外交换的次数不会超过树的高度即 logn，所以堆的更新操作的时间复杂度为 O(logn)。 1.1 支持的操作堆支持以下一些常用操作: 添加一个元素: 将元素添加到数组的末尾，并对其从下往上的堆化，时间复杂度为 logn 删除堆顶元素: 删除堆顶元素，并用数组末尾元素填充堆顶，对新的堆顶元素从上往下的堆化，时间复杂度为 logn 构建堆: 自底向上的构建堆，时间复杂度为O(n) 堆排序: 包括建堆和排序，排序的时间复杂度为O(nlogn) 1.2 堆排序与快速排序堆排序与快速排序都是原地排序算法，排序的平均时间复杂度都是O(nlogn)，甚至堆排序比快排更加稳定。但是快排的性能还是比堆排序要好，原因有两个: 堆排序数据访问的方式没有快排友好。快排中数据是顺序访问的，但是堆排序是按照指数跳越访问的，对 CPU 缓存不友好 对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序。对于基于比较的排序算法来说，整个排序过程就是由两个基本的操作组成的，比较和交换（或移动）。快速排序数据交换的次数不会比逆序度多。但是堆排序的第一步是建堆，建堆的过程会打乱数据原有的相对先后顺序，导致原数据的有序度降低。 2. 实现2.1 小堆的实现我们选择小堆作为堆实现的示例，大堆的实现类似。对于堆而言最核心的就是从下往上和从上往下的堆化操作。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132class PriorityQueueBase(object): class _Item(object): __slots__ = '_key', '_value' def __init__(self, key, value): self._key = key self._value = value def __gt__(self, other): return self._key &gt; other._key def __lt__(self, other): return self._key &lt; other._key def __eq__(self, other): return self._key == other._keyclass HeapPriorityQueue(PriorityQueueBase): def __init__(self, content=()): \"\"\" :return: 构建堆 \"\"\" self._data = [self._Item(k,v) for k, v in content] if self._data: self._heap() def _heap(self): \"\"\" \"\"\" i = self._parent(len(self._data) - 1) while i &gt;= 0: self._downheap(i) i -= 1 def _parent(self, i): \"\"\" :param i: :return: 父节点索引 \"\"\" return (i - 1) // 2 def _left(self, i): \"\"\" :param i: :return: 左子节点索引 \"\"\" return i * 2 + 1 def _right(self, i): \"\"\" :param i: :return: 右子节点索引 \"\"\" return i * 2 + 2 def has_left(self, i): return self._left(i) &lt; len(self._data) def has_right(self, i): return self._right(i) &lt; len(self._data) def _swap(self, i, j): \"\"\" :return: 数据交换 \"\"\" self._data[i], self._data[i] = self._data[j], self._data[i] def _upheap(self, i): \"\"\" :param i: :return: 从下往上堆化 \"\"\" parent = self._parent(i) while self._data[parent] &gt; self._data[i] and i &gt; 0: self._swap(parent, i) i = parent parent = self._parent(parent) def _downheap(self, i): \"\"\" :param i: :return: 从上往下堆化 \"\"\" while self.has_left(i): small_child = self._left(i) if self.has_right(i): right = self._right(i) if self._data[small_child] &gt; self._data[right]: small_child = right if self._data[i] &gt; self._data[small_child]: self._swap(i, small_child) i = small_child else: break def __len__(self): return len(self._data) def is_empty(self): return len(self) == 0 def add(self, key, value): \"\"\" :param key: :param value: :return: 向堆中添加元素 \"\"\" self._data.append(self._Item(key, value)) self._upheap(len(self._data) - 1) def min(self): \"\"\" :return: 获取堆顶元素，但不删除 \"\"\" if not self.is_empty(): item = self._data[0] return item._key, item._value raise ValueError('Priority Queue is empty') def remove_min(self): \"\"\" :return: 获取并删除堆顶元素 \"\"\" if self.is_empty(): ValueError('Priority Queue is empty') item = self._data[0] self._data[0] = self._data.pop() self._downheap(0) return item._key, item._value 2.2 堆的原排序堆的原排序排序包括两个过程: 建堆+排序。建堆就是上面 _heap 方法展示的过程，通过由底向上构建堆，我们可以在 O(n) 的时间复杂度内实现堆构建。 排序时，我们将堆顶元素与数组最后的元素交换，然后对前 n-1 个元素组成的堆堆化，然后再将堆顶元素与数组倒数第二个元素交换，以此类推，当堆中只剩下一个元素时排序即完成。 很可惜的是，我们上面的小堆实现无法实现堆的原地排序，因为我们无法控制堆中的元素个数，以达到缩减堆范围的目的。但是实现起来也很简单，通过添加额外的可控的计数器作为堆元素个数的记录，而不是直接使用 len(self._data) 我们就可以很容易实现。 2.2 可删除和修改任意位置的堆最后我们介绍一种可更新和删除任意位置的堆。我们使用一个叫作定位器 Locator 对象作为堆中的元素，Locator记录了元素在堆中数组的索引，在执行更新和删除操作时，将Locator作为参数传递给函数，就可以直接定位元素位置，并对其执行更新操作。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class AdaptHeapPriorityQueue(HeapPriorityQueue): class Locator(HeapPriorityQueue._Item): __slots__ = '_index' def __init__(self, key, value, index): super(AdaptHeapPriorityQueue.Locator, self).__init__(key, value) self._index = index def __init__(self): super(AdaptHeapPriorityQueue, self).__init__() def add(self, key, value): token = self.Locator(key, value, len(self._data)) self._data.append(token) self._upheap(len(self._data) - 1) return token def _swap(self, i, j): super(AdaptHeapPriorityQueue, self)._swap(i, j) self._data[i]._index = i self._data[j]._index = j def _bubble(self, j): if j &gt; 0 and self._data[j] &lt; self._data[self._parent(j)]: self._upheap(j) else: self._downheap(j) def update(self, loc, key, value): j = loc._index if not (0 &lt; j &lt; len(self) and self._data[j] is loc): raise ValueError('invalid locator') loc._key = key loc._value = value self._bubble(j) def remove(self, loc): j = loc._index if not (0 &lt; j &lt; len(self) and self._data[j] is loc): raise ValueError('invalid locator') if j == len(self) - 1: self._data.pop() else: self._data[j] = self._data.pop() self._bubble(j) return loc._key, loc._value 3 算法堆有众多应用，限于篇幅，我们在接下来的一节来专门讲解。 参考: 王争老师专栏-数据结构与算法之美 《数据结构与算法：python语言实现》","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"20 递归树","slug":"alog/recursion_tree","date":"2018-10-26T16:00:00.000Z","updated":"2019-04-14T01:55:16.578Z","comments":true,"path":"2018/10/27/alog/recursion_tree/","link":"","permalink":"http://yoursite.com/2018/10/27/alog/recursion_tree/","excerpt":"利用树计算递归函数的时间复杂度","text":"利用树计算递归函数的时间复杂度 参考: 王争老师专栏-数据结构与算法之美","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"19 红黑二叉树","slug":"alog/red_black_tree","date":"2018-10-25T16:00:00.000Z","updated":"2019-04-14T01:55:16.578Z","comments":true,"path":"2018/10/26/alog/red_black_tree/","link":"","permalink":"http://yoursite.com/2018/10/26/alog/red_black_tree/","excerpt":"搞不懂的”红黑数”….","text":"搞不懂的”红黑数”…. 参考: 王争老师专栏-数据结构与算法之美 《数据结构与算法：python语言实现》","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"18 二叉查找树与完全二叉树","slug":"alog/binary_tree","date":"2018-10-24T16:00:00.000Z","updated":"2019-04-14T01:55:16.570Z","comments":true,"path":"2018/10/25/alog/binary_tree/","link":"","permalink":"http://yoursite.com/2018/10/25/alog/binary_tree/","excerpt":"有散列表了，为什么还要”一颗树”","text":"有散列表了，为什么还要”一颗树” 1. 特性2. 实现2.1 二叉搜索树我们实现的二叉搜索树将支持: 标准映射操作: __setitem__ __getitem__ __delitem__ 有序映射操作: find_lt find_range 基于位置操作 after(p) before(p) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244from collections import MutableMappingfrom linked_tree import LinkedBinaryTreeclass MapBase(MutableMapping): class _Item(object): __slots__ = '_key', '_value' def __init__(self, k, v): self._key = k self._value = v def __eq__(self, other): return self._key == other._key def __lt__(self, other): return self._key &lt; other._key def __gt__(self, other): return self._key &gt; other._keyclass TreeMap(LinkedBinaryTree, MapBase): class Position(LinkedBinaryTree.Position): def key(self): return self.element()._key def value(self): return self.element()._value def _subtree_search(self, p, k): \"\"\" :param p: :param k: :return: 在子树中搜索值为 k 的节点，未搜索到返回最后搜索路经的最终位置 \"\"\" p_value = p.key() if p_value == k: return p elif p_value &gt; k: if self.left(p): return self._subtree_search(self.left(p), k) else: if self.right(p): return self._subtree_search(self.right(p), k) return p def _subtree_first_position(self, p): \"\"\" :return: 返回子树迭代时，第一个位置节点 \"\"\" walk = p while self.left(walk): walk = self.left(walk) return walk def _subtree_last_position(self, p): \"\"\" :param p: :return: 返回子树迭代时，最后一个位置节点 \"\"\" walk = p while self.right(walk): walk = self.right(walk) return walk ################# 引导方法 ####################### def first(self): \"\"\" :return: 返回树迭代序列的第一个节点 \"\"\" return self._subtree_first_position(self.root()) if len(self) &gt; 0 else None def last(self): \"\"\" :return: 返回树迭代序列的最后一个节点 \"\"\" return self._subtree_last_position(self.root()) if len(self) &gt; 0 else None def before(self, p): \"\"\" :param p: :return: 返回迭代序列中位于 p 之前的，最大节点 \"\"\" self._validate(p) if self.left(p): return self._subtree_last_position(self.left(p)) else: walk = p ancestor = self.parent(walk) while ancestor and self.left(ancestor) is walk: walk = ancestor ancestor = self.parent(ancestor) return ancestor def after(self, p): \"\"\" :param p: :return: 返回迭代序列中位于 p 之后的，最小节点 \"\"\" self._validate(p) if self.right(p): self._subtree_first_position(self.right(p)) else: walk = p ancestor = self.parent(walk) while ancestor and self.right(ancestor) is walk: walk = ancestor ancestor = self.parent(ancestor) return ancestor def find_position(self, k): \"\"\" :param k: :return: 查找值等于 k 的位置节点 \"\"\" if self.is_empty(): return None else: p = self._subtree_search(self.root(), k) # avl 平衡树的钩子函数 self._rebalance_access(p) return p ####################### 有序映射 ###################### def find_min(self): \"\"\" :return: 查找树中的最小值 \"\"\" if self.is_empty(): return None else: p = self.first() return p.key(), p.value() def find_ge(self, k): \"\"\" :param k: :return: 查找大于等于 k 的最小节点 \"\"\" p = self.find_position(k) if p and p.key() &lt; k: p = self.after(p) return p.key(), p.value() if p else None, None def find_range(self, start, stop): \"\"\" :param start: :param stop: :return: 查找值位于 start &lt;= k &lt; stop 的节点 \"\"\" if not self.is_empty(): if start is None: p = self.first() else: p = self.find_position(start) if p and p.key() &lt; start: p = self.after(p) while p and (stop is None or p.key() &lt; stop): yield p.key(), p.value() p = self.after(p) ########################### 增删改查节点操作 ################ def __getitem__(self, item): \"\"\" :param item: :return: 查找 item 映射的值 \"\"\" if not self.is_empty(): p = self.find_position(item) self._rebalance_access(p) if p.key() == item: return p.value() raise KeyError('Key Error:' + repr(item)) def __setitem__(self, key, value): \"\"\" :param key: :param value: :return: 设置键 key 的值为 value \"\"\" if self.is_empty(): leaf = self._add_root(self._Item(key, value)) else: p = self.find_position(key) if p.key() == key: p.element()._value = value self._rebalance_access(p) return else: item = self._Item(key, value) if p.key() &lt; key: leaf = self._add_right(p, item) else: leaf = self._add_left(p, item) self._rebalance_insert(leaf) def __iter__(self): \"\"\" :return: 产生键的一个迭代 \"\"\" p = self.first() while p: yield p.key() p = self.after(p) def delete(self, p): \"\"\" :param p: :return: 删除位置节点 p \"\"\" self._validate(p) if self.left(p) and self.right(p): r = self._subtree_last_position(self.left(p)) self._replace(p, r.element()) p = r parent = self.parent(p) self._delete(p) self._rebalance_delete(parent) def __delitem__(self, key): \"\"\" :param key: :return: 删除键 key \"\"\" if not self.is_empty(): p = self._subtree_search(self.root(), key) if p.key() == key: self.delete(p) return self._rebalance_access(p) raise KeyError('Key Error: ' + repr(key)) ################### 平衡二叉树的钩子函数 ############### def _rebalance_delete(self, p): pass def _rebalance_insert(self, p): pass def _rebalance_access(self, p): pass 参考: 王争老师专栏-数据结构与算法之美 《数据结构与算法：python语言实现》","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"17 树的存储与遍历","slug":"alog/tree_base","date":"2018-10-23T16:00:00.000Z","updated":"2019-04-14T01:55:16.582Z","comments":true,"path":"2018/10/24/alog/tree_base/","link":"","permalink":"http://yoursite.com/2018/10/24/alog/tree_base/","excerpt":"如何表示和存储一颗树？","text":"如何表示和存储一颗树？ 1. 特性树是我们接触的第一种非线性结构，在树中一个”父”元素可以有一个或多个”子”元素，这种组织关系要比一个序列中两个元素之间简单的”前”,”后”关系更加复杂。 最常用的树是二叉树，即一个父节点最多只有两个子节点，在二叉树的基础上如果我们按照特定的数据分布在树的各个节点组织数据，我们就可以得到诸如二叉搜索树，堆，红黑二叉树等多种具有特定用途的数据结构。 下面就是从树到二叉树的抽象层次结构，本节我们就来介绍如何存储和实现一个树。 123 Tree(树) BinaryTree(二叉树) LinkedTreeArrayBinaryTree LinkedBinaryTree 我们将 Tree，BinaryTree 实现为抽象基类，来定义和抽象普通树和二叉树可执行操作，并以二叉树的链式存储为例来实现一颗二叉树。我们会在堆章节中实现一个基于数组的二叉树。一颗普通树的链式存储与基于数组的存储与二叉树类似，我们会简单阐述它们的实现方式。 2. 实现2.1 TreeTree 被实现为 Python 抽象基类，我们使用一种叫作 Position 的位置对象作为对树节点访问的代理。通过 Position 对象提供的辅助功能，我们可以验证待操作节点是否属于被操作的树，并抽象树的节点所表达的”父子”，以及迭代过程中的前后关系。 一个普通树能执行的操作有限，通过包括以下几种: 获取和判断树的根节点 获取节点的子节点树，并借此判断节点是否为叶子节点 获取节点的父节点和所有子节点 获取树的所有节点 获取树中节点个数，判断树是否未空 获取树或节点的高度和深度 树的前序遍历和后序遍历 需要注意的是中序是二叉树特有的遍历方式，一颗普通的树没有中序遍历。下面是一个普通树的抽象实现。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100import abcclass Tree(object): __metaclass__ = abc.ABCMeta class Position(object): __metaclass__ = abc.ABCMeta @abc.abstractmethod def element(self): \"\"\" :return: 返回存储在 p 中的元素值 \"\"\" @abc.abstractmethod def __eq__(self, other): pass @abc.abstractmethod def __ne__(self, other): return not self == other @abc.abstractmethod def root(self): \"\"\" :return: 返回树的根节点 \"\"\" pass @abc.abstractmethod def parent(self, p): \"\"\" :param p: :return: 返回 p 节点的父节点 \"\"\" pass @abc.abstractmethod def children(self, p): \"\"\" :param p: :return: 返回 p 节点孩子的迭代 \"\"\" @abc.abstractmethod def num_children(self, p): \"\"\" :param p: :return: 返回节点 p 孩子的个数 \"\"\" pass @abc.abstractmethod def __len__(self): pass def is_root(self, p): \"\"\" :param p: :return: 判断位置 p 表示的节点是否是根节点 \"\"\" return self.root() == p def is_leaf(self, p): \"\"\" :param p: :return: 判断位置 p 表示的节点是否是叶子节点 \"\"\" return self.num_children(p) == 0 def is_empty(self): \"\"\" :return: 判断树是否为空 \"\"\" return len(self) == 0 def depth(self, p): \"\"\" :param p: :return: 返回 p 节点的深度 \"\"\" if self.is_root(p): return 0 else: return 1 + self.depth(self.parent(p)) def height(self, p=None): \"\"\" :return: 返回树的高度 \"\"\" p = p or self.root() return self._height(p) def _height(self, p): if self.is_leaf(p): return 0 else: return 1 + max(self._height(c) for c in self.children(p)) 2.2 BinaryTree相对于普通树，二叉树是具有如下属性的树: 每个节点至多两个节点 每个节点被命名为左右子节点 在顺序上，同一个节点左孩子优于右孩子 因此二叉树与普通的树相比多了如下3个操作: 获取节点的左右孩子 获取节点的兄弟节点 需要注意的是虽然封装原则表名类的外部行为不需要依赖类的内部实现，而操作的效率却极大的依赖实现方式，所以我们更倾向于在 Tree 类的每个更具体的子类中提供合适的更新操作。因此我们不会在基类中限制树的更新操作。 1234567891011121314151617181920212223242526272829303132333435363738394041424344class BinaryTree(Tree): __metaclass__ = abc.ABCMeta @abc.abstractmethod def left(self, p): \"\"\" :param p: :return: 返回节点的左孩子 \"\"\" pass @abc.abstractmethod def right(self, p): \"\"\" :param p: :return: 返回节点的右孩子 \"\"\" pass def slide(self, p): \"\"\" :param p: :return: 返回节点的兄弟节点 \"\"\" parent = self.parent(p) if parent is not None: left = self.left(parent) right = self.right(parent) if left == p: return right else: return left def children(self, p): \"\"\" :param p: :return: 返回节点的所有子节点 \"\"\" left = self.left(p) if p is not None: yield left right = self.right(p) if right is not None: yield right 2.3 LinkedBinaryTreeLinkedBinaryTree 是我们第一个具体实现的链式二叉树。除了必需实现的抽象方法，更新操作外，我们还提供了树的四中遍历方式，用来迭代树中的元素。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235from collections import dequefrom tree import BinaryTreeclass LinkedBinaryTree(BinaryTree): class _Node(object): __slots__ = \"element\", \"parent\", \"left\", \"right\" def __init__(self, element, parent=None, left=None, right=None): self.element = element self.parent = parent self.left = left self.right = right class Position(BinaryTree.Position): def __init__(self, container, node): self._node = node self._container = container def element(self): return self._node.element def __eq__(self, other): return type(other) is type(self) and self._node is other._node def _make_position(self, node): if node is not None: return self.Position(self, node) def _validate(self, p): if not isinstance(p, self.Position): raise TypeError('p must be proer Position type') if p._container is not self: raise ValueError('p not belong to this container') if p._node.parent is p._node: raise ValueError('p will no longer valid') return p._node def __init__(self): self._root = None self._size = 0 def __len__(self): return self._size def root(self): return self._make_position(self._root) def parent(self, p): node = self._validate(p) return self._make_position(node.parent) def left(self, p): node = self._validate(p) return self._make_position(node.left) def right(self, p): node = self._validate(p) return self._make_position(node.right) def num_children(self, p): node = self._validate(p) count = 0 if node.left is not None: count += 1 if node.left is not None: count += 1 return count def _add_root(self, e): \"\"\" :param e: :return: 向树添加根节点 \"\"\" if self._root is not None: raise ValueError('Root exists') self._root = self._Node(e) self._size += 1 return self._make_position(self._root) def _add_left(self, p, e): \"\"\" :param p: :param e: :return: 为节点添加左子节点 \"\"\" node = self._validate(p) if node.left is not None: raise ValueError('Left child exists') self._size += 1 left_node = self._Node(e, node) node.left = left_node return self._make_position(left_node) def _add_right(self, p, e): \"\"\" :param p: :param e: :return: 为节点添加左子节点 \"\"\" node = self._validate(p) if node.right is not None: raise ValueError('right child exists') self._size += 1 right_node = self._Node(e, node) node.right = right_node return self._make_position(right_node) def _replace(self, p, e): \"\"\" :param p: :param e: :return: 替换节点的元素值 \"\"\" node = self._validate(p) old = node.element node.element = e return old def _delete(self, p): \"\"\" :param p: :return: 删除节点， 不能通过移动元素值来删除元素，因为 Position 内部是通过 Node 判断是否相等的 \"\"\" node = self._validate(p) if node.left and node.right: raise ValueError('p must leaf') child = node.left if node.left else node.right if child is not None: child.parent = node.parent if node is self._root: self._root = child else: if node is node.parent.left: node.parent.left = child else: node.parent.right = child node.parent = node self._size -= 1 return node.element def attach(self, p, t1, t2): \"\"\" :param p: :param t1: :param t2: :return: 在叶子节点附加左右子树 \"\"\" node = self._validate(p) if not type(self) is type(t1) is type(t2): raise TypeError() if not self.is_leaf(p): raise ValueError('p must leaf') self._size += len(t1) + len(t2) if not t1.is_empty(): node.left = t1._root t1._root.parent = node t1._size = 0 t1._root = None if not t2.is_empty(): node.right = t2._root t2._root.parent = node t2._size = 0 t2._root = None def positions(self): \"\"\" :return: 返回树所有位置的一个迭代 \"\"\" return self.preorder() def __iter__(self): for p in self.positions(): yield p.element() def preorder(self): \"\"\" :return: 树的前序遍历 \"\"\" if not self.is_empty(): for p in self._subtree_preorder(self.root()): yield p def _subtree_preorder(self, p): yield p for i in self.children(p): for other in self._subtree_preorder(i): yield other def postorder(self): \"\"\" :return: 后序遍历 \"\"\" if not self.is_empty(): for p in self._subtree_postorder(self.root()): yield p def _subtree_postorder(self, p): for i in self.children(p): for other in self._subtree_preorder(i): yield other yield p def breadthfirst(self): \"\"\" :return: 广度优先遍历 \"\"\" if not self.is_empty(): queue = deque() queue.append(self.root()) while len(queue) &gt; 0: p = queue.popleft() for c in self.children(p): queue.append(c) yield p def inorder(self): \"\"\" :return: 中序遍历 \"\"\" if not self.is_empty(): return self._subtree_inorder(self.root()) def _subtree_inorder(self, p): left = self.left(p) if left is not None: for other in self._subtree_inorder(left): yield other yield p right = self.right(p) if right is not None: for other in self._subtree_inorder(right): yield other 3.相关算法不考虑特殊的树，仅仅是普通的二叉树就有很多应用，比如计算目录的容量，表达式树。与树相关的递归也是经常考的算法题。但是考虑篇幅的原因，我会在讲解完所有的树之后，用几篇单独的文章来说明与树相关的算法。 3.1 表达式树作为二叉树的第一个例子，我们将使用二叉树来表示算数表达式的结构。我们将定义一个 BinaryTree 的子类 ExpressionTree，在其内部的每个节点必需存储一个操作符，每个叶子节点则必需存储一个数字。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677from expression import infix_to_postfixfrom linked_tree import LinkedBinaryTreeclass ExpressionTree(LinkedBinaryTree): def __init__(self, token, left=None, right=None): super(ExpressionTree, self).__init__() if not isinstance(token, (str, unicode)): raise TypeError('Token must be a string') self._add_root(token) if left or right: if token not in '+-*/': raise ValueError('token must be in +-*/') self.attach(self.root(), left, right) def __str__(self): result = [] if not self.is_empty(): self._parenthesize_recur(self.root(), result) return ''.join(result) def _parenthesize_recur(self, p, result): if self.is_leaf(p): result.append(p.element()) else: result.append('(') self._parenthesize_recur(self.left(p), result) result.append(p.element()) self._parenthesize_recur(self.right(p), result) result.append(')') def evaluate(self): \"\"\" :return: 计算表达式树的值 \"\"\" return self._evaluate_cur(self.root()) def _evaluate_cur(self, p): if self.is_leaf(p): return float(p.element()) else: left = self._evaluate_cur(self.left(p)) op = p.element() right = self._evaluate_cur(self.right(p)) if op == '+': return left + right elif op == '-': return left - right elif op == '/': return left / right else: return left * right @staticmethod def build_expression_tree(expression): \"\"\" :param expression: 表达式默认以空格分隔 :return: 构建表达式树 \"\"\" stack = [] postfix = infix_to_postfix(expression) for i in postfix: if i not in '+-*/': stack.append(ExpressionTree(i)) else: right = stack.pop() left = stack.pop() stack.append(ExpressionTree(i, left, right)) t = stack.pop() return tif __name__ == '__main__': expression = '10 / 5 + 1 + ( 100 / 10 )' t = ExpressionTree.build_expression_tree(expression) print t print t.evaluate() 在原书 《数据结构与算法：python语言实现》 中，通过 build_expression_tree 方法构建表达式树时，要求传入的表达式必需是完全括号，即形如 2 * 6 + 2 的表达式必需写成(2 * 6) + 2 才能正确执行。对于一般的算数表达式必需先借助栈，将中缀表达式转换为后缀表达式才能正确构建表达式树，整个过程类似于栈中表达式的求值过程。 3.2 树遍历的应用树的遍历有很多应用，但是这些应用都有一个共通的特点，即他们都是在树的遍历过程的前后附加一些特殊操作。利用面向对象编程中的模板方法模式，我们可以将树的遍历过程定义为一个通用的计算机制，并在迭代的过程中定义好钩子函数。所有类似的应用都可以通过继承并自定义钩子函数的方式快速实现。 对于树的遍历而言，通常有四个变量是我们会利用的信息，我们需要在遍历的前后将它们传递给钩子函数: p: 当前节点的位置对象 d: p 的深度 path: 从根到 p 的路经 result: p 所有子节点的遍历结果 下面是树遍历过程的模板方法的实现。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class EulerTour(object): def __init__(self, tree): self._tree = tree def execute(self): if not self._tree.is_empty(): return self._tour(self._tree.root(), 0, []) def _tour(self, p, d, path): \"\"\" :param p: :param d: :param path: :param result: :return: \"\"\" self._hook_previsit(p, d, path) path.append(0) result = [] for c in self._tree.children(p): result.append(self._tour(c, d + 1, path)) path[-1] += 1 value = self._hook_postvisit(p, d, path, result) path.pop() return value def _hook_previsit(self, p, d, path): pass def _hook_postvisit(self, p, d, path, result): passclass BinaryEulerTour(BinaryEulerTour): def __init__(self, tree): super(BinaryEulerTour, self).__init__(tree) def execute(self): if not self._tree.is_empty(): return self._tour(self._tree.root(), 0, []) def _tour(self, p, d, path): self._hook_previsit(p, d, path) result = [None, None] if self._tree.left(p): path.append(0) result[0] = self._tour(self._tree.left(p), d + 1, path) path.pop() self._hook_invisit(p, d, path) if self._tree.right(p): path.append(1) result[1] = self._tour(self._tree.right(p), d + 1, path) path.pop() value = self._hook_postvisit(p, d, path, result) return value def _hook_invisit(self, p, d, path): pass 此时如果我们想构建一个表示目录结构的目录树，并计算目录的大小，借助于 EulerTour 可以很容易的实现 123456class DiskSpace(EulerTour): def __init__(self, tree): super(DiskSpace, self).__init__(tree) def _hook_postvisit(self, p, d, path, result): return p.element() + sum(result) 4. linkcode 习题参考: 王争老师专栏-数据结构与算法之美 《数据结构与算法：python语言实现》","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"16 哈希算法","slug":"alog/hash","date":"2018-10-22T16:00:00.000Z","updated":"2019-04-14T01:55:16.574Z","comments":true,"path":"2018/10/23/alog/hash/","link":"","permalink":"http://yoursite.com/2018/10/23/alog/hash/","excerpt":"如何使用使用哈希算法？","text":"如何使用使用哈希算法？ 1 特性将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是哈希算法，而通过原始数据映射之后得到的二进制值串就是哈希值。优秀的哈希算法必需满足如下几点要求: 不能反向推导: 从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法）； 输入数据敏感: 对输入数据非常敏感，哪怕原始数据只修改了一个 Bit，最后得到的哈希值也大不相同； 散列冲突小: 散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小； 执行效率高: 哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值。 1.1 应用哈希算法的应用非常多，最常见的有如下七种: 安全加密: 不能反向推导 + 散列冲突小，无法通过哈希值逆推出原文 唯一标识: 输入数据敏感 + 散列冲突小，可以通过哈希值的比较间接判断，原文是否相等 数据校验: 输入数据敏感 + 散列冲突小，数据损坏，哈希值就会发生变化 散列函数: 散列函数对哈希算法更加看重的是散列的平均性和哈希算法的执行效率 负载均衡: 利用哈希算法的唯一标识功能，可以将同一客户端 IP 或 session 路由到同一服务器 路由的服务器编号=hash(client_ip or session_id) % len(server_list) 数据分片: 利用哈希算法的唯一标识功能，无需比较就可以将相同的数据归类在一起 分配到的机器编号=hash(keyword) / len(server_list) 分布式存储: 数据分片 + 一致性哈希算法 2. 实现2.1 一致性哈希算法 利用一致性哈希算法，可以解决缓存等分布式系统的扩容、缩容导致数据大量搬移的难题。下面是 Python 实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657'''consistent_hashing.py is a simple demonstration of consistenthashing.'''import bisectimport hashlibclass ConsistentHash: '''ConsistentHash(n,r) creates a consistent hash object for a cluster of size n, using r replicas. It has three attributes. num_machines and num_replics are self-explanatory. hash_tuples is a list of tuples (j,k,hash), where j ranges over machine numbers (0...n-1), k ranges over replicas (0...r-1), and hash is the corresponding hash value, in the range [0,1). The tuples are sorted by increasing hash value. The class has a single instance method, get_machine(key), which returns the number of the machine to which key should be mapped.''' def __init__(self, num_machines=1, num_replicas=1): self.num_machines = num_machines self.num_replicas = num_replicas hash_tuples = [(j, k, my_hash(str(j) + \"_\" + str(k))) \\ for j in range(self.num_machines) \\ for k in range(self.num_replicas)] # Sort the hash tuples based on just the hash values hash_tuples.sort(lambda x, y: cmp(x[2], y[2])) self.hash_tuples = hash_tuples def get_machine(self, key): '''Returns the number of the machine which key gets sent to.''' h = my_hash(key) # edge case where we cycle past hash value of 1 and back to 0. if h &gt; self.hash_tuples[-1][2]: return self.hash_tuples[0][0] hash_values = map(lambda x: x[2], self.hash_tuples) index = bisect.bisect_left(hash_values, h) return self.hash_tuples[index][0]def my_hash(key): '''my_hash(key) returns a hash in the range [0,1).''' return (int(hashlib.md5(key).hexdigest(), 16) % 1000000) / 1000000.0def main(): ch = ConsistentHash(7, 3) print \"Format:\" print \"(machine,replica,hash value):\" for (j, k, h) in ch.hash_tuples: print \"(%s,%s,%s)\" % (j, k, h) while True: print \"\\nPlease enter a key:\" key = raw_input() print \"\\nKey %s maps to hash %s, and so to machine %s\" \\ % (key, my_hash(key), ch.get_machine(key)) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import bisectimport md5class ConsistentHashRing(object): \"\"\"Implement a consistent hashing ring.\"\"\" def __init__(self, replicas=100): \"\"\"Create a new ConsistentHashRing. :param replicas: number of replicas. \"\"\" self.replicas = replicas self._keys = [] self._nodes = &#123;&#125; def _hash(self, key): \"\"\"Given a string key, return a hash value.\"\"\" return long(md5.md5(key).hexdigest(), 16) def _repl_iterator(self, nodename): \"\"\"Given a node name, return an iterable of replica hashes.\"\"\" return (self._hash(\"%s:%s\" % (nodename, i)) for i in xrange(self.replicas)) def __setitem__(self, nodename, node): \"\"\"Add a node, given its name. The given nodename is hashed among the number of replicas. \"\"\" for hash_ in self._repl_iterator(nodename): if hash_ in self._nodes: raise ValueError(\"Node name %r is \" \"already present\" % nodename) self._nodes[hash_] = node bisect.insort(self._keys, hash_) def __delitem__(self, nodename): \"\"\"Remove a node, given its name.\"\"\" for hash_ in self._repl_iterator(nodename): # will raise KeyError for nonexistent node name del self._nodes[hash_] index = bisect.bisect_left(self._keys, hash_) del self._keys[index] def __getitem__(self, key): \"\"\"Return a node, given a key. The node replica with a hash value nearest but not less than that of the given name is returned. If the hash of the given name is greater than the greatest hash, returns the lowest hashed node. \"\"\" hash_ = self._hash(key) start = bisect.bisect(self._keys, hash_) if start == len(self._keys): start = 0 return self._nodes[self._keys[start]] 参考: 王争老师专栏-数据结构与算法之美 《数据结构与算法：python语言实现》","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"15 散列表与链表","slug":"alog/hash_list","date":"2018-10-21T16:00:00.000Z","updated":"2019-04-14T01:55:16.575Z","comments":true,"path":"2018/10/22/alog/hash_list/","link":"","permalink":"http://yoursite.com/2018/10/22/alog/hash_list/","excerpt":"“形影不离”的散列表与链表","text":"“形影不离”的散列表与链表 1. 特性散列表和链表，经常会被放在一起使用。原因是散列表虽然支持高效的数据插入、删除、查找操作，但是散列后的数据都是无序存储的，无法支持按照某种顺序快速地遍历数据。散列表是动态的数据结构，如果每次按序访问都要拷贝到数组，排序然后在遍历，效率太低了。而支持动态创建的链表刚好能解决散列表的有序遍历问题。 2. 散列表的实现在讲解散列表与链表的应用之前，我们先来解决上一篇文章遗漏的散列表的实现问题。散列表的原理并复杂，但是一个高效的哈希函数可能数学家精心研究的结果。这里我们不弄的太过复杂，我们介绍两种哈希表的实现，一种使用分离链表，另一种使用包含线性探测的开放寻址。 虽然这两种实现解决冲突的方法差异很大，但是也存在很多共性，因此我们基于 MapBase 扩展一个新的 HashMapBase 类。 2.1 HashMapBase123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from abc import abstractmethodfrom random import randrangeclass HashMapBase(MapBase): def __init__(self, cap=11, p=109345121): self._table = cap * [None] self._n = 0 # 压缩函数 MAD 的参数 # [(ai + b) mod p] mod N # - N: 散列表内部数组的大小 # - p: 比 N 大的素数 # - a，b 是从区间 [0, p-1] 任意选择的整数，并且 a &gt; 0 self._prime = p # p self._scale = 1 + randrange(p - 1) # a self._shift = randrange(p) # b def _hash_function(self, k): return (hash(k) * self._scale + self._shift) % self._prime % len(self._table) def __len__(self): return self._n def __getitem__(self, k): j = self._hash_function(k) return self._bucket_setitem(j, k) def __setitem__(self, key, value): j = self._hash_function(key) self._bucket_setitem(j, key, value) if self._n &gt; len(self._table) // 2: self._resize(2 * len(self._table) - 1) def __delitem__(self, key): j = self._hash_function(key) self._bucket_delitem(j, key) self._n -= 1 def _resize(self, c): old = list(self.items()) self._table = c * [None] self._n = 0 for k, v in old: self[k] = v @abstractmethod def _bucket_getitem(self, j, k): pass @abstractmethod def _bucket_setitem(self, j, k, v): pass @abstractmethod def _bucket_delitem(self, j, k): pass 2.2 ChainHashMap分离链表法的实现 12345678910111213141516171819202122232425262728class ChainHashMap(HashMapBase): def _bucket_getitem(self, j, k): bucket = self._table[j] if bucket is None: raise KeyError('Key error' + repr(k)) return bucket[k] def _bucket_setitem(self, j, k, v): if self._table[j] is None: # 可以使用链表，红黑树，跳表优化, # 示例使用的是 &lt;11 映射&gt; 实现的 UnsortedTableMap self._table[j] = UnsortedTableMap() oldsize = len(self._table[j]) self._table[j][k] = v if len(self._table[j]) &gt; oldsize: self._n += 1 def _bucket_delitem(self, j, k): bucket = self._table[j] if bucket is None: raise KeyError('Key error' + repr(k)) del bucket[k] def __iter__(self): for bucket in self._table: if bucket is not None: for key in bucket: yield key 2.3 ProbeHashMap线性探测的开放寻址法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class ProbeHashMap(HashMapBase): # 标记删除的哨兵 _AVAIL = object() def _is_available(self, j): return self._table[j] is None or self._table[j] is ProbeHashMap._AVAIL def _find_slot(self, j, k): # 位置探测 first_avail = None while True: if self._is_available(j): if first_avail is None: first_avail = j # 查找失败，探测到的第一个空位置 if self._table[j] is None: return False, first_avail # 查找成功，元素 k 的位置 elif k == self._table[j]._key: return True, j j = (j + 1) % len(self._table) def _bucket_getitem(self, j, k): found, s = self._find_slot(j, k) if not found: raise KeyError('Key error' + repr(k)) return self._table[s]._value def _bucket_setitem(self, j, k, v): found, s = self._find_slot(j, k) if not found: self._table[s] = self._Item(k, v) self._n += 1 else: self._table[s]._value = v def _bucket_delitem(self, j, k): found, s = self._find_slot(j, k) if not found: raise KeyError('Key error' + repr(k)) self._table[s] = self._AVAIL def __iter__(self): for j in range(len(self._table)): if not self._is_available(j): yield self._table[j]._key 3. 散列表与链表3.1 LRU 缓存淘汰算法借助于散列表和链表可以实现时间复杂度降为 O(1)的 LRU 缓存淘汰算法。这里我们使用 Python 内置的 dict 与 链表一章实现的 DoubleLink。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class LRU(object): def __init__(self, capacity=3): assert capacity &lt;= 0 self.capacity = capacity self.num = 0 self.link = DoubleLink() self.node_mp = &#123;&#125; def _re_cache(self, value): \"\"\" :return: 值存在更新缓存 \"\"\" if value in self.node_mp: node = self.node_mp[value] self.link.delete_node(node) self.node_mp[value] = self.link.insert_head(value) return True return False def cache(self, value): if not self._re_cache(value): if self.num &lt; self.capacity: # 缓存未满 self.node_mp[value] = self.link.insert_head(value) self.num += 1 else: # 缓存满 node_tail = self.link._tail._pre del self.node_mp[node_tail._element] self.link.delete_node(node_tail) self.node_mp[value] = self.link.insert_head(value) def __str__(self): r = [] s = self.link._head._next while s != self.link._tail: r.append(str(s._element)) s = s._next return '--&gt;'.join(r)lru = LRU()lru.cache(1)lru.cache(2)lru.cache(3)print lrulru.cache(1)print lrulru.cache(1)print lrulru.cache(4)print lru 参考: 王争老师专栏-数据结构与算法之美 《数据结构与算法：python语言实现》","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"14 散列表","slug":"alog/hash_map","date":"2018-10-20T16:00:00.000Z","updated":"2019-04-14T01:55:16.575Z","comments":true,"path":"2018/10/21/alog/hash_map/","link":"","permalink":"http://yoursite.com/2018/10/21/alog/hash_map/","excerpt":"散列表原理","text":"散列表原理 1. 特性散列表是数组的一种扩展，利用的是数组支持按照下标随机访问的特性，其由三个核心部分组成: key: 元素的键 hash func: 散列函数，将键隐射为底层数组的下标 table: 底层的数组 散列表通过散列函数把元素的键映射为数组的下标来实现在数组中保存和查询元素。在整个散列表的实现中，下面是我们要关注的核心问题： 散列函数设计 散列冲突的解决 装载因子以及散列表的动态扩容 1.1 散列函数散列函数在设计上有三点基本要求: 因为数组下标是从 0 开始的的，所以散列函数计算得到的散列值必需是一个非负整数； 如果 key1 = key2，那 hash(key1) == hash(key2)； 如果 key1 ≠ key2，那 hash(key1) ≠ hash(key2) 第三点看起来合情合理，但是要想找到一个不同的 key 对应的散列值都不一样的散列函数，几乎是不可能的。因此散列过程中会产生散列冲突。而且数组的存储空间有限，也会加大散列冲突的概率。 1.2 散列冲突常用的散列冲突解决方法有两类，开放寻址法（open addressing）和链表法（chaining）。 开放寻址法开放寻址法的核心思想是，如果出现了散列冲突，就重新探测一个空闲位置，将其插入。重新探测新的位置有很多方法，常见有线性探测，二次探测和双重散列，我们将其统称为探测函数。散列函数和探测函数一起，确定了元素的一系列可存储位置。 插入过程就是按序探测第一个非空位置并存储 查找过程就是按照相同的探测顺序，逐一比较数组中的元素和要查找的元素直至找到相等元素(找到)或一个空位置(不存在)。 因为数组空闲位置是判断是查找的判定条件，所以不能通过直接将数组元素置空来删除散列表中的元素。我们可以将删除的元素，特殊标记为 deleted。当探测查找的时候，遇到标记为 deleted 的空间，并不是停下来，而是继续往下探测。 不管采用哪种探测方法，当散列表中空闲位置不多的时候，散列冲突的概率就会大大提高。为了尽可能保证散列表的操作效率，我们会尽可能保证散列表中有一定比例的空闲槽位。我们用装载因子（load factor）来表示空位的多少。 散列表的装载因子 = 填入表中的元素个数 / 散列表的长度 装载因子越大，说明空闲位置越少，冲突越小，散列表的性能越好。 链表法 链表法是一种更加常用的散列冲突解决办法，在散列表中，每个“桶（bucket）”或者“槽（slot）”会对应一条链表，所有散列值相同的元素我们都放到相同槽位对应的链表中。 插入时，通过散列函数计算出对应的散列槽位，将其插入到对应链表中，时间复杂度是 O(1)。 查找、删除时，通过散列函数计算出对应的槽，然后遍历链表查找或者删除。时间复杂度跟链表的长度 k 成正比，也就是 O(k)。对于散列比较均匀的散列函数来说，理论上讲，k=n/m，其中 n 表示散列中数据的个数，m 表示散列表中“槽”的个数。 2. 实现散列表的性能与散列函数，散列冲突和装载因子有关，要想实现一个工业级的散列表就要从这三个因素入手。 2.1 散列函数设计散列函数的设计遵循以下几个要点: 散列函数不能太复杂 散列函数生成的值要尽可能随机并且均匀分布，这样才能避免或者最小化散列冲突 实际工作中，还需要综合考虑包括关键字的长度、特点、分布、还有散列表的大小等在内的各个因素 2.2 装载因子控制对于没有频繁插入和删除的静态数据集合来说，因为数据是已知的，我们可以根据数据的特点、分布等，设计出完美的、极少冲突的散列函数。 对于动态数据集，我们可以动态扩缩容: 装载因子过大时，重新申请一个更大的散列表，动态扩容。 装载因子过小时，可以启动动态缩容。如果我们更加在意执行效率，能够容忍多消耗一点内存空间，也可以不缩容 需要注意的是动态扩缩容时，因为散列表的大小发生了变化，数据存储的位置就变了，所以需要通过散列函数重新计算每个数据的存储位置。在散列表的动态扩容中，装载因子阈值的设置非常重要，太小会导致内存浪费严重，太大会导致冲突过多，要权衡时间、空间复杂度。如果内存空间不紧张，对执行效率要求很高，可以降低负载因子的阈值；相反，如果内存空间紧张，对执行效率要求又不高，可以增加负载因子的值，甚至可以大于 1。 2.3 避免低效扩容动态扩容一个 1G 的散列表依旧很慢，为了解决一次性扩容耗时过多的情况，我们可以将扩容操作穿插在插入操作的过程中，分批完成。 当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个到多个数据放入到新散列表。将扩容过程分散到每次的插入操作中。 2.4 冲突解决方法选择开放寻址法开放寻址法中，散列表的数据都存储在数组中，所以开放寻址法的优点与使用数组类似 可以有效地利用 CPU 缓存加快查询速度 序列化起来比较简单。 但是缺点也很明显: 删除数据的时候比较麻烦，需要特殊标记已经删除掉的数据 所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。 使用开放寻址法只能适用装载因子小于 1 的情况。接近 1 时，就可能会有大量的散列冲突，导致大量的探测、再散列等，性能会下降很多。所以比起链表法更浪费内存空间。当数据量比较小、装载因子小的时候，适合采用开放寻址法。这也是 Java 中的 ThreadLocalMap使用开放寻址法解决散列冲突的原因. 链表法链表法利用的是链表这种离散的内存空间，因此 对内存的利用率更高。因为链表结点可以在需要的时候再创建，无需像开放寻址法那样事先申请好 对大装载因子的容忍度更高。对于链表法来说，只要散列函数的值随机均匀，即便装载因子变成 10，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多。 但是缺点也很明显，链表对于存储小的数据会浪费很多空间(指针的存在)，离散的内存分布也无法利用 CPU 的缓存加速。 链表法中的链表可以改造为其他高效的动态数据结构，比如跳表、红黑树。这样，即便出现散列冲突，极端情况下，所有的数据都散列到同一个桶内，那最终退化成的散列表的查找时间也只不过是 O(logn)。这样也就有效避免了前面讲到的散列碰撞攻击。 所以基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。 2.5 java 的 HashMap我们以 java 中的 HashMap 来说一说如何实现一个工业及的散列表: 初始大小: HashMap 默认的初始大小是 16，这个默认值是可以设置的 装载因子和动态扩容: HashMap 最大装载因子默认是 0.75，当 HashMap 中元素个数超过 0.75*capacity（capacity 表示散列表的容量）的时候，就会启动扩容，每次扩容都会扩容为原来的两倍大小。 散列冲突解决: HashMap 底层采用链表法，在 JDK1.8 版本中引入了红黑树。而当链表长度太长（默认超过 8）时，链表就转换为红黑树。当红黑树结点个数少于 8 个的时候，又会将红黑树转化为链表 3. Python 的 dict上面我们介绍了散列表的实现原理，但是实现一个工业及的散列表是在大量数学和实验的基础上实现的。因为我主要用的还是 Python ，因此我就以 Python 中的dict 实现来介绍当前工业级的散列表的最佳实践，并着手实现一个简单的散列表。 3.1 散列函数Python 中的散列函数通常有两个部分组成: 哈希码: 将一个键映射到一个整数 压缩函数: 将哈希码映射到散列表内部数组的索引 这么做的目的是将哈希码的计算与具体的散列表大小相独立，这样就可以为每个对象开发一个通用的哈希码，并且可以用于任意大小的散列表，只有压缩函数与散列表的大小有关。 3.2 哈希码不同对象的哈希码有如下几种实现方式: 对于数值类型的对象，我们可以简单的把用于表示数值 X 的各个位所表示的值作为它的哈希码。如果数值的位数超过哈希码的长度，比如将 64 位浮点数哈希为 32 位的整数，可以对前后 32 求和或做异或处理 对于字符串或元组形式表示的可变长度对象，通常使用多项式哈希和循环移位哈希，这两种方式都会考虑字符串中字符的位置 多项式哈希码的计算方式如下，其中 i 表示字符串中第 i 个字符，a 是非 0 常数。在处理英文字符串时 33，37，39，41 是合适 a 值。 1p(t)=x1 + a(x2 + a(x3 + .... + a(xn−1 + axn))) 循环移位的 Python 算法如下，在处理英文字符串时 5 位移动能产生最少的散列冲突。 1234567def hash_code(s): mask = (1 &lt;&lt; 32) - 1 h = 0 for c in s: h = (h &lt;&lt; 5 &amp; mask) | (h &gt;&gt; 27) h += ord(c) return h Python 中的哈希码Python 中只有不可变的数据类型可以哈希，以确保一个对象的生命周期中其哈希码不变。对于字符串 Python 使用类似于多项式哈希码的技术，精心设计了字符串的哈希码，没有使用异或和相加。使用相似的基于元组每个元素的哈希码的组合技术计算元组的哈希码。对于 frozenset 对象，元素的顺序是无关的，因此一个自然的选择是用异或值计算单个哈希码而不用任何移位。 用户自定义对象默认是不可哈希的，除非自定义 __hash__ 内置函数，hash() 函数会调用此方法获取对象的哈希码。通过计算组合属性的哈希码作为自定义对象的哈希码是常见方法。 123def __hash__(): return hash(self._red, self._green, self._blue) 一个重要的规则是，__eq__ 与 __hash__ 的实现必需一致，即如果x==y，则 hash(x)==hash(y)。比如 hash(1)==hash(1.0) 3.3 压缩函数一个好的压缩函数应该确保两个不同的哈希码映射到相同索引的可能性为 1/N，工业级别最常用的压缩函数是 MAD(Multiply-Add-Divide)。选择这个压缩函数是为了消除在哈希码集合中的重复模式，并且得到更好的哈希函数。 1234[(ai + b) mod p] mod N- N: 散列表内部数组的大小- p: 比 N 大的素数- a，b 是从区间 [0, p-1] 任意选择的整数，并且 a &gt; 0 3.4 散列冲突处理散列冲突解决方案中的开放寻址法，有多个变种。常见的包括线性探测，二次探测，双哈希策略。Python 中采用的是迭代地探测桶。这种方法下散列表的负载因子可以达到 2/3。12345# 迭代地探测桶A[(h(k) + f(i)) mod N]- h(k): 哈希码- f(i): 基于伪随机数产生器的函数，它提供一个基于原始哈希码位的可重复的但是随机 的，连续的地址探测序列 4. 散列表的简单实现作为散列表实现的简单示例，我们将介绍散列表的两种实现，链表法和线性探测的开放寻址法。限于篇幅，我们把这部分内容放到下一篇文章来讲解。 参考: 王争老师专栏-数据结构与算法之美 《数据结构与算法：python语言实现》","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"13 跳表","slug":"alog/skip_list","date":"2018-10-19T16:00:00.000Z","updated":"2020-05-10T10:03:57.935Z","comments":true,"path":"2018/10/20/alog/skip_list/","link":"","permalink":"http://yoursite.com/2018/10/20/alog/skip_list/","excerpt":"跳表: 链表上的“二分查找”","text":"跳表: 链表上的“二分查找” 1. 特性跳表是一种动态数据结构，支持快速的插入、删除、查找操作，时间复杂度都是 O(logn)。实现上跳表使用空间换时间的思想，通过构建多级索引来提高查询的效率，实现了基于链表的“二分查找”。 1.1 跳表的结构跳表就是在有序链表的基础上添加了多层”索引”。通过每隔几个节点提取一个节点形成上层索引，每层索引的节点个数成等比数列分布，从顶向下的每次查询都会将查询区间“折半”，从而达到 O(logN) 的时间复杂度。每次查询对查询区间的缩减取决于索引构建策略，通过改变索引构建策略，有效平衡执行效率和内存消耗。待会我们会看到更加具体的分析过程。 跳表是一种各方面性能都比较优秀的动态数据结构，可以支持快速的插入、删除、查找操作，写起来也不复杂，甚至可以替代红黑树。Redis 中的有序集合（Sorted Set）就是在跳表的基础上实现的。 1.2 跳表的查找假设我们每隔两个节点构建一层索引，最上层有两个节点，总共有 N 个节点。则第 h 层的节点个数为 N/2^h，包含最底层的链表在内总共有 logN 层。如果每一层都要遍历 m 个结点，那在跳表中查询一个数据的时间复杂度就是 O(m*logn)。对于每隔两个节点构建的索引 m=3。 原因是，假设我们要查找的数据是 x，在第 k 级索引中，我们遍历到 y 结点之后，发现 x 大于 y，小于后面的结点 z，所以我们通过 y 的 down 指针，从第 k 级索引下降到第 k-1 级索引。在第 k-1 级索引中，y 和 z 之间只有 3 个结点（包含 y 和 z），所以，我们在 K-1 级索引中最多只需要遍历 3 个结点，依次类推，每一级索引都最多只需要遍历 3 个结点。 所以在跳表中查询任意数据的时间复杂度就是 O(logn)。而整个跳表需要额外添加的节点数为n/2+n/4+n/8…+8+4+2=n-2，所以空间复杂度为 O(n)。 如果我们每三个结点或五个结点，抽一个结点到上级索引。总的索引结点大约就是 n/3+n/9+n/27+…+9+3+1=n/2，而查询时间复杂度的系数就会从 3 变成 4。因此通过改变索引构建策略，有效平衡执行效率和内存消耗。 1.3 跳表的插入跳表的插入有两个要点: 要保证原始链表中数据的有序性 要维护索引与原始链表大小之间的平衡，避免复杂度退化 因此在插入前需要先找到插入位置，然后通过一个随机函数，来决定将这个结点插入到哪几级索引中。整个过程的时间复杂度= O(logn)(查找) + O(1)(链表的插入) 1.4 跳表的删除删除的过程只是在查找的基础上多了链表的删除操作，对于双向链表而言删除的时间复杂度也是 O(logn)。需要注意的是删除的节点也可能出现在索引中，需要一并删除。 1.5 跳表与红黑树跳表和红黑树都是非常高效的动态数据结构，在插入、删除、查找以及迭代输出有序序列上，时间复杂度都是 O(logn)。但是存在以下不同: 按照区间来查找数据，跳表比红黑树更加高效，跳表可以在 O(logn) 的时间复杂度定位区间的起点，然后在原始链表中顺序往后遍历即可 相对于红黑树跳表更加简单灵活，通过改变索引构建策略，可以有效平衡执行效率和内存消耗 红黑树出现的更早，很多编程语言中的 Map 类型都是通过红黑树实现的。可以直接拿来用，但是跳表并没有一个现成的实现，想要使用必须自己实现。 2. 实现跳表的实现有以下几个关键点: SkipNode 表示调表中的一个节点，每个SkipNone都包含一个 next=[SkipNone]: len(next) 就是当前节点的层高 next[i] 表示第 i 层的后继节点 通过随机法，来决定一个节点的层数 无论查找，插入，还是删除，我们都需要获取带查找节点的前驱节点 查找前驱节点，必须从最顶层查找到最底层，因为需要保留每一层的前驱节点 Go 语言中，我们还需要为保存的值实现接口 下面是调表的 Python 与 Go 实现。 2.1 Python 实现下面是 Python 的跳表实现。 12 2.2 Go 实现12 参考: 王争老师专栏-数据结构与算法之美 《数据结构与算法：python语言实现》","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"11 映射","slug":"alog/map","date":"2018-10-18T16:00:00.000Z","updated":"2019-04-14T01:55:16.577Z","comments":true,"path":"2018/10/19/alog/map/","link":"","permalink":"http://yoursite.com/2018/10/19/alog/map/","excerpt":"无处不在的映射","text":"无处不在的映射 1. 映射前面我们讲解了基于数组和链表最基础的数据结构。在继续下面的内容之前，我们先来说一说映射。因为映射与我们接下来的很多数据结构与算法相关。映射可以看作是搜索或查找的扩展，后面介绍的很多数据结构都是为实现快速的增删改查。因此在继续其他数据结构的介绍之前，我想先介绍一下映射的抽象数据类型以及它的常见几种实现方式。 1.1 映射的抽象数据类型抽象基类Python 中使用抽象基类来表达抽象数据类型。如下所示，抽象基类包含两类方法 一是由 abc.abstractmethod 装饰的抽象方法，这些方法必需由继承自抽象基类的子类来提供具体实现 二是在抽象方法基础上定义的具体方法，基类上的具体方法通过继承可以实现最大化代码复用 123456789101112import abcclass ADT(object): __metaclass__ = abc.ABCMeta @abc.abstractmethod def abstract_method(self): pass def specific_method(self): return self.abstract_method() Python 中 映射 map 的 ADT 与 MutableMapping 抽象基类相对应。 映射的抽象方法映射 M 有如下五个抽象方法，这些方法必需由子类提供具体实现: M[k]: 返回键 k 对应的值，键不存在则触发异常，对应 Python __getitem__ M[k]=v: 对应 Python __setitem__ del M[k]: 对应 Python __delitem__ len(M): 对应 Python __len__ iter(M): 迭代映射 M 中的所有键，对应 Python __iter__ 映射的具体方法为了方便其他功能实现，映射包含了如下具体方法，子类通过继承 MutableMapping 可以自动获取: K in M M.get(k, d=None) M.setdefault(k, d) M.pop(k, d=None) M.popitem() M.clear() M.keys() M.values() M.items() M.update(M2) M == M2 M ！= M2 因为这些方法很容易做到见名知意，我就不再一一解释了。 1.2 map 的实现层次map ADT 有众多的实现方式，为了方便代码重用，我们使用如下层次结构 MutableMapping 是 Python 提供的映射的抽象，提供了现成的映射具体方法 MapBase: 继承自 MutableMapping，为自定义的映射类型提供扩展支持 UnsortedMap: 基于未排序数组的映射实现 HashMapBase: 映射的散列表实现 SortedTableMap: 基于二分查找的映射实现 SkipList: 映射的跳表实现 TreeMap: 二叉搜索树木及其变种的映射实现 2. 实现本节我们就以最简单的 UnsortedMap 为例，实现一个最简单的映射。更加高级的实现我们会在后面一一讲解。 2.1 MapBaseMapBase 是我们在 MutableMapping 基础上自定义的抽象基类，它提供了一个 _Item 类用于保存键与值的映射关系。12345678910111213141516171819class MapBase(MutableMapping): class _Item(object): __slots__ = '_key', '_value' def __init__(self, k, v): self._key = k self._value = v def __eq__(self, other): return self._key == other._key def __ne__(self, other): return not (self == other) def __lt__(self, other): return self._key &lt; other._key def __gt__(self, other): return self._key &gt; other._key 2.2 UnsortedMap123456789101112131415161718192021222324252627282930class UnsortedTableMap(MapBase): def __init__(self): self._table = [] def __getitem__(self, k): for item in self._table: if k == item._key: return item._value raise KeyError('Key error' + repr(k)) def __setitem__(self, k, v): for item in self._table: if k == item._key: item._value = v return self._table.append(self._Item(k, v)) def __delitem__(self, k): for j in range(len(self._table)): if k == self._table[j]._key: self._table.pop(j) return raise KeyError('Key error' + repr(k)) def __len__(self): return len(self._table) def __iter__(self): for item in self._table: yield item._key","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"12 二分查找","slug":"alog/binary_search","date":"2018-10-18T16:00:00.000Z","updated":"2019-04-14T01:55:16.569Z","comments":true,"path":"2018/10/19/alog/binary_search/","link":"","permalink":"http://yoursite.com/2018/10/19/alog/binary_search/","excerpt":"不简单的简单二分查找","text":"不简单的简单二分查找 1. 特性二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为 0。 二分查找看似简单，但是二分查找的变体一点都不简单，“你”经常看到的二分查找其实是最简单的二分查找即: 有序数组中不存在重复元素，通过二分查找值等于给定值的数据。注意不重复和等于，这里存在很多的变体。 其次二分查找存在很大的局限性: 二分查找依赖的是顺序表结构，简单点说就是数组。 主要原因是二分查找算法需要按照下标随机访问元素。数组按照下标随机访问数据的时间复杂度是 O(1) 如果是链表，随机访问的时间复杂度是 O(n)。如果数据使用链表存储，二分查找的时间复杂就会变得很高。 其他数据结构存储的数据，则无法应用二分查找。 二分查找针对的是有序数据。如果数据没有序，就要先排序。 所以，二分查找只能用在插入、删除操作不频繁，一次排序多次查找的场景中 针对频繁插入删除的动态数据集合二分查找将不再适，快速查找需要使用二叉树 数据量太大不适合二分查找: 二分查找依赖数组，而数组为了支持随机访问，需要连续的内存空间，对内存的要求苛刻。 要注意数组要求的连续内存意味着即便系统上有零散的 2G 内存也无法申请到连续的 1G 内存。虽然数组对内存要求苛刻，但是同等条件下数组却是最省内存空间的存储方式，因为除了数据本身之外，不需要额外存储其他信息。 1.1 二分查找的变形如果放开不重复和等于的限制，二分查找有很多变形，典型的包括: 查找第一个值等于给定值的元素 查找最后一个值等于给定值的元素 查找第一个大于等于给定值的元素 查找最后一个小于等于给定值的元素 凡是用二分查找能解决的，绝大部分我们更倾向于用散列表或者二叉查找树。即便是二分查找在内存使用上更节省，但是毕竟内存如此紧缺的情况并不多。实际上，“值等于给定值”的二分查找确实不怎么会被用到，二分查找更适合用在“近似”查找问题，在这类问题上，二分查找的优势更加明显。比如今天讲的这几种变体问题，用其他数据结构，比如散列表、二叉树，就比较难实现了。 2. 实现2.1 最简单的二分查找123456789101112def binary_search(A, value): start = 0 end = len(A) - 1 while start &lt;= end: mid = start + ((end -start) &gt;&gt; 1) if A[mid] == value: return mid elif A[mid] &lt; value: start = mid + 1 else: end = mid -1 最简单的二分查找实现中有以下几个需要注意的点: 循环退出条件是 start &lt;= end 不是 start &lt; end 使用 mid=(low+high)/2 对mid取值是有问题的。因为如果 low 和 high 比较大的话，两者之和就有可能会溢出。改进的方法是将 mid 的计算方式写成 low+(high-low)/2。更进一步，如果要将性能优化到极致的话，我们可以将这里的除以 2 操作转化成位运算 low+((high-low)&gt;&gt;1)。因为相比除法运算来说，计算机处理位运算要快得多。 start 和 end 的更新，如果直接写成 low=mid 或者 high=mid，就可能会发生死循环。比如，当 high=3，low=3 时，如果 a[3] 不等于 value，就会导致死循环。 2.2 查找第一个值等于给定值的元素12345678910111213def bs_first(A, value): start = 0 n = end = len(A) - 1 while start &lt;= end: mid = start + ((end - start) &gt;&gt; 1) if A[mid] &lt; value: start = mid + 1 elif A[mid] &gt; value: end = mid - 1 else: # 判断当前 mid 是否为第一个出现的值 if mid == 0 or (A[mid-1] != value): return mid end = mid - 1 2.3 查找最后一个值等于给定值的元素12345678910111213def bs_end(A, value): start = 0 n = end = len(A) - 1 while start &lt;= end: mid = start + ((end - start) &gt;&gt; 1) if A[mid] &lt; value: start = mid + 1 elif A[mid] &gt; value: end = mid - 1 else: # 判断当前 mid 是否为最后一个出现的值 if mid == n or (A[mid + 1] != value): return mid end = mid + 1 2.4 查找第一个大于等于给定值的元素1234567891011def bs_gte_first(A, value): start = 0 n = end = len(A) - 1 while start &lt;= end: mid = start + ((end - start) &gt;&gt; 1) if A[mid] &gt;= value: # 判断是否为第一个 if mid == 0 or (A[mid - 1] &lt; value): return mid end = mid - 1 else: start = mid + 1 2.5 查找最后一个小于等于给定值的元素1234567891011def bs_lte_end(A, value): start = 0 n = end = len(A) - 1 while start &lt;= end: mid = start + ((end - start) &gt;&gt; 1) if A[mid] &gt; value: end = mid -1 else: # 判断是否为最后一个 if mid == n or (A[mid + 1] &gt; value): return mid start = mid + 1 参考: 王争老师专栏-数据结构与算法之美 《数据结构与算法：python语言实现》","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"10 工业级的排序算法","slug":"alog/sort_4","date":"2018-10-17T16:00:00.000Z","updated":"2019-04-14T01:55:16.580Z","comments":true,"path":"2018/10/18/alog/sort_4/","link":"","permalink":"http://yoursite.com/2018/10/18/alog/sort_4/","excerpt":"实现一个通用的，高效的工业级排序函数","text":"实现一个通用的，高效的工业级排序函数 1. 排序算法对比前面我们介绍了最常见最经典的几个排序算法，它们有不同的时间复杂度，空间复杂度与使用情景。那么如何用它们实现一个通用的、高效率的排序函数呢？ 线性排序算法的时间复杂度比较低，但适用场景太过比较特殊，所以几乎不会使用。 为了兼顾任意规模数据的排序，一般都会首选时间复杂度是 O(nlogn) 的排序算法来实现排序函数。比如 Java 语言采用堆排序实现排序函数，C 语言使用快速排序实现排序函数。 归并排序由于不是原地排序算法，空间复杂度为 O(n)，数剧集大时过于占用内存，所以很少使用。 1.2 快排优化快速排序在最坏情况下的时间复杂度是 O(n2)，原因主要是我们的分区点选择不够合理。有两种比较常用合理的分区算法: 三数取中法: 每间隔某个固定的长度，取数据出来比较，将中间值作为分区点 随机法: 从要排序的区间中，随机选择一个元素作为分区点 此外快速排序是用递归来实现的，递归要警惕堆栈溢出。为了避免快速排序里，递归过深而堆栈过小，导致堆栈溢出，我们有两种解决办法：第一种是限制递归深度。一旦递归过深，超过了我们事先设定的阈值，就停止递归。第二种是通过在堆上模拟实现一个函数调用栈，手动模拟递归压栈、出栈的过程，这样就没有了系统栈大小的限制。 2. 实现2.1 Glibc 的 qsort我们以 Glibc 中的 qsort() 函数为例来说明如何实现一个排序函数: qsort() 会优先使用归并排序来排序输入数据，因为小数据集下，归并排序不会占用多少内存，且排序快 要排序的数据量比较大的时候，qsort() 会改为用快速排序算法来排序 qsort() 使用“三数取中法”选择分区点 qsort() 是通过自己实现一个堆上的栈，手动模拟递归来解决操作系统的堆栈溢出问题 在快速排序的过程中，当排序区间的元素个数小于等于 4 时，qsort() 就退化为插入排序；因为我们前面也讲过，在小规模数据面前，插入排序比递归调用的快排更快 在 qsort() 插入排序的算法实现中，还利用了哨兵技术，来减少判断的次数 2.2 Tim-Sort1pass 参考: 王争老师专栏-数据结构与算法之美 《数据结构与算法：python语言实现》","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"09 线性排序","slug":"alog/sort_3","date":"2018-10-16T16:00:00.000Z","updated":"2019-04-14T01:55:16.579Z","comments":true,"path":"2018/10/17/alog/sort_3/","link":"","permalink":"http://yoursite.com/2018/10/17/alog/sort_3/","excerpt":"非基于比较的三个排序算法: 桶排序，计数排序，基数排序","text":"非基于比较的三个排序算法: 桶排序，计数排序，基数排序 1. 线性排序桶排序、计数排序、基数排序的时间复杂度是线性的，所以我们把这类排序算法叫作线性排序（Linear sort）。之所以能做到线性的时间复杂度，主要原因是，这三个算法是非基于比较的排序算法，都不涉及元素之间的比较操作。 这几种排序算法理解起来都不难，时间、空间复杂度分析起来也很简单，但是对要排序的数据要求很苛刻，所以我们今天学习重点的是掌握这些排序算法的适用场景。 桶排序和计数排序的排序思想是可以对数剧集进行有限分类，都是针对范围不大的数据，将数据划分成不同的桶来实现排序，只不过二者桶的粒度不同。基数排序要求数据可以划分成高低位，位之间有递进关系。比较两个数，我们只需要比较高位，高位相同的再比较低位。而且每一位的数据范围不能太大，因为基数排序算法需要借助桶排序或者计数排序来完成每一个位的排序工作。 2. 实现2.1 桶排序桶排序的核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。 只要桶的数量接近数据的个数，并且数据在所有桶内分配均匀，桶排序的时间复杂度接近 O(n)。显然桶排序对要排序数据有苛刻的限制: 首先，要排序的数据需要很容易就能划分成 m 个桶，并且，桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序。 其次，数据在各个桶之间的分布是比较均匀的。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不平均，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为 O(nlogn) 的排序算法了。 1234567891011121314151617181920from collections import dequedef sort_bucket(S): \"\"\" :param S: [(v, item)] :return: \"\"\" m = max((i[0] for i in S)) bucket_map = [deque() for i in range(m + 1)] for i in S: bucket = bucket_map[i[0]] bucket.append(i) c = 0 for i in range(m + 1): b = bucket_map[i] while len(b) &gt; 0: S[c] = b.popleft() c += 1 桶排序比较适合用在外部排序中。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。 2.2 计数排序计数排序可视为桶排序的特殊情况，当要排序的 n 个数据，所处的范围并不大的时候，比如最大值是 k，我们就可以把数据划分成 k 个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。计数排序之所以叫作计数排序，是因为其特殊的通过“计数”进行排序的算法得名的。 1234567891011121314151617def sort_count(S): if len(S) &lt;= 1: return S m = max(S) bucket = [0] * (m + 1) for i in S: bucket[i] += 1 for i in range(1, m + 1): bucket[i] += bucket[i - 1] tmp = [0] * len(S) for i in S: bucket[i] -= 1 tmp[bucket[i]] = i for i in range(len(S)): S[i] = tmp[i] 计数排序只能用在数据范围不大的场景，如果数据范围 k 比要排序的数据 n 大很多，就不适合用排序了。而且，而且计数排序只能用在给非负整数得排序中，如果要排序的数据是其他类型的，要其在不改变相对大小的情况下，转化为非负整数。 2.3 基数排序基数排序的核心是可以将数据分割出独立的“位”来比较，然后按照从低位到高位依次排序，只要每次按位排序的算法是稳定的就可以得到正确的排序。 根据每一位来排序，我们可以用刚讲过的桶排序或者计数排序，它们的时间复杂度可以做到 O(n)。如果要排序的数据有 k 位，那我们就需要 k 次桶排序或者计数排序，总的时间复杂度是 O(k*n)。当 k 不大的时候基数排序的时间复杂度就近似于 O(n)。 显然基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到 O(n) 了 123456789101112131415def sort_base(S, num): if len(S) &lt;= 1: return S T = [(0, i) for i in S] for n in range(num): T = [(i[1] // 10 ** n % 10, i[1]) for i in T] sort_bucket(T) print T for i in range(len(S)): S[i] = T[i][1]aa = [43, 41, 31, 57]sort_base(aa, 2)print aa 参考: 王争老师专栏-数据结构与算法之美 《数据结构与算法：python语言实现》","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"08 基于比较的排序(下)","slug":"alog/sort_2","date":"2018-10-15T16:00:00.000Z","updated":"2019-04-14T01:55:16.579Z","comments":true,"path":"2018/10/16/alog/sort_2/","link":"","permalink":"http://yoursite.com/2018/10/16/alog/sort_2/","excerpt":"基于分治编程思想的归并排序和快速排序","text":"基于分治编程思想的归并排序和快速排序 1. 分治前面讲到的三种排序算法，平均时间复杂度都是 O(n2)，只是适合规模较小的数剧集，接下来要讲的归并排序和快速排序，平均时间复杂度都是 O(nlogn)，它们都用到了分治思想。 分治，顾名思义，就是分而治之，将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了。分治与我们前面提到的递归很像，分治算法一般都是通过递归实现的。 虽然快排和归并排序都采用了分治的思想，但是它们完全不一样。归并排序的处理过程是由下到上的，先处理子问题，然后再合并。而快排正好相反，快排的处理过程是由上到下的，先分区，然后再处理子问题。归并排序虽然是稳定的但是它是非原地排序算法。快速排序通过设计巧妙的原地分区函数，可以实现原地排序，解决了归并排序占用太多内存的问题。正因为此，归并排序没有快排应用广泛。 2. 实现2.1 归并排序归并排序的核心是将数组从中间分成前后两个部分，然后对前后两个部分分别排序，再将它们合并起来。 1234567891011121314151617181920def merge(a, b, c): i = j = 0 while i + j &lt; len(c): if i == len(a) or (j &lt; len(b) and a[i] &gt; b[j]): c[i + j] = b[j] j += 1 else: c[i + j] = a[i] i += 1def sort_merge(alist): if len(alist) &lt;= 1: return alist mid = len(alist) // 2 left = alist[:mid] right = alist[mid:] sort_merge(left) sort_merge(right) merge(left, right, alist) 归并排序并不是原地排序算法，原因很简单 merge 函数在合并两个已排序数组时使用了额外的存储空间，其空间复杂度为 O(n)。最好最坏和平均时间复杂度都是 O(nlogn)，在整个比较过程并没有发生数据交换，只要 merge 函数保持元素的相对顺序，归并排序是稳定的排序算法。 2.2 快速排序快排的算法描述快排排序由以下 3 个步骤组成: 分解: 如果待排序列 S 有至少两个元素，从 S 中选择一个特定的元素 x 作为基准，将 S 中的元素分别放置在 3 个序列中: L 存储 S 中小于 x 的元素 E 存储 S 中等于 x 的元素 G 存储 S 中大于 x 的元素 递归: 递归的排序序列 L 和 G 合并: 按照 L,E,G 的顺序将元素放回 S 中 123456789101112131415161718192021222324252627def sort_quick(S): n = len(S) if len(S) &lt;= 1: return x = S.first() # 基准 x L = LinkedQueue() E = LinkedQueue() G = LinkedQueue() # 分解 while not S.empty(): if S.first() &lt; x: L.enqueue(S.dequeue()) elif S.first() &gt; x: G.enqueue(S.dequeue()) else: E.enqueue(S.dequeue()) # 递归 sort_quick(L) sort_quick(G) # 合并 while not L.is_empty(): S.enqueue(L.dequeue()) while not E.is_empty(): S.enqueue(E.dequeue()) while not G.is_empty(): S.enqueue(G.dequeue()) 快排的原地排序快排的原地排序的核心是选择数组中的一个数据项作为分区点 x，然后遍历数组通过数据交换，使得 x 左边的数据都小于 x，x 右边的数据都大于 x。x 将数组分成了两个区间，然后对这两个区间递归执行此过程直至区间长度为 1 ，完成排序。 1234567891011121314151617def sort_quick(alist, left, right): if left &gt;= right: return alist l = left + 1 r = right x = alist[left] while l &lt;= r: while l &lt;= r and alist[l] &lt; x: l += 1 while l &lt;= r and alist[r] &gt; x: r -= 1 if l &lt;= r: alist[l], alist[r] = alist[r], alist[l] alist[left], alist[r] = alist[r], alist[left] sort_quick(alist, left, r - 1) sort_quick(alist, r + 1, right) 显然这个过程发生了数据交换，但是并没有使用额外的存储空间，所以快排并不是稳定的排序算法，但是原地排序算法。 快排的最好和平均时间复杂度都是O(nlogn)，但是极端情况下，如果数组本身是有序的，并且我们选择最大或者最小(两端)的数据作为分区点，我们需要大约 n 次分区才能完成排序过程。快排的时间复杂度就会退化为 O(n2)。但是退化到 O(n2) 的概率非常小，我们可以通过合理的选择分区点来避免这种情况。 3. 算法3.1 求无序数组中的第 K 大元素利用快排的分区思想，我们可以在O(n) 时间复杂度内求无序数组中的第 K 大元素。 我们选择数组区间 A[0…n-1] 的最后一个元素 A[n-1] 作为 pivot，对数组 A[0…n-1] 原地分区，这样数组就分成了三部分，A[0…p-1]、A[p]、A[p+1…n-1]。如果 p+1=K，那 A[p] 就是要求解的元素；如果 K&gt;p+1, 说明第 K 大元素出现在 A[p+1…n-1] 区间，我们再按照上面的思路递归地在 A[p+1…n-1] 这个区间内查找。同理，如果 K&lt;p+1，那我们就在 A[0…p-1] 区间查找。 12345678910111213141516171819def quick_select(S, left, right, k): r = right l = left + 1 pivot = S[left] while l &lt;= r: while l &lt;= r and S[l] &lt;= pivot: l += 1 while l &lt;= r and S[r] &gt;= pivot: r -= 1 if l &lt;= r: S[l], S[r] = S[r], S[l] S[left], S[r] = S[r], S[left] if r + 1 == k: return S[r] elif r + 1 &gt; k: return quick_select(S, left, r - 1, k) else: return quick_select(S, r + 1, right, k) 参考: 王争老师专栏-数据结构与算法之美 《数据结构与算法：python语言实现》","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"07 基于比较的排序(上)","slug":"alog/sort_1","date":"2018-10-13T16:00:00.000Z","updated":"2019-04-14T01:55:16.579Z","comments":true,"path":"2018/10/14/alog/sort_1/","link":"","permalink":"http://yoursite.com/2018/10/14/alog/sort_1/","excerpt":"基于比较的排序算法: 冒泡排序、插入排序和选择排序","text":"基于比较的排序算法: 冒泡排序、插入排序和选择排序 1. 排序算法要点排序算法太多了，因此除了要学习各种算法的原理，代码实现之外；我们还要搞明白如何比较和分析一个排序算法。评价一个算法可以从如下几个方面入手: 算法的执行效率: 算法的执行效率包括最好情况、最坏情况、平均情况的时间复杂度；之所以要区分这三种情况，是因为大多数排序算法在不同有序度的数据集上的时间复杂度不相同 时间复杂度的系数、常数 、低阶；通常我们要排序的数剧集并不大，因此需要将系数、常数 、低阶考虑进来 比较次数和交换（或移动）次数，基于比较的排序算法需要进行数据的比较和移动两个操作，因此需要把这两个操作考虑进来 排序算法的内存消耗，排序算法中，我们将空间复杂度为O(1) 的算法称为原地排序算法 排序算法的稳定性，稳定性指如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。 这些方面可以帮助我们分析和更好的理解每种算法的特点。 1.1 排序算法分类排序算法太多了，最经典的、最常用的包括：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序。如上图所示按照时间复杂度它们分成了三类。 本节我们先来介绍基于比较的排序算法:冒泡排序、插入排序和选择排序。 这三种时间复杂度为 O(n2) 的排序算法中，冒泡排序、选择排序，可能就纯粹停留在理论的层面了，学习的目的也只是为了开拓思维，实际开发中应用并不多，但是插入排序还是挺有用的。后面讲排序优化的时候，我会讲到，有些编程语言中的排序函数的实现原理会用到插入排序算法。 1.2 有序度有序度是数组中具有有序关系的元素对的个数，逆序度恰恰相反，完全有序的数组的有序度叫作满有序度。逆序度的定义正好跟有序度相反显然 逆序度 = 满有序度 - 有序度。排序的过程就是一种增加有序度，减少逆序度的过程，最后达到满有序度，排序完成。 在下面的冒泡和插入排序的实现中，我们会发现无论是插入排序还是选择排序，每次数据交换只会增加 1 个有序度(因为它们只会对相邻的元素进行交换)。因此这两种排序的数据交换次数是相同的都是数剧集的逆序度。但是冒泡排序的实现更复杂需要更多次的赋值操作。所以如果将时间复杂度的系数、常数 、低阶考虑进来，冒泡排序并没有插入排序快。 2. 实现2.1 冒泡排序冒泡排序的核心是每次只会操作相临的两个数据，比较它们的大小，并在不满足大小关系时交换；在 n 次操作之后将最大或者最小值移动到最前端。 总共经过 n 次冒泡之后，排序即可完成。冒泡过程还可以优化。当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，不用再继续执行后续的冒泡操作。 冒泡是原地排序算法，只要我们在代码中不交换相等的元素，冒泡排序就是稳定的排序算法 12345def sort_bubble(alist): for end in range(len(alist), 1, -1): for i in range(1, end): if alist[i] &lt; alist[i-1]: alist[i], alist[i-1] = alist[i-1], alist[i] 2.2 插入排序插入排序将数据分为已排序和未排序两个区间，然后不断取未排序区间中的元素插入到已排序区间中，并保持已排序区间的有序；重复直至未排序区间为空即可。 插入排序是原地排序算法，在插入排序中，对于值相等的元素，我们只要将后出现的元素插入到后出现元素的后面，插入排序就是稳定的排序算法。 12345678def sort_insert(alist): for end in range(1, len(alist)): tmp = alist[end] p = end - 1 while p &gt;= 0 and alist[p] &gt; tmp: alist[p + 1] = alist[p] p -= 1 alist[p + 1] = tmp 2.3 选择排序选择排序将数据分为已排序和未排序两个区间，不同的是选择排序每次会从未排序区间找出最小的元素放到已排序区间的末尾，而不是插入。 选择排序是原地排序算法，最好最坏和平均时间复杂度都是O(n2)，但是选择排序并不是稳定的排序算法。原因是选择排序每次都要找剩余未排序元素中的最小值，并和前面的元素交换位置，这样就破坏了稳定性。 1234567def sort_select(alist): for start in range(0, len(alist)): m = start for i in range(start + 1, len(alist)): if alist[i] &lt; alist[m]: m = i alist[start], alist[m] = alist[m], alist[start] 参考: 王争老师专栏-数据结构与算法之美 《数据结构与算法：python语言实现》","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"06 递归","slug":"alog/recursion","date":"2018-10-12T16:00:00.000Z","updated":"2019-04-14T01:55:16.577Z","comments":true,"path":"2018/10/13/alog/recursion/","link":"","permalink":"http://yoursite.com/2018/10/13/alog/recursion/","excerpt":"递归是一种应用非常广泛的算法（或者编程技巧）。很多数据结构和算法的编码实现都要用到递归，比如 DFS 深度优先搜索、前中后序二叉树遍历等等。所以，搞懂递归非常重要。","text":"递归是一种应用非常广泛的算法（或者编程技巧）。很多数据结构和算法的编码实现都要用到递归，比如 DFS 深度优先搜索、前中后序二叉树遍历等等。所以，搞懂递归非常重要。 1 特性基本上，所有的递归问题都可以用递推公式来表示。要想使用递归解决问题，必需满足三个前提条件: 一个问题的解可以分解为几个子问题的解，子问题就是规模更小的问题 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样 存在递归终止条件 1.1 如何写递归代码写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再找出终止条件，最后将递推公式和终止条件翻译成代码。 编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。 1.2 递归存在的问题使用递归时会存在很多问题，最常见的两个是: 递归代码要警惕堆栈溢出 递归代码要警惕重复计算 在时间效率上，递归代码里多了很多函数调用，当这些函数调用的数量较大时，就会积聚成一个可观的时间成本。在空间复杂度上，因为递归调用一次就会在内存栈中保存一次现场数据，所以在分析递归代码空间复杂度时，需要额外考虑这部分的开销。 1.3 应用递归有利有弊，利是递归代码的表达力很强，写起来非常简洁；而弊就是空间复杂度高、有堆栈溢出的风险、存在重复计算、过多的函数调用会耗时较多等问题。所以，在开发过程中，我们要根据实际情况来选择是否需要用递归的方式来实现。 参考: 王争老师专栏-数据结构与算法之美","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"05 队列","slug":"alog/queue","date":"2018-10-11T16:00:00.000Z","updated":"2019-04-14T01:55:16.577Z","comments":true,"path":"2018/10/12/alog/queue/","link":"","permalink":"http://yoursite.com/2018/10/12/alog/queue/","excerpt":"先进者先出，这就是典型的”队列”。","text":"先进者先出，这就是典型的”队列”。 1. 特性我们知道，栈只支持两个基本操作：入栈 push()，出栈 pop()，队列与栈类似基本操作只有两个: 入队 enqueue() 向对尾添加一个数据，出队 dequeue() 从队列头部取出一个数据。 1.1 应用队列的应用非常广泛，特别是一些具有某些额外特性的队列，比如循环队列、阻塞队列、并发队列。它们在很多偏底层系统、框架、中间件的开发中，起着关键性的作用。比如高性能队列 Disruptor、Linux 环形缓存，都用到了循环并发队列；Java concurrent 并发包利用 ArrayBlockingQueue 来实现公平锁等。 队列可以应用在任何有限资源池中，用于排队请求。对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队。 1.2 阻塞队列阻塞队列其实就是在队列基础上增加了阻塞操作。它有两个显著特征: 队列空时，从队头取数据会被阻塞，直到队列中有了数据才能返回 队列满时，向队尾插入数据会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回我们可以使用阻塞队列轻松实现一个“生产者，消费者模型”。 1.3 并发队列线程安全的队列我们叫作并发队列，最简单直接的实现方式是直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。 实际上，基于数组的循环队列，利用 CAS 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。 1.4 双端对列双端对列是一种类对列数据结构，支持在对列的头部和尾部都能进行插入和删除操作。Python 中的 collections.deque 就是一个双端对列的实现。 2. 实现跟栈一样，队列可以用数组来实现，也可以用链表来实现。用数组实现的队列叫作顺序队列，用链表实现的队列叫作链式队列。 基于链表的实现方式，可以实现一个支持无限排队的无界队列（unbounded queue），但是可能会导致过多的请求排队等待，请求处理的响应时间过长。所以，针对响应时间比较敏感的系统，基于链表实现的无限排队的线程池是不合适的。 而基于数组实现的有界队列（bounded queue），队列的大小有限，所以线程池中排队的请求超过队列大小时，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理。不过，设置一个合理的队列大小，也是非常有讲究的。队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源、发挥最大性能。 2.1 顺序队列顺序队列的实现需要两个指针，head 指针指向队列头部，tail 指针指向队列尾部，即下一个入队元素将被保存的位置索引。显然随着不断的入队，出队 tail 指针出超过数组的索引范围，此时即便数组还有空闲位置也无法继续添加数据。此时借鉴在数组删除中介绍的方法，如果没有空间，我们只需要在下一次入队时集中触发以此数据移动操作，队列中的数据集中移动数组最前方即可。另一种解决方案则是使用循环队列。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class ArrayQueue(object): __CAPACITY__ = 10 def __init__(self): self.head = self.tail = 0 self._capacity = self.__CAPACITY__ self._buf = [0] * self._capacity def __len__(self): return self.tail - self.head def is_empty(self): return len(self) == 0 def is_end(self): return self.tail == self._capacity def is_full(self): return len(self) == self._capacity def enqueue(self, value): if self.is_full(): self._resize(self._capacity * 2) elif self.is_end(): self._move() self._buf[self.tail] = value self.tail += 1 def dequeue(self): if self.is_empty(): raise ValueError('queue is empty') h = self._buf[self.head] self._buf[self.head] = 0 self.head += 1 if len(self) &lt; self._capacity / 4: self._resize(self._capacity / 2) return h def _resize(self, size): buf = [0] * size base = len(self) for i in range(base): vi = self.head + i buf[i] = self._buf[vi] self._capacity = size self._buf = buf self.head = 0 self.tail = base def _move(self): i = 0 base = len(self) while i &lt; base: self._buf[i] = self._buf[self.head + i] i ++ self.head = 0 self.tail = base 2.2 链式队列基于单链表的队列，我们在链表那一章已经包含在单链表的实现中。这里我们就基于双链表来实现一个队列，也把之前遗留的循环链表的实现补上。 12345678910111213141516171819202122232425262728293031323334353637383940class LinkedCircularQueue(object): class _Node(object): __slots__ = \"_element\", \"_next\", \"_pre\" def __init__(self, element, nxt=None): self._element = element self._next = nxt def __init__(self): self._tail = None self._size = 0 def __len__(self): return self._size def is_empty(self): return self._size == 0 def dequeue(self): if self.is_empty(): raise ValueError('queue is empty') self._size -= 1 node = self._tail._next if self.is_empty(): self._tail = None else: self._tail._next = node._next node._next = None value = node.element return value def enqueue(self, value): node = self._Node(value) if self.is_empty(): node._next = node else: node._next = self._tail._next self._tail._next = node self._tail = node self._size += 1 2.3 循环对列循环对列与顺序队列类似，但是通过循环利用底层的数组有效的避免了数据移动。循环队列实现相比顺序队列更复杂，有以下几点需要注意: 追加到队尾的元素的位置不在是 tail 而是要判断 tail 是否大于 n，如果大于插入位置则为 tail % n，同时更新 tail 应该更新为 tail % n + 1 队空的条件仍然是 tail == head 但是队列满的条件则为 (tail + 1) % n == head 下面是另外一种类似的实现方式，记录头节点和队列中的元素个数，而不是尾节点，个人觉得这种实现方式更加直观。 1234567891011121314151617181920212223242526272829303132333435363738394041424344class ArrayCircularQueue(object): __CAPACITY__ = 10 def __init__(self): self._head = 0 self._size = 0 self._capacity = self.__CAPACITY__ self._buf = [0] * self._capacity def __len__(self): return self._size def is_empty(self): return self._size == 0 def is_full(self): return self._size == len(self._buf) def enqueue(self, value): if self.is_full(): self._resize(self._capacity * 2) vi = (self._head + self._size) % self._capacity self._buf[vi] = value self._size += 1 def dequeue(self): if self.is_empty(): raise ValueError('queue is empty') value = self._buf[self._head] self._buf[self._head] = 0 self._head = (self._head + 1) % self._capacity if self._size &lt; self._capacity / 4: self._resize(self._capacity / 2) self._size -= 1 return value def _resize(self, size): buf = [0] * size for i in range(self._size): vi = (i + self._head) % self._capacity buf[i] = self._buf[vi] self._capacity = size self._buf = buf self._head = 0 2.4 双端对列链表那一章，我们实现了一个双向链表，在此基础上我们来实现一个双端队列。前面对双向链表的抽象实现是非常重要的，因为我们后面很多数据结构都是建立在双向链表的基础上。 12345678910111213141516171819202122from double_link import DoubleLinkclass LinkedDeque(DoubleLink): def __init__(self): super(LinkedDeque, self).__init__() def insert_first(self, value): self.insert_between(value, self._head, self._head._next) def insert_last(self, value): self.insert_between(value, self._tail._pre, self._tail) def delete_first(self): if self.is_empty(): raise ValueError('Deque is empty') self.delete_node(self._head._next) def delete_last(self): if self.is_empty(): raise ValueError('Deque is empty') self.delete_node(self._tail._pre) 2.5 并发对列一个基于 CAS 的无锁并发队列实现起来是很复杂的，我们暂时先把这个放一放，在这个系列的结尾会用专门的一篇文章来讲解实现。 参考: 王争老师专栏-数据结构与算法之美 《数据结构与算法：python语言实现》","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"04 栈","slug":"alog/stack","date":"2018-10-10T16:00:00.000Z","updated":"2019-04-14T01:55:16.580Z","comments":true,"path":"2018/10/11/alog/stack/","link":"","permalink":"http://yoursite.com/2018/10/11/alog/stack/","excerpt":"后进者先出，先进者后出，这就是典型的“栈”结构。","text":"后进者先出，先进者后出，这就是典型的“栈”结构。 1. 特性栈是一种“操作受限”的线性表，只允许在一端插入和删除数据。主要包含两个操作，入栈和出栈，也就是在栈顶插入一个数据和从栈顶删除一个数据。 2. 实现栈既可以用数组来实现，也可以用链表来实现。用数组实现的栈，我们叫作顺序栈，用链表实现的栈，我们叫作链式栈。如果要实现一个支持动态扩容的栈，我们只需要底层依赖一个支持动态扩容的数组就可以了。当栈满了之后，我们就申请一个更大的数组，将原来的数据搬移到新数组中。 2.1 顺序栈顺序栈依赖一个能自动扩缩容的数组容器，我们可以像数组章节一样，自己实现一个数组容器，也可以直接使用 Python 内置的 list。list 的接口已经包含并大大超过了栈的可操作范围，这里我们采用一种称为”适配器模式”的通用方法，将栈操作委托给一个内部的 list 实例，来实现一个顺序栈。 这个实现起来很简单，就不写的过于复杂了。 1234567891011121314class ArrayStack(object): def __init__(self): self._buf = [] def __len__(self): return len(self._buf) def pop(self): if len(self._buf) &lt; 1: raise ValueError('stack is empty') return self._buf.pop() def push(self, value): self._buf.append(value) 2.2 链式栈在链表的头部插入和删除一个节点的时间复杂度都是 O(1)，因此我们很容易的就可以将链表的头部作为栈顶实现一个链式栈，并且我们的都不管链表的尾，链表只要维护一个指向头节点指针和自身大小的计数即可。 注意不要将链表的尾作为栈顶，虽然可以实现 O(1) 向链尾插入节点，但是删除尾节点需要遍历整个链表。在链表章节，我们已经实现了一个”超纲的”链式栈，这里就不再累述了。 3. 相关算法操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构, 用来存储函数调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。这种栈被称为函数调用栈。除此之外诸如表达式求值，括号匹配以及实现浏览器的前进后退功能都与栈有关。 3.1 表达式求值对一个类似于 3-(1/4+7)*3 中缀表达式进行求值分成两步: 将中缀表达式转换为后缀表达式 对后缀表达式进行求值 这两步都用到了栈。为了简单起见，我们只处理+ - * / () 四种运算 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667from stack import ArrayStackfrom operator import add, div, mul, subop_priority = &#123; '(': 1, '+': 2, '-': 2, '*': 3, '/': 3, ')': 4&#125;op_func = &#123; '+': add, '-': sub, '*': mul, '/': div&#125;def infix_to_postfix(expression): s = ArrayStack() r = [] expression = expression.split(' ') for e in expression: if e not in op_priority: r.append(e) elif e == '(': s.push(e) elif e == ')': if s.top() != '(': r.append(s.pop()) s.pop() else: while len(s) &gt; 0: t = s.top() if op_priority[t] &gt;= op_priority[e]: r.append(s.pop()) else: break s.push(e) while len(s) &gt; 0: r.append(s.pop()) return ''.join(r)def calculate_postfix(expression): s = ArrayStack() expression = expression.split(' ') for e in expression: if e not in op_priority: s.push(e) else: right = float(s.pop()) left = float(s.pop()) # print left, right value = op_func[e](left, right) s.push(value) return s.pop()def main(): print infix_to_postfix('( A + B ) * ( C + D )') print infix_to_postfix('A + B * C') print calculate_postfix('7 8 + 3 2 + /') print infix_to_postfix('6 / 3') print calculate_postfix('15 3 /') 3.2 括号匹配括号匹配有两个类似的问题，一个是类似于对形如 (1 + 2) + (10) 表达式检测括号是否成对出现；另一个更加常用的是检测 HTML 标签是否完整匹配。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556from stack import ArrayStack# 括号匹配def per_check(expression): left = '(&#123;[' right = ')&#125;]' s = ArrayStack() expression = expression.replace(' ', '') for e in expression: if e in left: s.push(left.index(e)) elif e in right: i = right.index(e) if len(s) &lt;=0: return False elif i != s.pop(): return False if len(s) &gt; 0: return False return True# html 标签匹配def html_match(html_string): start = 0 s = ArrayStack() while start != -1: start = html_string.find('&lt;', start) if start == -1: break end = html_string.find('&gt;', start + 1) tag = html_string[start + 1: end] print tag if tag.startswith('/'): if len(s) &lt;= 0: return False else: top = s.pop() # print top, tag[1:] if top != tag[1:]: return False else: s.push(tag) start = end if len(s) &gt; 0: return False return Truedef main(): # print per_check('&#123;&#123;([][])&#125;()&#125;') # print per_check('()]') print html_match('&lt;a&gt;&lt;/a&gt;')if __name__ == \"__main__\": main() 3.3 浏览器的前进后退功能12345678910111213141516171819202122232425262728from stack import ArrayStackclass Browser(object): def __init__(self): self._back = ArrayStack() self._forward = ArrayStack() def back(self): \"\"\" :return: 后退 \"\"\" if len(self._back) &gt; 0: self._forward.push(self._back.pop()) def forward(self): \"\"\" :return: 前进 \"\"\" if len(self._forward) &gt; 0: self._back.push(self._forward.pop()) def new_click(self): \"\"\" :return: 打开新连接 \"\"\" while len(self._forward) &gt; 0: self._forward.pop() 4. linkcode 习题参考: 王争老师专栏-数据结构与算法之美 《数据结构与算法：python语言实现》","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"03 链表","slug":"alog/linkedlist","date":"2018-10-09T16:00:00.000Z","updated":"2019-04-14T01:55:16.576Z","comments":true,"path":"2018/10/10/alog/linkedlist/","link":"","permalink":"http://yoursite.com/2018/10/10/alog/linkedlist/","excerpt":"相比于数组必需使用连续的内存空间，链表通过“指针”将一组零散的内存块串联起来使用，因此更加灵活。","text":"相比于数组必需使用连续的内存空间，链表通过“指针”将一组零散的内存块串联起来使用，因此更加灵活。 1. 特性我们知道数组受限于保持数据的连续，插入和删除都需要移动大量的数据，而在链表中插入或者删除一个数据，我们并不需要为了保持内存的连续性而搬移结点，因为链表的存储空间本身就不是连续的。所以，在链表中插入和删除一个数据是非常快速的。 但是，有利就有弊。链表要想随机访问第 k 个元素，就没有数组那么高效了。因为链表中的数据并非连续存储的，所以无法像数组那样，根据首地址和下标，通过寻址公式就能直接计算出对应的内存地址，而是需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点。 1.1 性能比较由于数组和链表是两种截然不同的内存组织方式。正是因为内存存储的区别，它们插入、删除、随机访问操作的时间复杂度正好相反。不过，数组和链表的对比，并不能局限于时间复杂度。而且，在实际的软件开发中，不能仅仅利用复杂度分析就决定使用哪个数据结构来存储数据。 数组简单易用，在实现上使用的是连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对 CPU 缓存不友好，没办法有效预读。 数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足（out of memory）”。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然地支持动态扩容，我觉得这也是它与数组最大的区别。 虽然常见的数组容器都支持动态扩容，但是当需要申请更大的内容容纳更多的数据，数据的拷贝操作是非常耗时的。 除此之外，如果你的代码对内存的使用非常苛刻，那数组就更适合你。因为链表中的每个结点都需要消耗额外的存储空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍。而且，对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片，如果是 Java 语言，就有可能会导致频繁的 GC（Garbage Collection，垃圾回收）。 2. 实现链表有多种结构，常见的有单链表，双链表，循环链表。接下来我们就来介绍并实现这三种常见的链表。 2.1 单链表 单链表之所以叫”单”链表，是因为它的每个节点只保存了指向下一个节点的指针，而没有保存指向它自己的前一个节点的指针。因此在插入节点时，我们必需先获取插入位置之前的节点。 为了方便后续用链表去实现栈和队列，我们在单链表中显示的维护一个 _head 和_tail 的指针用于指向链表的首尾节点，并实现下面三个方法: 在链表的头部插入一个节点 删除链表的头节点 在链表尾部添加一个节点 因为我们必需遍历整个链表才能获取尾节点的前一个节点，所以很难高效的从单链表的尾部删除元素，所以我们不会实现一个删除尾节点的方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172class Link(object): class _Node(object): __slots__ = \"_next\", \"_element\" def __init__(self, element, nxt=None): self._next = nxt self._element = element def __str__(self): a = self._next b = [str(self._element)] while a: b.append(str(a._element)) a = a._next return '-&gt;'.join(b) def __init__(self): self._head = None self._tail = None self._size = 0 def __len__(self): return self._size def is_empty(self): return self._size == 0 def _insert_tail(self, element): \"\"\" :param element: :return: 链表尾部追加元素 \"\"\" node = self._Node(element) if self.is_empty(): self._head = self._tail = node else: self._tail._next = node self._tail = node self._size += 1 def _remove_head(self): \"\"\" :return: 链表首部删除元素 \"\"\" if self.is_empty(): raise ValueError('link is empt') answer = self._head._element self._head = self._head._next self._size -= 1 if self.is_empty(): self._tail = None return answer def _insert_head(self): \"\"\" :return: 链表首部添加元素 \"\"\" node = self._Node(element) if self.is_empty(): self._head = self._tail = node else: node._next = self._head self._head = node self._size += 1 # 栈方法 pop = _remove_head push = _insert_head # 堆方法 enqueue = _insert_tail dequeue = _remove_head 2.4 循环链表 循环链表跟单链表唯一的区别就在尾结点。单链表的尾结点指针指向空地址，表示这就是最后的结点了。而循环链表的尾结点指针是指向链表的头结点。 和单链表相比，循环链表的优点是从链尾到链头比较方便。当要处理的数据具有环型结构特点时，就特别适合采用循环链表。比如著名的约瑟夫问题 双向链表的实现与单向链表大体上相同，除了尾节点的特殊处理。因此，我们暂时忽略循环链表的实现，等到下一章我们使用一个循环链表来实现一个队列，再来展示循环链表的实现。 2.3 双链表 双向链表需要额外的两个空间来存储后继结点和前驱结点的地址。可以支持双向遍历，这样也带来了双向链表操作的灵活性。链表在插入和删除时必需先找到被操作节点的前驱节点，而单链表并不支持直接获取其前驱节点，必需从头开始重新遍历链表。而双向链表支持双向便利可直接获取，所以操作起来更加灵活。 除了插入、删除操作有优势之外，对于一个有序链表，双向链表的按值查询的效率也要比单链表高一些。因为，我们可以记录上次查找的位置 p，每次查询时，根据要查找的值与 p 的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据。 在双链表的实现中，我们将引入头哨兵和尾哨兵；使用哨兵可以帮助我们避免处理链表中没有节点时的特殊情况帮助我们简化双向链表的实现。这里可以与上面不使用哨兵的单向链表作对比。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class DoubleLink(object): class _Node(object): __slots__ = \"_element\", \"_next\", \"_pre\" def __init__(self, element, pre=None, nxt=None): self._element = element self._next = nxt self._pre = pre def __init__(self): self._head = self._Node(None) self._tail = self._Node(None) self._head._next = self._tail self._tail._pre = self._head self._size = 0 def __len__(self): return self._size def is_empty(self): return self._size == 0 def insert_between(self, element, pre_node, next_node): \"\"\" :param element: :param pre_node: :param next_node: :return: 在两个节点之间插入一个节点 \"\"\" node = self._Node(element, pre=pre_node, nxt=next_node) pre_node._next = node next_node._pre = node self._size += 1 return node def insert_head(self, element): return self.insert_between(element, self._head, self._head._next) def delete_node(self, node): \"\"\" :param node: :return: 删除节点 \"\"\" element = node._element node._pre._next = node._next node._next._pre = node._pre self._size -= 1 node._pre = node._next = node._element = None return element 3. 相关算法相比与数组，链表写起来就很复杂，有如下一些算法，可以帮助我们练习链表。为了简化代码实现，下面所有函数的参数 link 都是 下面 Node 类的实例 123456789101112class Node(object): def __init__(self, data=None, nxt=None): self.data = data self.nxt = nxt def __str__(self): a = self.nxt b = [str(self.data)] while a: b.append(str(a.data)) a = a.nxt return '-&gt;'.join(b) 3.1 单链表反转123456789def reverse(link): pre = None pwd = link while pwd: # print pre.data, pwd.data # pwd.nxt, pwd, pre = pre, pwd.nxt, pwd pwd.nxt, pre, pwd = pre, pwd, pwd.nxt # pre, pwd.nxt, pwd = pwd, pre, pwd.nxt return pre 3.2 链表中环的检测12345678def has_cycle(link): one = double = link while double.nxt and double.nxt.nxt: one = one.nxt double = double.nxt.nxt if one is double: return True return False 3.3 两个有序链表的合并123456789101112131415161718def merge(link_one, link_two): link = Node() a = link_one b = link_two c = link while a and b: if a.data &lt; b.data: c.nxt = a a = a.nxt else: c.nxt = b b = b.nxt c = c.nxt if a is not None: c.nxt = a if b is not None: c.nxt = b return link.nxt 3.4 删除链表倒数第 n 个节点12345678910111213141516171819def delete_last(link, n): if link is None: return link pre = None first = link second = link for i in range(1, n): second = second.nxt if second is None: return None while second.nxt: second = second.nxt pre = first first = first.nxt if pre is None: return first.nxt else: pre.nxt = first.nxt return link 3.5 求链表的中间节点123456789101112def get_middle(link): if link is None or link.nxt is None: return link, None one = link double = link while double.nxt and double.nxt.nxt: one = one.nxt double = double.nxt.nxt if double.nxt is None: return one, None else: return one, one.nxt 3.6 基于链表实现 LRU 缓存算法实现思路如下: 我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。 如果此数据没有在缓存链表中，又可以分为两种情况： 如果此时缓存未满，则将此结点直接插入到链表的头部； 如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。 实际上，我们可以继续优化这个实现思路，比如引入（Hash table）来记录每个数据的位置，将缓存访问的时间复杂度降到 O(1)，具体实现会在散列表与连表相关章节给出。 4. linkcode 习题参考: 王争老师专栏-数据结构与算法之美 《数据结构与算法：python语言实现》","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"02 数组","slug":"alog/array","date":"2018-10-07T16:00:00.000Z","updated":"2019-04-14T01:55:16.569Z","comments":true,"path":"2018/10/08/alog/array/","link":"","permalink":"http://yoursite.com/2018/10/08/alog/array/","excerpt":"数组和链表应该是数据结构与算法中最基础的数据结构。与链表相比，数组更加简单，所以相比于数组的实现和与之相关的算法，充分认识数组的内在特性反而更加重要。","text":"数组和链表应该是数据结构与算法中最基础的数据结构。与链表相比，数组更加简单，所以相比于数组的实现和与之相关的算法，充分认识数组的内在特性反而更加重要。 1. 特性数组（Array）是一种线性表数据结构，用一组连续的内存空间，来存储一组具有相同类型的数据。正是由于连续的内存空间和存储相同类型数据的特性，使得数组支持基于下标的“随机访问”。但是也正是为了维持这种连续的特性，使得数组的插入和删除操作必需作大量的数据移动，因为数组内不能”弯曲”也不能出现”空洞”。 1.1 插入如果数组是有序的，插入一个新的元素到第 k 位置则必需移动 k 之后的所有数据；但是如果数组中的数据本身是无序的，我们可以直接将第 k 位的数据移动到数组元素的最后，再把新的元素插入到原来的第 k 位以避免大量的数据移动。 1.2 删除数组的删除与插入类似，如果要删除第 k 位的元素，为了保证数组内的连续行，也需要移动大量的数据，不然数组就会出现空洞，内存就不连续了。如果数据内的数据是有序，则这种移动不可避免，如果是数组是无序的，可以直接用数组最后的元素覆盖第 k 位的元素。 实际上，在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。我们可以将多次删除操作集中在一起执行，来提高删除的效率。我们可以先记录下已经删除的数据。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。 1.3 动态扩容因为需要为数组分配连续的内存空间，因此数组在使用前就需要预先确定好大小。当需要向满的数组中插入数据时，我们就需要重新分配一块更大的空间，将原来的数据复制过去，然后再将新的数据插入。数组的插入，删除以及由此待来的动态扩容是非常基础的操作，因此大多数编程语言除了基本的底层数组之外，都提供了包含丰富接口的数组容器，方便程序员编程适用，比如 Python 中的列表(list)。 1.4 数组与数组容器的使用选择何时使用数组何时使用编程语言提供的数组容器，专栏-数据结构与算法之美给了下面的一些建议: 容器都有额外的性能损耗，如果特别关注性能，或者希望使用基本类型，就可以选用数组。 如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可以直接使用数组。 对于业务开发，直接使用容器就足够了，省时省力。毕竟损耗一丢丢性能，完全不会影响到系统整体的性能。但如果你是做一些非常底层的开发，比如开发网络框架，性能的优化需要做到极致，这个时候数组就会优于容器，成为首选。 2. 实现列表和元组是 Python 提供的数组容器，它们都是引用结构，即 list 内部的底层数组存储的不是元素本身，而是列表元素的内存地址，这些内存地址指向每个列表元素。 除了数组容器之外，array 和 ctypes 提供了创建原始底层数组(即保存的不是内存地址而是元素本身的原始数组)的方法。array 模块提供的 array 类只支持基于 C 语言的原始数据类型，不支持用户自定义的数据类型，自定义类型的底层数组由 ctypes 这个底层模块提供。 下面我们就以 ctypes 提供的底层数组为基础创建了一个类似 list 的数组容器。这里的实现并不完备，目的是为了展示 Python list 的底层实现。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import ctypesclass DynamicArray(object): def __init__(self): self._n = 0 # 列表当中实际存储的元素个数 self._capacity = 1 # 当前分配的底层数组，能存储的元素个数 self._buf = self._make_array(self._capacity) # 底层数组的引用 def __len__(self): return self._n def __getitem__(self, item): \"\"\" :param item: :return: 简单起见，只支持简单的正向索引 \"\"\" if 0 &lt;= item &lt; self._n: return self._buf[item] raise IndexError('%s out of range' % self.__class__.__name__) def append(self, value): if self._n == self._capacity: self._resize(size= 2 * self._capacity) self._buf[self._n] = value self._n += 1 def _resize(self, size): \"\"\" :param c: :return: 底层数组的动态扩容 \"\"\" buf = self._make_array(size) for i in xrange(self._n): buf[i] = self._buf[i] self._buf = buf self._capacity = size @staticmethod def _make_array(size): \"\"\"创建一个指定大小的底层数组\"\"\" return (size * ctypes.py_object)() def insert(self, k, value): if self._n == self._capacity: self._resize(2 * self._capacity) for i in xrange(self._n, k, -1): self._buf[i] = self._buf[i - 1] self._buf[k] = value self._n += 1 def remove(self, value): \"\"\" :param value: :return: 删除第一值等于 value 的元素 \"\"\" for i in xrange(self._n): if self._buf[i] == value: for j in xrange(i, self._n - 1): self._buf[j] = self._buf[j + 1] self._buf[self._n - 1] = None # 删除最后一个元素的引用，以便被回收 self._n -= 1 return raise ValueError('value not found') 3. 相关算法与数组专门相关的算法并不多，因为太底层了。这里我们介绍两个: 用数组实现的位图以及凯撒密码 3.1 位图123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import arrayimport numbersclass BitMap(object): def __init__(self): self._buf = array.array('L') # 'L' 表示 32 位无符号的整数 @staticmethod def __check(num): if not (isinstance(num, numbers.Integral) and num &gt;= 0): raise ValueError(\"num is not unsigned int\") return num / 32, num % 32 def __len__(self): return len(self._buf) def __getitem__(self, item): return self._buf[item] def __iter__(self): return iter(self._buf) def __contains__(self, item): i, b = self.__check(item) return i &lt; len(self._buf) and (self._buf[i] &amp; (1 &lt;&lt; b)) def __str__(self): r = [] # print self._buf for i in xrange(len(self._buf)): if self._buf[i]: for j in xrange(32): if self._buf[i] &amp; (1 &lt;&lt; j): r.append(32 * i + j) return str(r) def add(self, num): i, b = self.__check(num) while i &gt;= len(self._buf): self._buf.append(0) self._buf[i] |= (1 &lt;&lt; b) def union(self, bit_map): for i, v in enumerate(bit_map): if i &lt; len(self._buf): self._buf[i] |= v else: self._buf.append(v)def main(): bm = BitMap() bm.add(1) bm.add(144) bm.add(9) bm.add(9) print bm print 9 in bm print 8 in bm y = BitMap() y.add(9) y.add(42) print y bm.union(y) print bm 3.2 凯撒密码有关凯撒密码的说明，大家可以看看百科的说明:凯撒密码 12345678910111213141516171819202122class CaesarCipher(object): def __init__(self, shift): self.encode = [(chr(ord('A') + (i + shift) % 26)) for i in range(26)] self.decode = [(chr(ord('A') + (i - shift) % 26)) for i in range(26)] print self.encode print self.decode def encrypt(self, message): return self._transform(message, self.encode) def decrypt(self, message): return self._transform(message, self.decode) @staticmethod def _transform(message, code): m = list(message) r = [] for i in m: if i.isupper(): t = code[ord(i) - ord('A')] r.append(t) return ''.join(r) 4. linkcode 习题参考: 王争老师专栏-数据结构与算法之美 《数据结构与算法：python语言实现》","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"01 数据结构与算法学习开篇","slug":"alog/algo_start","date":"2018-10-06T16:00:00.000Z","updated":"2019-04-14T01:55:16.568Z","comments":true,"path":"2018/10/07/alog/algo_start/","link":"","permalink":"http://yoursite.com/2018/10/07/alog/algo_start/","excerpt":"数据结构与算法前前后后已经学习了很长时间，从一开始看《数据结构与算法分析:C语言描述》，到后来看 Python 相关实现《Problem Solving with Algorithms and Data Structures using Python》，但是一直感觉不得其法。","text":"数据结构与算法前前后后已经学习了很长时间，从一开始看《数据结构与算法分析:C语言描述》，到后来看 Python 相关实现《Problem Solving with Algorithms and Data Structures using Python》，但是一直感觉不得其法。 究其原因，一方面是自己对 C 语言一知半解，所以一开始练习太少；另一方面是工作中使用太少，知识这种东西只有到达一定的熟练程度才能真正发现其作用。现在有幸在极客时间订阅了王争老师的专栏数据结构与算法之美，看过之后感觉很好，相比与之前看的书籍有很好的发散和扩展，恰逢《数据结构与算法：python语言实现》也刚刚面市。所以决定下定决心在 2018 最后三个月好好重新学习数据结构与算法。因此就有了这个系列的博客，希望监督自己多加练习。 本系列博客会按照专栏数据结构与算法之美的结构组织，也会从中摘录部分内容，在此特地申明，也非常推荐大家订阅此专栏。然后会以《数据结构与算法：python语言实现》作为辅助来扩展内容，并且在每篇文章的最后，我会尽可能给出与当篇文章相关的 linkcode 习题。 当然作为自己的博客，目的不是复制别人的内容，是想对常用的数据结构作一个总结，然后督促自己动手实现这些数据结构和算法。最后想说一句，数据结构和算法看起来没用，我们平时大多数使用的都是语言内置好的容器和现成的函数，我们只需要知道它们的功能，无需关注它们的实现细节，我们也能写出我们的程序。但是程序的效率很大程度上依赖锁使用组件的效率，我们不关注这些细节，可能别人已经写好了，但是我们却没正确的使用。Python 就有以本书叫《Python: Faster Way》，如果我们对 Python 常见数据结构有所了解，其时很自然的就会明白它们上面所说明的用法。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"16 wrapt 模块实战","slug":"wrapt/python_decorator_16","date":"2018-06-06T16:00:00.000Z","updated":"2020-05-24T15:31:44.467Z","comments":true,"path":"2018/06/07/wrapt/python_decorator_16/","link":"","permalink":"http://yoursite.com/2018/06/07/wrapt/python_decorator_16/","excerpt":"装饰器和 wrapt 模块的介绍已经结束，作为整个系列的最后一篇的实战篇，我们来实现在一开始我提出的一个需求","text":"装饰器和 wrapt 模块的介绍已经结束，作为整个系列的最后一篇的实战篇，我们来实现在一开始我提出的一个需求 1. 应用场景在我日常的开发过程中，经常要查询各种数据库，比如 mysql, mongo，es 。基本上所有的数据库对查询语句能添加的查询条件都有限制。对于大批量的查询条件，只能分批多次查询，然后将查询结果合并。我们能不能将这种查询分批在合并的操作抽象出来实现为一个装饰器，在需要时对查询函数包装即可？下面是我的一个实现示例。 2. 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576#!/usr/bin/python# -*- coding: utf-8 -*-\"\"\"作用：用于优化的装饰器功能： 1. 实现分组迭代，分批查询的装饰器\"\"\"import osimport sysimport wraptimport inspectimport pandasdef get_slice(total_num, slice_num): \"\"\" :return: 等大小切片 \"\"\" r = [] n = total_num / slice_num m = total_num % slice_num end = 0 for i in range(1, n + 1): start = slice_num * (i - 1) end = slice_num * i r.append(slice(start, end)) else: if m &gt; 0: r.append(slice(end, end + m)) return rdef slice_call(iter_param, slice_num=500): @wrapt.decorator def wrapper(wrapped, instance, args, kwargs): # 函数自省 param = inspect.getcallargs(wrapped, *args, **kwargs) if instance: param.pop('self') if 'kwargs' in param: kwargs = param.pop('kwargs',&#123;&#125;) param.update(kwargs) iter_value = param.get(iter_param) if iter_value is None: return wrapped(**param) if isinstance(iter_value, pandas.DataFrame): iter_value.reset_index(drop=True, inplace=True) # 分批 total_num = len(iter_value) slice_iter = get_slice(total_num, slice_num) result = [] # 合并 for s in slice_iter: param[iter_param] = iter_value[s] result.append(wrapped(**param)) if result: return pandas.concat(result) else: return pandas.DataFrame() return wrapper# slice_call 使用示例@slice_call(iter_param='names')def get_video_by_name(self, names, c_type): where_name = \"'\" + \"','\".join(names) + \"'\" sql = ('select * from table' 'where a=\"%s\" and b in (%s) and c&gt;=0;' % (c_type, where_name)) print sql df = self.mysql_obj.query('', sql) df['updateTime'] = df['updateTime'].apply(lambda x: x.strftime(\"%Y-%m-%d\")) return df slice_call 函数在使用有一个限制条件，被包装函数的返回值必需是 pandas.DataFrame。因为在我日常的工作中，经常使用到 pandas 进行数据分析，对我来说，DataFrame 是一个非常通用的数据结构，因此就在此基础上构建了 slice_call 装饰器。整个实现中使用的额外知识就是函数的自省，由 inspect 模块提供，其他有关装饰器的部分都是前面博客介绍的内容，相信大家应该很容易就能看懂。 结语至此 Python 装饰器的内容就先到此为止，接下来想结合 wrapt, unittest, mock 来说一说如何在 Python 中作单元测试。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"wrapt","slug":"wrapt","permalink":"http://yoursite.com/tags/wrapt/"}]},{"title":"15 wrapt 模块使用","slug":"wrapt/python_decorator_15","date":"2018-06-05T16:00:00.000Z","updated":"2020-05-24T15:31:40.330Z","comments":true,"path":"2018/06/06/wrapt/python_decorator_15/","link":"","permalink":"http://yoursite.com/2018/06/06/wrapt/python_decorator_15/","excerpt":"GrahamDumpleton wrapt blog 的翻译部分到此就结束。很可惜的是作者并没有把猴子补丁部分写完，查阅了 wrapt 的官方文档，上面只介绍了 wrapt 的装饰器，代理对象以及 synchronized 同步装饰器，也没有介绍猴子补丁相关内容。不过已经介绍的内容足够用了，接下来我想结合 wrapt 的文档介绍一下 wrapt 模块的使用，算是整个博客的总结。","text":"GrahamDumpleton wrapt blog 的翻译部分到此就结束。很可惜的是作者并没有把猴子补丁部分写完，查阅了 wrapt 的官方文档，上面只介绍了 wrapt 的装饰器，代理对象以及 synchronized 同步装饰器，也没有介绍猴子补丁相关内容。不过已经介绍的内容足够用了，接下来我想结合 wrapt 的文档介绍一下 wrapt 模块的使用，算是整个博客的总结。 1. 前文回顾在阐述 wrapt 的使用之前，有必要对之前的内容做一个简单总结，因为 wrapt 模块的接口正是与之前的内容一一对应的。GrahamDumpleton 编码 wrapt 的本意是想实现为 Python 代码中添加猴子补丁，然而 Python 中装饰器，辅助测试的模拟库与猴子补丁的实现方式极其相似，因此 GrahamDumpleton 就按照如下的方式为将我们讲解了 wrapt 模块的功用。 如何在 Python 实现一个通用的装饰器 如何使用 wrapt 实现模拟库来辅助单元测试 如何为 Python 添加猴子补丁 装饰器，模拟库，猴子补丁的实现是递进的。装饰器通常是在导入时，在被装饰的函数定义之后立即运行，且永久全局有效；模拟库作用的范围变窄，需要实现只作用于特定范围，比如特定的测试函数中；猴子补丁更随意，通常在类创建一段时间之后再执行，这种延迟性导致猴子补丁存在相对导入的次序问题。对于我们而言搞清楚装饰器与模拟库的使用即可，能使用到猴子补丁的情景少之又少。 装饰器那如何实现一个装饰器？传统的通过闭包实现的装饰器存在以下问题: 无法保留函数的自省属性和签名信息，无法获取函数源代码 无法将装饰器应用于另一个为实现描述符的装饰器之上.简单的装饰器实现不会遵守被包装对象的描述符协议，因而破坏了Python对象的执行模型 为解决这些问题和解决代码复用问题，wrapt 创建了以下对象或函数: 代理对象: ObjectProxy，解决了自省问题 包装对象: FunctionWrapper, BoundFunctionWrapper 继承自 ObjectProxy，并为装饰行为实现了描述符协议 工厂函数: decorator 解决了创建装饰器的代码复用问题。 wrapt 为辅助单元测试提供了另外一个工厂函数 transient_function_wrapper，其能创建一个仅仅限于特定范围的临时补丁。 装饰器实现的核心就是包装器对象，它同时接收包装函数，和被包装函数，并作为装饰结果的返回值替换被包装函数。在被包装函数被调用时，实际调用包装函数。所以包装对象同时实现了对象代理和描述符协议。 2. wrapt 接口123456789101112# wrapt.__init__from .wrappers import (ObjectProxy, CallableObjectProxy, FunctionWrapper, BoundFunctionWrapper, WeakFunctionProxy, resolve_path, apply_patch, wrap_object, wrap_object_attribute, function_wrapper, wrap_function_wrapper, patch_function_wrapper, transient_function_wrapper)from .decorators import (adapter_factory, AdapterFactory, decorator, synchronized)from .importer import (register_post_import_hook, when_imported, notify_module_loaded, discover_post_import_hooks) wrapt 模块提供的接口大体上分成了以下几类: 代理对象: ObjectProxy, CallableObjectProxy, WeakFunctionProxy 包装对象: FunctionWrapper, BoundFunctionWrapper 装饰器工厂函数: function_wrapper, decorator 辅助测试的工厂函数: wrap_function_wrapper, patch_function_wrapper, transient_function_wrapper 猴子补丁相关: .importer synchronized: java synchronized 的 Python 实现 接下来我们会详细介绍上述典型接口的使用方式。 2. 代理对象 ObjectProxy所谓代理包含两个层面的意思: 将上层的请求传递给后端的对象 将后端对象的返回值返回给上层的调用方 wrapt 模块的底层实现就是基于透明对象代理的包装器类。这种代理对象不仅代理了普通方法和属性的访问，也代理了众多内置方法和自省属性。这使得代理对象和被代理对象在 Python 的数据模型层面是完全一致。使用代理对象去代替被代理对象不会打破 Python 的内省机制。并且我们可以在代理对象上自定义属性和方法，以此来重载被代理对象的默认功能。 2.1 对象联动123456class ObjectProxy(with_metaclass(_ObjectProxyMetaType)): __slots__ = '__wrapped__' def __init__(self, wrapped): object.__setattr__(self, '__wrapped__', wrapped) ObjectProxy 是 wrapt 代理功能实现的基类，通常不直接使用，而是作为自定义代理对象的基类使用。代理对象实现了如下功能: 所有对代理对象的访问都会传递给被代理对象，包括比较操作，哈希这些 Python 的内置方法 在代理对象上自定义的方法会覆盖被代理对象同名方法，因此我们可以通过代理对象实现对被代理对象的方法重载 所有对代理对象属性的修改都会传递并修改后端的被代理对象 对后端被代理对象属性的直接修改也会直接反映在代理对象之上 也就是说默认情况下，对 ObjectProxy 的操作，方法是重载的，而对属性的修改，是直接作用在被代理对象上的。 123456789101112&gt;&gt;&gt; table = &#123;&#125;&gt;&gt;&gt; proxy = wrapt.ObjectProxy(table)&gt;&gt;&gt; proxy['key-1'] = 'value-1'&gt;&gt;&gt; proxy['key-2'] = 'value-2'&gt;&gt;&gt; proxy.keys()['key-2', 'key-1']&gt;&gt;&gt; table.keys()['key-2', 'key-1']&gt;&gt;&gt; isinstance(proxy, dict)True 2.2 不可变对象上述操作对于不可变对象的自操作是特例。 1234567891011121314&gt;&gt;&gt; value = 1&gt;&gt;&gt; proxy = wrapt.ObjectProxy(value)&gt;&gt;&gt; type(proxy)&lt;type 'ObjectProxy'&gt;&gt;&gt;&gt; proxy += 1&gt;&gt;&gt; type(proxy)&lt;type 'ObjectProxy'&gt;&gt;&gt;&gt; print(proxy)2&gt;&gt;&gt; print(value)1 对于不可变对象，被代理对象保存的被代理对象的副本，因此对其自身的修改不会影响到后端的被代理对象。 2.3 类型比较由于 Python 复杂的对象模型和底层设计，以及 instance 函数内在比较逻辑，想把 ObjectProxy 类型比较的原理说清楚实在不容易。这里就不深入见解了，简而言之 ObjectProxy 类实例的__class__ 属性返回的是被代理对象的__class__ 属性值，instance()在进行类型检查时，首先比较的是 __class__，所以对代理对象进行类型比较的结果与以被代理对象本身进行比较的结果完全一致。同时由于抽象基类机制，ObjectProxy 实例与 ObjectProxy 类的类型比较也能正常进行。 12345678910111213141516171819202122232425&gt;&gt;&gt; value = 1&gt;&gt;&gt; proxy = wrapt.ObjectProxy(value)&gt;&gt;&gt; type(proxy)&lt;type 'ObjectProxy'&gt;&gt;&gt;&gt; class CustomProxy(wrapt.ObjectProxy):... pass&gt;&gt;&gt; proxy = CustomProxy(1)&gt;&gt;&gt; type(proxy)&lt;class '__main__.CustomProxy'&gt;# 与被代理对象的类型比较&gt;&gt;&gt; proxy.__class__&lt;type 'int'&gt;&gt;&gt;&gt; isinstance(proxy, int)True# 与代理对象的类型比较&gt;&gt;&gt; isinstance(proxy, wrapt.ObjectProxy)True&gt;&gt;&gt; isinstance(proxy, CustomProxy)True 2.4 方法重载方法重载只要在自定义代理对象上自定义同名的方法即可，在代理对象内，通过 __wrapped__ 属性可以访问到原始的被代理的对象。 123456789101112131415161718def function(): print('executing', function.__name__)class CallableWrapper(wrapt.ObjectProxy): def __call__(self, *args, **kwargs): print('entering', self.__wrapped__.__name__) try: return self.__wrapped__(*args, **kwargs) finally: print('exiting', self.__wrapped__.__name__)&gt;&gt;&gt; proxy = CallableWrapper(function)&gt;&gt;&gt; proxy()('entering', 'function')('executing', 'function')('exiting', 'function') 2.5 属性重载因为对 ObjectProxy 属性的访问都会直接代理至后端被代理对象，那如何自定义 ObjectProxy 自身的属性呢？ 方法一，任何以 _self_ 开头的属性只会保存在 ObjectProxy 上，不会传递给后端的被代理对象 12345678910111213141516171819202122232425262728def function(): print('executing', function.__name__)class CallableWrapper(wrapt.ObjectProxy): def __init__(self, wrapped, wrapper): super(CallableWrapper, self).__init__(wrapped) self._self_wrapper = wrapper def __call__(self, *args, **kwargs): return self._self_wrapper(self.__wrapped__, args, kwargs)def wrapper(wrapped, args, kwargs): print('entering', wrapped.__name__) try: return wrapped(*args, **kwargs) finally: print('exiting', wrapped.__name__)&gt;&gt;&gt; proxy = CallableWrapper(function, wrapper)&gt;&gt;&gt; proxy._self_wrapper&lt;function wrapper at 0x1005961b8&gt;&gt;&gt;&gt; function._self_wrapperTraceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;AttributeError: 'function' object has no attribute '_self_wrapper' 方法二，借助于 @property，定义属性描述符1234567891011121314151617181920212223242526272829class CustomProxy(wrapt.ObjectProxy): def __init__(self, wrapped): super(CustomProxy, self).__init__(wrapped) self._self_attribute = 1 @property def attribute(self): return self._self_attribute @attribute.setter def attribute(self, value): self._self_attribute = value @attribute.deleter def attribute(self): del self._self_attribute&gt;&gt;&gt; proxy = CustomProxy(1)&gt;&gt;&gt; print proxy.attribute1&gt;&gt;&gt; proxy.attribute = 2&gt;&gt;&gt; print proxy.attribute2&gt;&gt;&gt; del proxy.attribute&gt;&gt;&gt; print proxy.attributeTraceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;AttributeError: 'int' object has no attribute 'attribute' 方法三，将属性定义为类属性123456789101112131415&gt;&gt;&gt; class CustomProxy(ObjectProxy):... attribute = None...&gt;&gt;&gt; def function():... print('executing', function.__name__)...&gt;&gt;&gt; j = CustomProxy(function)&gt;&gt;&gt; j.attribute = 2&gt;&gt;&gt;&gt;&gt;&gt; function.attribute = 5&gt;&gt;&gt; print(j.attribute)2&gt;&gt;&gt; print(function.attribute)5 3. 扩展的代理对象除了默认 ObjectProxy 代理基类，wrapt 还提供了另外两个通用的代理对象。 3.1 CallableObjectProxy1234class CallableObjectProxy(ObjectProxy): def __call__(self, *args, **kwargs): return self.__wrapped__(*args, **kwargs) CallableObjectProxy 代理对象专用于代理函数，只是额外的附加了__call__方法，让代理对象成为可调用对象。 3.2 WeakFunctionProxy123456# 代理有点长，不粘了，有兴趣查看 wrapt 的源码class WeakFunctionProxy(ObjectProxy): __slots__ = ('_self_expired', '_self_instance') def __init__(self, wrapped, callback=None): 默认情况下，代理对象通过 __wrapped__ 属性保存了对被代理对像的引用，这样会导致被代理对象始终被引用而无法被垃圾处理器收回，WeakFunctionProxy 的作用就是实现在代理对象中实现对被代理对象的弱引用。在代理对象中实现弱引用并不容易，特别是对绑定方法对象的处理，以及要避免在回调函数中出现循环引用。有兴趣的同学可以看看 wrapt 的源代码。 3.3 自定义代理对象如上述两个内置扩展的代理对象，通过继承 ObjectProxy，我们也可以自定代理对象。代理对象中的方法会覆盖被代理对象的同名方法，利用这个特性我们可以重载被代理对象的行为，这在单元测试中经常使用，待会会有使用的详细示例。 4. 包装对象下面是在代理对象基础上实现包装器的简单示例，包装器继承自 wrapt.ObjectProxy，并将被代理对象作为参数传递给 ObjectProxy，从而具备了代理功能，并在此基础上附加了描述协议的处理逻辑。我们需要使用或者自定义包装对象的情景很少，此处不再对其作过多描述。 123456789101112class CallableWrapper(wrapt.ObjectProxy): def __init__(self, wrapped, wrapper): super(CallableWrapper, self).__init__(wrapped) self._self_wrapper = wrapper def __get__(self, instance, owner): function = self.__wrapped__.__get__(instance, owner) return BoundCallableWrapper(function, self._self_wrapper) def __call__(self, *args, **kwargs): return self._self_wrapper(self.__wrapped__, args, kwargs) 5. 辅助测试5.1 工厂函数wrapt 中有三个辅助测试的包装对象 wrapt.wrap_function_wrapper: 创建猴子补丁的工厂函数，会创建永久有效的补丁 wrapt.patch_function_wrapper: 简化 wrapt.wrap_function_wrapper 的装饰器函数 wrapt.transient_function_wrapper: 创建一个仅仅限于特定范围的临时补丁 下面是它们的使用实例 1234567891011121314151617181920212223242526def wrapper(wrapped, instance, args, kwargs): return wrapped(*args, **kwargs)class Example(object): def name(self): return 'name'import wrapt# 版本一wrapt.wrap_function_wrapper(Example, 'name', wrapper) # 等同于wrapt.wrap_function_wrapper('example', 'Example.name', wrapper)# 版本二@wrapt.patch_function_wrapper('example', 'Example.name')def wrapper(wrapped, instance, args, kwargs): return wrapped(*args, **kwargs)# 版本三，wrapper 只对 test_method() 函数有效@transient_function_wrapper('example', 'Example.name')def wrapper(wrapped, instance, args, kwargs): return wrapped(*args, **kwargs)@wrapper def test_method(): pass 5.2 高阶用法除了上述简单的使用示例外，12 使用 wrapt 辅助测试 还有更高级的使用示例，下面是示例代码。 包装一个返回函数的被包装对象12345678910111213141516171819202122from wrapt import transient_function_wrapper, function_wrapperdef function(): passclass ProductionClass(object): def method(self, a, b, c, key): return function@function_wrapperdef result_function_wrapper(wrapped, instance, args, kwargs): return wrapped(*args, **kwargs)@transient_function_wrapper(__name__, 'ProductionClass.method')def apply_ProductionClass_method_wrapper(wrapped, instance, args, kwargs): return result_function_wrapper(wrapped(*args, **kwargs))@apply_ProductionClass_method_wrapperdef test_method(): real = ProductionClass() func = real.method(3, 4, 5, key='value') result = func() 包装一个类示例的被包装对象123456789101112131415161718192021222324252627from wrapt import transient_function_wrapper, function_wrapperclass StorageClass(object): def run(self): passstorage = StorageClass()class ProductionClass(object): def method(self, a, b, c, key): return storage@function_wrapperdef run_method_wrapper(wrapped, instance, args, kwargs): return wrapped(*args, **kwargs)@transient_function_wrapper(__name__, 'ProductionClass.method')def apply_ProductionClass_method_wrapper(wrapped, instance, args, kwargs): storage = wrapped(*args, **kwargs) storage.run = run_method_wrapper(storage.run) return storage@apply_ProductionClass_method_wrapperdef test_method(): real = ProductionClass() data = real.method(3, 4, 5, key='value') result = data.run() 6. synchronizedsynchronized 装饰器实现了 java 中的同步原语 synchronized 功能。synchronized 功能和实现请参阅 07 实现 java 的 @synchronized 装饰器，下面是其使用方式 6.1 作为装饰器123456789101112131415161718192021222324@synchronized # lock bound to function1def function1(): pass@synchronized # lock bound to function2def function2(): pass@synchronized # lock bound to Classclass Class(object): @synchronized # lock bound to instance of Class def function_im(self): pass @synchronized # lock bound to Class @classmethod def function_cm(cls): pass @synchronized # lock bound to function_sm @staticmethod def function_sm(): pass 6.2 作为上下文管里器1234567891011121314151617181920class Class(object): @synchronized def function_im_1(self): pass def function_im_2(self): with synchronized(self): passclass Class(object): @synchronized @classmethod def function_cm(cls): pass def function_im(self): with synchronized(Class): pass 6.3 基于任意对象作同步除了使用默认的内置锁，synchronized 支持接收任意对象实现同步。但是作为同步而传入的对象必需能添加属性，因为 synchronized 会在传入的对象上保存创建的锁对象。因此为解除这个限制，synchronized 也支持传入支持 .require 和 .release 的类锁对象实现同步。 123456789101112131415161718192021class Data(object): passdata = Data()def function_1(): with synchronized(data): passdef function_2(): with synchronized(data): pass# synchronized 使用到了 vars(data)，任何没有 `__dict__` 属性的对象，都会调用失败&gt;&gt;&gt; vars(&#123;&#125;)Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File \"&lt;ipython-input-3-880c6250c41c&gt;\", line 1, in &lt;module&gt; vars(&#123;&#125;)TypeError: vars() argument must have __dict__ attribute 6.4 基于传入的类锁对象作同步12345semaphore = threading.Semaphore(2)@synchronized(semaphore)def function(): pass 任何支持 acquire() 和 release() 对象均可作为 synchronized的参数，因此用户可传入包含这两个方法的自定义对象来实现额外的功能。 7. decorator12def decorator(wrapper=None, enabled=None, adapter=None): pass decorator 工厂函数是 function_wrapper 工厂函数的升级版本，在装饰器基础上添加了另外两个控制功能，enabled 和 adapter参数必需作为关键词参数被使用。 7.1 装饰启动开关静态控制enabled 参数用于控制装饰器是否被启用，接收布尔值作为参数，enabled=True 时，装饰器正常启用，enabled=False 时不会应用任何包装器。因此，这提供了一种方便的方法，可以全局禁用特定的decorator，而不必删除decorator的所有用法，或者使用decorator函数的特殊变体。 123456789101112ENABLED = False@wrapt.decorator(enabled=ENABLED)def pass_through(wrapped, instance, args, kwargs): return wrapped(*args, **kwargs)@pass_throughdef function(): pass&gt;&gt;&gt; type(function)&lt;type 'function'&gt; 动态控制在定义修饰符时为启用的选项提供一个布尔值，从而控制修饰符是否应该应用。因此，这是一个全局开关，一旦禁用，就无法在运行时在进程执行时动态地重新启用它。类似地，一旦启用，就不能禁用它。 提供布尔值的另一种方法是为enabled提供一个可调用对象 callable，该值返回一个布尔值。每次调用修饰函数时都将调用callable。如果callable返回True，表示decorator是活动的，则将调用包装器函数。如果callable返回False，包装器函数将被绕过，原始包装函数将被直接调用。 如果enabled不是None、布尔值或可调用值，则将对提供的对象执行布尔检查。这允许使用支持逻辑操作的定制对象。如果定制对象计算为False，包装器函数将再次被绕过。 123456def _enabled(): return True@wrapt.decorator(enabled=_enabled)def pass_through(wrapped, instance, args, kwargs): return wrapped(*args, **kwargs) 7.2 更改签名信息默认的包装函数的签名来自被包装对象，adapter 参数的作用用于修改包装函数的签名信息。其接收一个函数作为参数，此函数的签名信息将作为包装函数的签名信息被返回。这个用的很少，就不再累述了。 实战有关 wrapt 的模块的实现和接口到此就介绍完了，在本系列博客的开篇我提到了我使用装饰器的一个典型应用场景: 对数据库查询实现分批操作。在接下来的的博客中，作为实战篇，我们来看看如何通过 wrapt 实现这个比较通用的需求。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"wrapt","slug":"wrapt","permalink":"http://yoursite.com/tags/wrapt/"}]},{"title":"14 为 Python 应用自动打补丁","slug":"wrapt/python_decorator_14","date":"2018-06-04T16:00:00.000Z","updated":"2020-05-24T15:31:35.997Z","comments":true,"path":"2018/06/05/wrapt/python_decorator_14/","link":"","permalink":"http://yoursite.com/2018/06/05/wrapt/python_decorator_14/","excerpt":"前面我们已经决绝了猴子补丁的导入次序问题，但是这个解决方案有个前提，就是我们必需能修改应用程序代码，以在程序的最开始执行我们的注册函数。本节我们的目的是找到另一种解决方案取消这个限制。","text":"前面我们已经决绝了猴子补丁的导入次序问题，但是这个解决方案有个前提，就是我们必需能修改应用程序代码，以在程序的最开始执行我们的注册函数。本节我们的目的是找到另一种解决方案取消这个限制。 1. 猴子补丁的问题所在在之前关于猴子的文章中，我们讨论了导入次序问题。也就是说，正确使用猴子补丁取决于，我们能在任何其他代码导入我们想要修补的模块之前为其打上打补丁。换句话说就是在我们打补丁之前，其他代码是否已经按名称导入了对模块内函数的引用，并将其存储在它自己的名称空间中。即在打补丁之前，其他模块是否已经使用了 from module import function 如果我们不能尽早进入，那么就需要对目标函数的所有使用打补丁，这在一般情况下是不可能的，因为我们不知道函数在哪里被导入。我所描述的一种解决方案是使用导入后钩子机制，使我们能够在模块被任何代码导入之前访问模块并打补丁。这种技术仍然依赖于在有效运行其他代码之前安装导入后钩子机制本身。这意味着必须手动修改应用程序的主Python脚本文件，这并不总是实用的。本文的目的是研究如何避免修改主Python脚本文件来解决导入次序问题。 2. 在 .pth 文件中执行代码作为Python导入系统的一部分，以及在哪些目录中搜索Python模块，有一种扩展机制，即可以将一个.pth扩展名文件安装到Python的site-packages目录中。用于指明Python包代码并不在默认的Python模块搜索路径上，而是存在于其他位置，通常是在site-packages的子目录中。.pth文件的目的是充当指向Python包的实际代码的指针。 在简单的情况下，.pth文件将包含与包含Python包代码的实际目录的名称相关的或绝对的路径名。如果它是一个相对路径名，那么它将相对于.pth文件所在的目录。 如果使用 .pth，当Python 解释器初始化时，它会创建Python模块的搜索路经，在添加所有默认搜索目录后，它将查找 site-packages内的所有目录，并解析每一个 .pth 文件，并将 .pth 内的目录添加到最后的搜索目录列表中。 现在，在Python的历史中，这个.pth机制被增强了，以支持一个特殊的情况。这种特殊情况是，如果.pth文件中的一行从导入开始，那么该行将作为Python代码执行，而不是简单地将其作为目录添加到要搜索模块的目录列表中。 这最初是为了允许为模块执行特殊的启动代码，以允许为Unicode注册一个非标准的编解码器。不过，它后来也被用于easy_install的实现中，如果您曾经运行过easy-install并查看了site-packages目录中的easy-install.pth文件，您会发现以下代码: 123import sys; sys.__plen = len(sys.path)./antigravity-0.1-py2.7.eggimport sys; new=sys.path[sys.__plen:]; del sys.path[sys.__plen:]; p=getattr(sys,'__egginsert',0); sys.path[p:p]=new; sys.__egginsert = p+len(new) 因此，只要能够将代码放在一行上，就可以在每次运行Python解释器时，在.pth文件中做一些非常古怪的事情。我(作者)认为可执行代码在.pth文件中的概念是非常危险的，到目前为止，我(作者)一直避免依赖.pth文件的这个特性。 我(作者)对.pth文件中的可执行代码的担心是它总是在运行。这意味着，即使您已经将预构建的RPM/DEB包或Python wheel 安装到系统中的Python安装环境中，并且认为这样做更安全，因为避免了作为根用户运行 setup.py。但是.pth文件意味着包仍然可以在您不知情的情况下运行代码，甚至不需要将模块导入任何应用程序。考虑到安全性，Python真应该有一个白名单机制，用于确定信任哪些.pth文件，以允许其在每次运行Python解释器(特别是作为根用户)时执行代码。 如果有人关心的话，我将把这个讨论留给其他人来讨论，至少现在我将展示如何使用.pth文件的这个特性(滥用)来实现对正在运行的任何Python应用程序进行自动的猴子补丁的机制。 3. 添加导入勾子在前一篇文章中，我们讨论的导入后钩子机制，在任何Python应用程序脚本文件的开头，我都需要手动添加如下代码: 123456789101112import osfrom wrapt import discover_post_import_hookspatches = os.environ.get('WRAPT_PATCHES')if patches: for name in patches.split(','): name = name.strip() if name: print 'discover', name discover_post_import_hooks(name) 它所做的是使用环境变量作为任何使用setuptools入口点注册的包的名称来源，这些入口点包含我们想要应用的猴子补丁。 了解了可以在.pth文件执行代码的能力,现在可以使用它，让这段代码在Python解释器启动时自动执行,从而避免了每次都需要手动修改每个Python应用程序，来应用我们的猴子补丁。 但是在实践中，我们需要的代码实际上要比这个稍微复杂一些，并且不能很容易地直接添加到.pth文件中，这是由于需要将所有代码写在一行上。因此，我们要做的是将所有代码放在一个单独的模块中，然后执行该模块。我们不希望每次都导入那个模块，也许用户看到它被导入时会感到害怕，即使它没有被使用，所以我们将通过环境变量的判断使用它。因此，我们可以在我们的.pth中使用的是: 1import os, sys; os.environ.get('AUTOWRAPT_BOOTSTRAP') and __import__('autowrapt.bootstrap') and sys.modules['autowrapt.bootstrap'].bootstrap() 也就是说，如果环境变量被设置为非空值，那么我们需要导入包含引导代码的模块并执行它。至于引导代码，这就有点麻烦了。我们不能只使用以前手动修改Python应用程序脚本文件时使用的代码。这是因为.pth文件的解析发生在Python解释器初始化。 问题有两个。第一个问题发生在执行导入钩子的发现，当.pth文件被执行时，它被处理的顺序是未知的，所以在我们的代码运行的时候，最终的Python模块搜索路径可能没有设置。第二个问题是.pth文件的处理发生在任何sitecustomize.py或usercustomize.py被处理完之前。因此，Python解释器可能不在其最终配置状态。因此，我们必须对我们所做的事情小心一点。 我们真正需要的是将任何操作延迟到Python解释器的初始化完成之后。问题是我们如何做到这一点。 4. site 模块Python解释器初始化的最后部分是由site 模块的main()函数完成的 1234567891011121314151617181920212223def main(): global ENABLE_USER_SITE abs__file__() known_paths = removeduppaths() if ENABLE_USER_SITE is None: ENABLE_USER_SITE = check_enableusersite() known_paths = addusersitepackages(known_paths) known_paths = addsitepackages(known_paths) if sys.platform == 'os2emx': setBEGINLIBPATH() setquit() setcopyright() sethelper() aliasmbcs() setencoding() execsitecustomize() if ENABLE_USER_SITE: execusercustomize() # .pth 在此之后执行 # Remove sys.setdefaultencoding() so that users cannot change the # encoding after initialization. The test for presence is needed when # this module is run as a script, because this code is executed twice. if hasattr(sys, \"setdefaultencoding\"): del sys.setdefaultencoding 我们希望依赖的.pth解析和代码执行是在addsitepackages()函数中完成的。因此，我们真正需要的是将代码的任何执行推迟到execsitecustomize()中或execusercustomize()函数运行之后。实现这一点的方法是对这两个函数进行修改，并在它们完成时触发我们的代码。 我们需要都打上补丁，因为usercustomize.py的执行是可选的，取决于ENABLE_USER_SITE环境变量是否为真。因此，我们的bootstrap()函数应该如下 1234567891011121314151617181920def _execsitecustomize_wrapper(wrapped): def _execsitecustomize(*args, **kwargs): try: return wrapped(*args, **kwargs) finally: if not site.ENABLE_USER_SITE: # 判断 _register_bootstrap_functions() return _execsitecustomizedef _execusercustomize_wrapper(wrapped): def _execusercustomize(*args, **kwargs): try: return wrapped(*args, **kwargs) finally: _register_bootstrap_functions() return _execusercustomizedef bootstrap(): site.execsitecustomize = _execsitecustomize_wrapper(site.execsitecustomize) site.execusercustomize = _execusercustomize_wrapper(site.execusercustomize) 尽管我曾经说过手工构建的猴子补丁有多糟糕，并且wrapt模块应该用于创建猴子补丁，但是在这种情况下，我们实际上不能使用wrapt模块。这是因为从技术上讲，作为用户安装的包，wrapt包此时可能不能使用。如果wrapt的安装方式是这样的，那么导入它的能力本身就依赖于.pth文件的处理。因此，我们使用一个函数闭包来使用简单的包装器。 在实际的包装器中，您可以看到两个包装器中哪个最终调用 _register_bootstrap_functions() 取决于ENABLE_USER_SITE是否为真，如果启用了对usersitecustomize()的支持，那么只能在execsitecustomize()中调用它。 最后，我们现在将_register_bootstrap_functions() 定义为: 1234567891011_registered = Falsedef _register_bootstrap_functions(): global _registered if _registered: return _registered = True from wrapt import discover_post_import_hooks for name in os.environ.get('AUTOWRAPT_BOOTSTRAP', '').split(','): discover_post_import_hooks(name) 5. 初始化包我们已经解决了所有问题，但是如何安装它，特别是如何安装自定义的.pth文件。为此我们使用一个设置.py文件: 12345678910111213141516import sysimport osfrom setuptools import setupfrom distutils.sysconfig import get_python_libsetup_kwargs = dict( name = 'autowrapt', packages = ['autowrapt'], package_dir = &#123;'autowrapt': 'src'&#125;, data_files = [(get_python_lib(prefix=''), ['autowrapt-init.pth'])], entry_points = &#123;'autowrapt.examples': ['this = autowrapt.examples:autowrapt_this']&#125;, install_requires = ['wrapt&gt;=1.10.4'],)setup(**setup_kwargs) 为了安装.pth，我们使用了setup()调用的data_files参数。使用distutils.sysconfig模块中的get_python_lib()函数确定安装文件的实际位置。前缀“空字符串”的参数确保了Python包安装的路经为 site-packages 的相对路径，而不是绝对路径。** 安装这个包时非常重要的一点是，您不能使用easy_install或python setup.py安装。只能使用pip安装这个包。 这样做的原因是，如果不使用pip，那么包安装工具可以将包安装为egg。在这种情况下，自定义.pth文件实际上将安装在egg目录中，而不是实际安装在site-packages目录中。 .pth文件只有被添加到 site-packages 目录中，才能用于映射autowrapt包存在的子目录。从site模块调用的addsitepackages()函数并不会处理包含在.pth文件添加的目录中的.pth文件，因此我们的自定义.pth文件将被跳过。** 在使用“pip”时，默认情况下不使用eggs，所以可行。 还要注意的是，这个包不能与buildout一起工作，因为它总是将包作为eggs安装，并且在Python 安装环境中安装任何脚本时，都会显式地设置Python模块搜索路径本身。 6. 使用示例此软件包的实际完整源代码可在: https://github.com/GrahamDumpleton/autowrapt 这个包也在PyPi上作为autowrapt发布，因此您可以尝试它，如果您真的想使用它的话。为了方便快速地测试它是否有效，autowrapt包打包了一个示例monkey patch。在上面的setyp.py被设置如下:** 1entry_points = &#123;'autowrapt.examples': ['this = autowrapt.examples:autowrapt_this']&#125;, 这个entry point 定义了一个名为autowrapt.examples的猴子补丁。定义了当导入 this 模块时，模块autowrapt.examples中的猴子补丁函数autowrapt_this()将被执行。** 所以要运行这个测试需要: pip install autowrapt 如果没有所需的最小版本，也应该安装wrapt模块。现在正常运行命令行解释器，并在提示符处执行: import this 这应该会显示Python的Zen。退出Python解释器，现在运行: AUTOWRAPT_BOOTSTRAP=autowrapt.examples python 这将再次运行Python解释器，并将环境变量AUTOWRAPT_BOOTSTRAP设置为autowrapt.examples,以匹配在setup.py中为autowrapt定义的entry point。autowrapt_this()”函数的实际代码是: 1234from __future__ import print_functiondef autowrapt_this(module): print('The wrapt package is absolutely amazing and you should use it.') 所以如果我们再一次运行: import this 我们现在应该看到Python Zen的扩展版本。在本例中，我们实际上并没有对目标模块中的任何代码打补丁，但它显示了补丁函数实际上是按预期被触发。 7. 其他机制虽然这种机制相当干净，并且只需要设置环境变量，但是不能像前面提到的那样与buildout一起使用。对于buildout，我们需要研究其他可以实现同样效果的方法。我将在下一篇关于这一主题的博文中讨论这些其他选择。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"wrapt","slug":"wrapt","permalink":"http://yoursite.com/tags/wrapt/"}]},{"title":"13 猴子补丁在 Python 中的加载次序问题","slug":"wrapt/python_decorator_13","date":"2018-06-03T16:00:00.000Z","updated":"2020-05-24T15:31:31.307Z","comments":true,"path":"2018/06/04/wrapt/python_decorator_13/","link":"","permalink":"http://yoursite.com/2018/06/04/wrapt/python_decorator_13/","excerpt":"本节我们就来解决如何在 Python 中打补丁的问题。","text":"本节我们就来解决如何在 Python 中打补丁的问题。 1. 猴子补丁的加载次序问题在第 11 篇博客中，我们提到了应用猴子补丁时可能存在的问题。具体地说，如果需要被打补丁的模块已经被导入并被其他代码使用，那么它可能已经在自己的名称空间中创建了一个被打补丁的目标函数的本地引用。因此，尽管猴子补丁可以正常工作，但是仍然无法覆盖这种原始函数已经导入，并过通过本地引用直接访问原始函数的情况。 导入次序问题的解决方案之一是所谓的导入钩子。这是在PEP 369中描述的一种机制，虽然它从未进入Python核心，但是仍然可以使用现有的api将这种能力移植到Python中。然后，在模块导入目标函数并在自己的名称空间中创建对函数的引用之前，我们可以添加其他功能来发现猴子补丁代码，并在导入模块时自动应用它。 Post import hook mechanism暂时将 “Post import hook” 称为导入后勾子。导入后勾子机制在 PEP 369 中有一个使用示例: 12345import imp@imp.when_imported('decimal')def register(decimal): Inexact.register(decimal.Decimal) 其基本思想是，当看到这段代码时，它将导致在Python导入系统中注册一个回调，以便在导入decimal模块时，调用装饰器应用的register()函数。register()函数的参数是对被注册的模块的引用。然后，该函数可以对模块执行一些操作，最后再将模块返回到最初请求导入的代码中。除了使用作为装饰器的@imp.where_imported函数 ，还可以显式地使用imp.register_post_import_hook() 函数来注册导入后钩子。 123456import impdef register(decimal): Inexact.register(decimal.Decimal)imp.register_post_import_hook(register, 'decimal') 尽管PEP 369从未被合并到Python中，但是wrapt 提供了类似功能的装饰器和函数。尽管装饰器和函数被用来解决导入次序问题。但如果目标模块在导入后钩子函数执行之前就已经被导入，我们仍会面临导入次序问题。 这个问题最简单的解决方案是修改应用程序的主Python脚本，并将您需要的所有的”导入后勾子”的注册设置为绝对的第一件事。也就是说，在从应用程序导入任何其他模块包括任何解析命令行参数的标准库之前注册”导入后勾子”。 尽管你确实可以做到这一点，但是由于注册函数会发生事实上的调用，这意味注册函数的执行可能转而导入那些将要被打补丁的模块，所以依然可能发生导入错误。 有一种间接的方式可以解决所有的问题，下面是应用这个原则的例子。方法是相对于导入猴子补丁代码，我们创建一个注册函数，只有当被补丁的模块被导入，猴子补丁才会被惰性加载，之后才会被执行。 123456789101112import sysfrom wrapt import register_post_import_hookdef load_and_execute(name): def _load_and_execute(target_module): __import__(name) patch_module = sys.modules[name] getattr(patch_module, 'apply_patch')(target_module) return _load_and_executeregister_post_import_hook(load_and_execute('patch_tempfile'), 'tempfile') patch_tempfile.py代码如下: 123456789from wrapt import wrap_function_wrapperdef _mkdtemp_wrapper(wrapped, instance, args, kwargs): print 'calling', wrapped.__name__ return wrapped(*args, **kwargs)def apply_patch(module): print 'patching', module.__name__ wrap_function_wrapper(module, 'mkdtemp', _mkdtemp_wrapper) 使用交互式解释器运行第一个脚本，以便将我们留在解释器中，然后，我们可以显示导入tempfile模块并执行mkdtemp()函数，看看会发生什么。 123456$ python -i lazyloader.py&gt;&gt;&gt; import tempfilepatching tempfile&gt;&gt;&gt; tempfile.mkdtemp()calling mkdtemp'/var/folders/0p/4vcv19pj5d72m_bx0h40sw340000gp/T/tmpfB8r20' 上述整个导入过程是这样的: register_post_import_hook 为 tempfile 模块注册了 _load_and_execute 函数 import tempfile 时，会先执行 _load_and_execute 函数，此时会加载patch_tempfile 模块，并执行 apply_patch 函数 apply_patch 接收 tempfile 模块对象作为参数后执行，并使用 wrap_function_wrapper 函数为 mkdtemp 打上补丁。 mkdtemp 执行的就是打补丁之后的函数 整个过程，tempfile 模块被导入时，猴子补丁才被惰性加载。 换句话说，与大多数猴子补丁不同，我们并不是强行导入一个模块，以便在可能使用的基础上应用猴子补丁。相反，猴子补丁代码保持休眠和未使用，直到目标模块稍后被导入。如果没有导入目标模块，则该模块的猴子补丁代码本身甚至没有导入。 3. 发现导入后勾子如上所述，导入后钩子提供了一种稍微更好的方法来设置猴子补丁，以便应用它们。这是因为只有当包含要修补的函数的目标模块被导入时，它们才会被激活。这避免了不必要地导入可能不使用的模块，否则会增加应用程序的内存使用。 导入次序仍然很重要，因此，要确保在导入任何其他模块之前设置所有导入后钩子。并且在每次更改应用的猴子补丁后，需要修改应用程序代码。如果只是为了调试问题而频繁地添加猴子补丁，则可能不太方便。 后一个问题的解决方案是将猴子补丁分离到单独的模块中，并使用一个注册机制来宣布它们的可用性。然后，Python应用程序可以在一开始就执行通用的模板代码，该代码根据提供的配置发现应该应用哪些猴子补丁。注册机制将允许在运行时发现猴子补丁模块。 这里可以使用的一种特殊的注册机制是setuptools入口点。使用这个我们可以打包猴子补丁，这样它们就可以被单独安装以备使用。这样一套方案的结构是: 123setup.pysrc/__init__.pysrc/tempfile_debugging.py 这个包的 setup.py 代码将会是: 123456789101112131415161718192021from setuptools import setupNAME = 'wrapt_patches.tempfile_debugging'def patch_module(module, function=None): function = function or 'patch_%s' % module.replace('.', '_') return '%s = %s:%s' % (module, NAME, function)ENTRY_POINTS = [ patch_module('tempfile'),]setup_kwargs = dict( name = NAME, version = '0.1', packages = ['wrapt_patches'], package_dir = &#123;'wrapt_patches': 'src'&#125;, entry_points = &#123; NAME: ENTRY_POINTS &#125;,)setup(**setup_kwargs) 作为一种约定，我们使用命名空间包，以便我们的猴子补丁模块易于识别。在本例中，父包将是wrapt_patch，因为我们专门使用wrapt。这个特定包的名称将是wrapt_patch.tempfile_debug,表示我们将创建一些猴子补丁，以帮助我们调试使用tempfile模块。 setup.py的关键部分是定义entry_points。它将被设置成程序包名到猴子补丁映射的列表，这个列表包含了这个补丁模块要作用的所有目标Python模块。此处 ENTRY_POINTS 的值为 123ENTRY_POINTS = [ 'tempfile = wrapt_patches.tempfile_debugging:patch_tempfile',] src/init.py 将包含: 12import pkgutil__path__ = pkgutil.extend_path(__path__, __name__) 这是创建命名空间包的要求。最后，猴子补丁实际上包含在src/tempfile_debug中。代码跟以前很像。 123456789from wrapt import wrap_function_wrapperdef _mkdtemp_wrapper(wrapped, instance, args, kwargs): print 'calling', wrapped.__name__ return wrapped(*args, **kwargs)def patch_tempfile(module): print 'patching', module.__name__ wrap_function_wrapper(module, 'mkdtemp', _mkdtemp_wrapper) 定义了包后，我们将它安装到正在使用的Python安装或虚拟环境中。现在，我们可以在Python应用程序主脚本文件的开头添加显式的注册，我们将添加: 123456789101112import osfrom wrapt import discover_post_import_hookspatches = os.environ.get('WRAPT_PATCHES')if patches: for name in patches.split(','): name = name.strip() if name: print 'discover', name discover_post_import_hooks(name) 如果我们在没有为猴子补丁特定配置的情况下运行应用程序，那么什么也不会发生。如果它们是启用的，那么它们将被自动发现并根据需要应用。 1234$ WRAPT_PATCHES=wrapt_patches.tempfile_debugging python -i entrypoints.pydiscover wrapt_patches.tempfile_debugging&gt;&gt;&gt; import tempfilepatching tempfile 理想的情况是，如果PEP 369真的进入了Python的核心，那么将类似的引导机制合并到Python本身中，以便在解释器初始化过程中尽早强制对猴子补丁进行注册。有了这一点，我们就有了一种有保证的方法来解决在做猴子补丁时的导入次序问题。 由于现在PEP 369还未进入Python的核心，所以我们在本例中所做的是修改Python应用程序自己添加引导代码，以便在应用程序执行的最开始执行注册。当应用程序归自己管理时这是可以的，但是如果想要对第三方应用程序进行打补丁，并且不希望修改其代码，那该怎么办呢?在这种情况下有什么选择? 在这种情况下可以使用一些技巧。下一篇关于猴子补丁主题的博文中我们将讨论为应用程序打补丁的可用选项。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"wrapt","slug":"wrapt","permalink":"http://yoursite.com/tags/wrapt/"}]},{"title":"12 使用 wrapt 辅助测试","slug":"wrapt/python_decorator_12","date":"2018-06-02T16:00:00.000Z","updated":"2020-05-24T15:31:27.034Z","comments":true,"path":"2018/06/03/wrapt/python_decorator_12/","link":"","permalink":"http://yoursite.com/2018/06/03/wrapt/python_decorator_12/","excerpt":"前面我们说道过 Python 中使用猴子补丁典型情景之一就是使用模拟库来帮助执行单元测试，本节我们先把补丁和模块导入的相对次序问题放一放，先来看看如何使用 wrapt 模块辅助单元测试。","text":"前面我们说道过 Python 中使用猴子补丁典型情景之一就是使用模拟库来帮助执行单元测试，本节我们先把补丁和模块导入的相对次序问题放一放，先来看看如何使用 wrapt 模块辅助单元测试。 1. 使用 wrapt 进行测试在Python中讨论单元测试时，用于辅助该任务的比较流行的包之一是 mock 包。但是我(wrapt 的作者)觉得 mock 包不符合我的思维方式。 也可能只是我试图应用它的东西不太适合。在我想要测试的内容中，通常我不仅想要模拟更低的层，而且我想要验证传递到下一层的数据，或者修改结果。换句话说，我通常仍然需要系统作为一个整体来结束，并可能在很长一段时间内。 因此，对于我需要做的更复杂的测试，我实际上一直在依靠wrapt的猴子补丁功能。很有可能，因为我写了wrapt，我更熟悉它的范例，或者我更倾向于更明确的方式。不管怎样，至少对我来说，wrapt 能帮助我更快地完成工作。 为了进一步解释 wrapt 的猴子补丁功能，我在这篇博客文章中向大家展示了用wrapt模块实现部分 Mock 包的功能。只要记住，对于Mock模块我是一个绝对的新手，也可能也我太笨了，不能理解如何正确简单地使用它来做我想做的事情。 Return values and side effects如果你正在使用Mock，并且希望在调用时临时覆盖类的方法返回的值，一种方法是: 123456789101112from mock import Mock, patchclass ProductionClass(object): def method(self, a, b, c, key): print a, b, c, key@patch(__name__+'.ProductionClass.method', return_value=3)def test_method(mock_method): real = ProductionClass() result = real.method(3, 4, 5, key='value') mock_method.assert_called_with(3, 4, 5, key='value') assert result == 3 就我迄今为止提出的wrapt包而言，一种类似的做法是: 123456789101112131415from wrapt import patch_function_wrapperclass ProductionClass(object): def method(self, a, b, c, key): print a, b, c, key@patch_function_wrapper(__name__, 'ProductionClass.method')def wrapper(wrapped, instance, args, kwargs): assert args == (3, 4, 5) and kwargs.get('key') == 'value' return 3def test_method(): real = ProductionClass() result = real.method(3, 4, 5, key='value') assert result == 3 不过，这里的一个问题是，wrapt.patch_function_wrapper()函数应用了一个永久补丁。在这个过程的生命周期中，这是可以的，但是在测试的情况下，我们通常希望一个补丁只应用于当时正在运行的单个单元测试函数。因此，补丁应该在测试结束时和调用下一个函数之前应该被删除。 对于该场景，wrapt包提供了另一个装饰器@wrapt.transient_function_wrapper。用来创建一个包装函数，该函数只应用于修饰函数所应用的特定调用的范围。因此，我们可以把上面写为: 12345678910111213141516from wrapt import transient_function_wrapperclass ProductionClass(object): def method(self, a, b, c, key): print a, b, c, key@transient_function_wrapper(__name__, 'ProductionClass.method')def apply_ProductionClass_method_wrapper(wrapped, instance, args, kwargs): assert args == (3, 4, 5) and kwargs.get('key') == 'value' return 3@apply_ProductionClass_method_wrapperdef test_method(): real = ProductionClass() result = real.method(3, 4, 5, key='value') assert result == 3 尽管这个示例展示了如何临时覆盖类的方法返回的值，但更典型的情况是，我们仍然希望能够调用原始的被覆盖的函数。可能验证传入的参数或从底层返回的返回值。当我尝试用Mock解决这个问题时，我想到的一般方法如下。 1234567891011121314151617from mock import Mock, patchclass ProductionClass(object): def method(self, a, b, c, key): print a, b, c, keydef wrapper(wrapped): def _wrapper(self, *args, **kwargs): assert args == (3, 4, 5) and kwargs.get('key') == 'value' return wrapped(self, *args, **kwargs) return _wrapper@patch(__name__+'.ProductionClass.method', autospec=True, side_effect=wrapper(ProductionClass.method))def test_method(mock_method): real = ProductionClass() result = real.method(3, 4, 5, key='value') 这里有两个技巧 第一个是@Mock.path 的 autospec=True参数，用于执行方法绑定 第二个是需要在对它应用任何mock之前从’ProductionClass’捕获原始方法，这样当调用mock的副作用函数时，我就可以反过来调用它。 毫无疑问，有人会告诉我，我做错了，有一种更简单的方法，但这是我在阅读模拟文档10分钟后所能想到的最好的方法。 当使用wrapt执行相同的操作时，使用的方式与模拟返回值没有什么不同。这是因为wrapt函数包装器能同时适用普通函数或方法，所以在包装方法时不需要额外处理。此外，当调用wrapt包装函数时，它总是传递被包装的原始函数，因此不需要使用任何魔法来隐藏它。 123456789101112131415from wrapt import transient_function_wrapperclass ProductionClass(object): def method(self, a, b, c, key): print a, b, c, key@transient_function_wrapper(__name__, 'ProductionClass.method')def apply_ProductionClass_method_wrapper(wrapped, instance, args, kwargs): assert args == (3, 4, 5) and kwargs.get('key') == 'value' return wrapped(*args, **kwargs)@apply_ProductionClass_method_wrapperdef test_method(): real = ProductionClass() result = real.method(3, 4, 5, key='value') 使用此功能可以轻松地拦截调用，来执行传递的数据的验证，但仍然可调用原始函数，我可以相对轻松地创建一大堆装饰器，以便对数据执行验证，因为数据可能是通过系统的不同部分传递的。然后，我可以将这些装饰器堆叠在任何需要添加它们的测试函数上。 2. 包装不同类型的返回值返回函数上面的示例包括能够返回一个假的返回值，返回原始值，或者在部分原始数据类型或集合上进行一些轻微的修改。但在某些情况下，我实际上希望在返回值周围放置一个包装器，以修改后续代码与返回值的交互方式。 第一个例子是包装函数返回另一个函数，这个函数将被调用链中更高的函数调用。在这里，我可能想在返回的函数周围放置一个包装器，以便在调用它时拦截它。 Mock 包的使用方式如下1234567891011121314151617181920212223242526from mock import Mock, patchdef function(): passclass ProductionClass(object): def method(self, a, b, c, key): return functiondef wrapper2(wrapped): def _wrapper2(*args, **kwargs): return wrapped(*args, **kwargs) return _wrapper2def wrapper1(wrapped): def _wrapper1(self, *args, **kwargs): func = wrapped(self, *args, **kwargs) return Mock(side_effect=wrapper2(func)) return _wrapper1@patch(__name__+'.ProductionClass.method', autospec=True, side_effect=wrapper1(ProductionClass.method))def test_method(mock_method): real = ProductionClass() func = real.method(3, 4, 5, key='value') result = func() 整个包装过程说明如下: ProductionClass.method 函数返回值是另一个函数 side_effect 指定了第一层的包装函数 wrapper1，截获了ProductionClass.method 返回的 function 函数 wrapper1 将 function 包装再 wrapper2 内返回给了调用链中更高层的函数 更高层的函数调用 function 时，调用的则是 wrapper2 wrapt 包的使用方式: 12345678910111213141516171819202122from wrapt import transient_function_wrapper, function_wrapperdef function(): passclass ProductionClass(object): def method(self, a, b, c, key): return function@function_wrapperdef result_function_wrapper(wrapped, instance, args, kwargs): return wrapped(*args, **kwargs)@transient_function_wrapper(__name__, 'ProductionClass.method')def apply_ProductionClass_method_wrapper(wrapped, instance, args, kwargs): return result_function_wrapper(wrapped(*args, **kwargs))@apply_ProductionClass_method_wrapperdef test_method(): real = ProductionClass() func = real.method(3, 4, 5, key='value') result = func() 整个包装过程说明如下: apply_ProductionClass_method_wrapper 装饰了原始的 ProductionClass.method 方法 apply_ProductionClass_method_wrapper 内 wrapped(*args, **kwargs) 返回结果就是 function，其又被 result_function_wrapper 装饰 调用链中更高层的函数调用 ProductionClass.method，实际调用的是 result_function_wrapper 本例使用了一个名为@wrapt.function_wrapper的新装饰器。还可以使用@wrapt.decorator。@wrapt.function_wrapper 实际上只是@wrapt.decorator的一个简化版本，它缺少一些在做显式的猴子补丁时通常不需要的铃铛和口子，但除此之外，它也可以用同样的方式使用。因此，我可以对结果返回的函数应用一个包装器。我甚至可以应用相同的原理应用在当函数作为参数传递给另一个函数的情况。 返回类的实例返回函数的另一个场景是返回类的实例。在这种情况下，我可能想要对类的实例的特定方法应用一个包装器。在mock 包中，需要再次使用“Mock”类，并且必须以不同的方式应用它来实现您想要的结果。现在我将不再关注mock，只关注wrapt的实现方式。 所以，根据需求，有几种方法可以用wrapt来实现。第一个方法是用封装原始方法的包装器直接替换实例上的方法 123456789101112131415161718192021222324252627from wrapt import transient_function_wrapper, function_wrapperclass StorageClass(object): def run(self): passstorage = StorageClass()class ProductionClass(object): def method(self, a, b, c, key): return storage@function_wrapperdef run_method_wrapper(wrapped, instance, args, kwargs): return wrapped(*args, **kwargs)@transient_function_wrapper(__name__, 'ProductionClass.method')def apply_ProductionClass_method_wrapper(wrapped, instance, args, kwargs): storage = wrapped(*args, **kwargs) storage.run = run_method_wrapper(storage.run) return storage@apply_ProductionClass_method_wrapperdef test_method(): real = ProductionClass() data = real.method(3, 4, 5, key='value') result = data.run() 包装过程是: apply_ProductionClass_method_wrapper 包装了 ProductionClass.method run_method_wrapper 包装 ProductionClass.method 的返回值 storage.run 这样可以得到想要的结果，但在本例中，实际上是一种糟糕的方法。问题是返回的对象是一个在测试之外有生命时间的对象。也就是说，我们正在修改一个存储在全局范围内的对象，该对象可能用于其他测试。通过简单地替换实例上的方法，我们进行了永久性的更改。 如果它是一个仅为一次调用而按需创建的类的临时实例，那么这是可以的，但是在其他情况下不行，因为它的影响是持久的。因此，我们不能修改实例本身，需要以其他方式封装实例来拦截方法调用。 为此，我们使用了所谓的对象代理。这是一个特殊的对象类型，我们可以创建一个实例来包装另一个对象。当访问代理对象时，任何访问属性的尝试都会从包装对象返回属性。类似地，调用代理上的方法将调用包装对象上的方法。 但是，拥有一个不同的代理对象允许我们更改代理对象上的行为，从而更改代码与包装对象的交互方式。因此，我们可以避免更改原始对象本身。因此，对于这个例子，我们可以做的是: 1234567891011121314151617181920212223242526from wrapt import transient_function_wrapper, ObjectProxyclass StorageClass(object): def run(self): passstorage = StorageClass()class ProductionClass(object): def method(self, a, b, c, key): return storageclass StorageClassProxy(ObjectProxy): def run(self): return self.__wrapped__.run()@transient_function_wrapper(__name__, 'ProductionClass.method')def apply_ProductionClass_method_wrapper(wrapped, instance, args, kwargs): storage = wrapped(*args, **kwargs) return StorageClassProxy(storage)@apply_ProductionClass_method_wrapperdef test_method(): real = ProductionClass() data = real.method(3, 4, 5, key='value') result = data.run() 整个包装过程如下: apply_ProductionClass_method_wrapper 包装了 ProductionClass.method 使用代理对象 StorageClassProxy 代理了对 storage 实例属性和方法的访问 StorageClassProxy 覆盖了 storage 的 run 方法 也就是说，我们在代理对象上定义run()方法，以拦截原始对象上相同方法的调用。然后我们可以继续返回假值，验证参数或结果，或者根据需要修改它们。通过代理，我们甚至可以通过向代理对象添加属性来拦截对原始对象属性的访问。 123456789101112131415161718192021222324252627from wrapt import transient_function_wrapper, ObjectProxyclass StorageClass(object): def __init__(self): self.name = 'name'storage = StorageClass()class ProductionClass(object): def method(self, a, b, c, key): return storageclass StorageClassProxy(ObjectProxy): @property def name(self): return self.__wrapped__.name@transient_function_wrapper(__name__, 'ProductionClass.method')def apply_ProductionClass_method_wrapper(wrapped, instance, args, kwargs): storage = wrapped(*args, **kwargs) return StorageClassProxy(storage)@apply_ProductionClass_method_wrapperdef test_method(): real = ProductionClass() data = real.method(3, 4, 5, key='value') assert data.name == 'name' 3. 更好的使用 Mock 模块这时你可能会说Mock做的远不止这些。你甚至可能想指出 mock 如何保存了调用的细节，这样就可以回溯，而不需要进行打点测试，这样甚至可以避免打点测试触发的异常被意外捕获的情况。 这是正确的，我们的意思是不要局限于使用基本的构建块本身，可以将多个模块结合使用，wrapt 是构建更好的模拟库进行测试的一个很好的基础。因此，我留给你们最后一个例子来让你们思考，如何使用 mock 来实现。 12345678910111213141516171819202122232425262728from wrapt import transient_function_wrapperclass ProductionClass(object): def method(self, a, b, c, key): passdef patch(module, name): def _decorator(wrapped): class Wrapper(object): @transient_function_wrapper(module, name) def __call__(self, wrapped, instance, args, kwargs): self.args = args self.kwargs = kwargs return wrapped(*args, **kwargs) wrapper = Wrapper() @wrapper def _wrapper(): return wrapped(wrapper) return _wrapper return _decorator@patch(__name__, 'ProductionClass.method')def test_method(mock_method): real = ProductionClass() result = real.method(3, 4, 5, key='value') assert real.method.__name__ == 'method' assert mock_method.args == (3, 4, 5) assert mock_method.kwargs.get('key') == 'value' 这是 wrapt 包实现猴子补丁的概览。还有一些其他的东西，但这是核心部分。我使用猴子补丁将工具添加到现有代码中以支持性能监视，但是我在这里展示了如何将相同的技术用于编写代码测试，以替代Mock等包。 正如我在上一篇文章中提到的，猴子补丁的一个主要问题是模块的导入结果与打补丁完成的时间相关。我将在下一篇文章中进一步讨论这个问题。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"wrapt","slug":"wrapt","permalink":"http://yoursite.com/tags/wrapt/"}]},{"title":"11 在 Python 中安全的使用猴子补丁","slug":"wrapt/python_decorator_11","date":"2018-06-01T16:00:00.000Z","updated":"2020-05-24T15:31:22.919Z","comments":true,"path":"2018/06/02/wrapt/python_decorator_11/","link":"","permalink":"http://yoursite.com/2018/06/02/wrapt/python_decorator_11/","excerpt":"在之前 10 篇博客中，我们几乎完整的讨论了装饰器的实现。现在我们将焦点从装饰器转移到猴子补丁上来。","text":"在之前 10 篇博客中，我们几乎完整的讨论了装饰器的实现。现在我们将焦点从装饰器转移到猴子补丁上来。 1. 猴子补丁通常在Python中永远不应该做的事情之一就是编写猴子补丁。但有些人认为这是一种有用的必需品，你可能无法避免修补第三方代码中的错误。其他人则可能会争辩说，现在有这么多的软件是开源的，所以您应该简单地向上游包维护人员提交一个补丁。 猴子补丁除了补丁还有其他用途。在Python中最常用的两种形式的猴子补丁是装饰器和使用模拟库来帮助执行单元测试，甚至你可能不把它与猴子补丁等同起来。另一个不常见的猴子补丁的例子是对现有的Python代码添加性能监视功能。 前面我们介绍了装饰器可能会导致什么问题。主要的问题就是，装饰器的实现方式可能没有保留适当的自省能力，当应用于类的方法时，它们可能也没有保留Python描述符协议的正确语义。当人们开始讨论如何修改任意代码，而不是简单地对自己的代码应用装饰器时，这两个问题就变得更加重要了，因为可能很容易地干扰现有代码的行为，或者以意想不到的方式打补丁。 典型的案例是，对一个类方法打补丁。与装饰器在类被创建时即运行不同，补丁代码运行时，类已经被创建，因此需要额外处理一些潜在问题。 我打算用这篇博文来解释wrapt包的猴补丁功能。尽管 wrapt 模块提供了创建装饰器的良好方式，但这并不是创建该包的主要目标。创建wrapt包的真正原因实际上是为猴子补丁代码实现健壮的机制。碰巧，安全执行猴子补丁所需的基本原则和机制也适用于实现装饰器。 2. 创建一个装饰器在开始修改任意代码之前，我们首先需要重新复述一下wrapt包如何用于创建装饰器。主要模式是: 12345678910111213141516171819import wraptimport inspect@wrapt.decoratordef universal(wrapped, instance, args, kwargs): if instance is None: if inspect.isclass(wrapped): # Decorator was applied to a class. return wrapped(*args, **kwargs) else: # Decorator was applied to a function or staticmethod. return wrapped(*args, **kwargs) else: if inspect.isclass(instance): # Decorator was applied to a classmethod. return wrapped(*args, **kwargs) else: # Decorator was applied to an instancemethod. return wrapped(*args, **kwargs) wrapt包创建装饰器的一个特性是，在装饰器中，可以确定装饰器所使用的上下文。即可以确定修饰符是被应用于类、函数或静态方法、类方法或实例方法中的哪一个。对于将装饰器应用于实例方法的情况，为类的实例提供了一个单独的参数。对于类方法，单独的参数是对类本身的引用。在这两种情况下，它们都与“args”和“kwargs”参数相分离，因此不需要自己动手提取它们。因此，我将使用wrapt创建的装饰器称为通用装饰器。换句话说，可以创建一个单独的装饰器，它可以跨函数、方法和类使用，可以在不同的调用场景中相应地调整装饰器的行为。而不再需要创建一个装饰器的多个实现，并确保在每个场景中都使用了正确的实现。 这种装饰器的使用与其他方式创建的装饰器无异。 12345class Example(object): @universal def name(self): return 'name' 需要注意的是 @ 符应用一个装饰器在Python2.4 中被加入。它仅仅是如下方式的语法糖 12345class Example(object): def name(self): return 'name' name = universal(name) 这么写仍然可行，当以这种方式编写时，它使装饰者在某种程度上成为一种猴子补丁。这是因为猴子补丁通常所做的就是在一些现有函数周围引入一个包装器，这样就可以对原始函数进行拦截。然后，包装器函数允许在调用原始函数之前或之后执行操作，或者允许修改传递给包装函数的参数，或者以某种方式修改结果，或者甚至完全替换结果。 与装饰器的一个重要区别是，装饰器在类被创建时即运行。相比之下，猴子补丁更随意，通常在类创建一段时间之后再执行。 事实上你所作的是: 12345class Example(object): def name(self): return 'name'Example.name = universal(Example.name) 尽管使用wrapt包创建的装饰器函数可以以这种方式使用，并且仍将按预期工作，但总体而言，我不建议以这种模式给类的现有方法添加补丁。这是因为这种方式实际上并不等同于当类被定义时在类的主体内做同样的事情。特别是Example.name的访问实际上调用了描述符协议，因此返回了实例方法。我们可以通过运行代码看到这一点: 12345678910class Example(object): def name(self): return 'name' print type(name)print type(Example.name)which produces:&lt;type 'function'&gt;&lt;type 'instancemethod'&gt; 一般来说，这可能并不重要，但我看到过一些非常奇怪的情况，它们的区别很重要。因此，为了解决这个问题，wrapt包提供了执行猴子补丁的另一种实现机制。在上面为类的方法添加包装器的情况下，使用这种机制可以避免由这种细微差别所引起的任何问题。 3. 猴子补丁创建猴子补丁的创建与装饰器创建类似，首先需要创建一个包装函数，猴子补丁的包装函数与装饰器是一样的，如下图所示 12def wrapper(wrapped, instance, args, kwargs): return wrapped(*args, **kwargs) 不同的是不是使用装饰器工厂函数 @wrapt.decorator 创建装饰器并将其应用到被包装对象上，而是使用 wrapt.wrap_function_wrapper() 函数。 1234567class Example(object): def name(self): return 'name'import wraptwrapt.wrap_function_wrapper(Example, 'name', wrapper) 在这种情况下，我们将类放在同一个代码文件中，但是我们也可以这样做:** 1234import exampleimport wraptwrapt.wrap_function_wrapper(example, 'Example.name', wrapper) 也就是说，我们将目标所在的模块作为第一参数，第二个参数则是我们希望应用包装器的目标方法对象的路径。我们也可以完全跳过导入模块，只使用模块的名称。 123import wraptwrapt.wrap_function_wrapper('example', 'Example.name', wrapper) 为了证明任何东西都可以被装饰器简化，我们最终可以把整个东西写成: 12345import wrapt@wrapt.patch_function_wrapper('example', 'Example.name')def wrapper(wrapped, instance, args, kwargs): return wrapped(*args, **kwargs) 在这个最后的示例中，将会发生的事情是，一旦导入了包含上述代码的模块，在“示例”模块中定义的指定目标函数将自动地使用包装器函数进行修补。 4. 延迟补丁问题现在需要着重提醒的是。在上述的操作之后应用补丁并不总是有效的。 问题的核心在于，是否正在对一个已导入的模块应用补丁。如果模块没有导入，wrap .wrap_function_wrapper() 调用将确保模块被导入，但是如果模块已经被代码的其他部分或第三方包导入，那么可能就会有问题。 特别的是，您尝试打补丁的目标函数是模块的一个正常的全局函数，其他一些代码可以通过以下步骤直接获取对它的引用: from example import function 如果你后来来了 12345import wrapt@wrapt.patch_function_wrapper('example', 'function')def wrapper(wrapped, instance, args, kwargs): return wrapped(*args, **kwargs) 最后，目标模块中包含的函数的副本将应用包装器，但是其他代码创建的对它的引用将没有包装器。即在打补丁之后导入的目标函数都是被包装的，之前的都是未被包装的。 为了确保在此场景中始终使用包装器，您不仅需要在原始模块中，而且还需要在存储引用的任何模块中对其进行补丁。这只在非常有限的情况下是可行的因为在现实中，如果函数是一个普通的函数，你将不知道函数在哪里被使用。 这个问题的一个确切体现就是对gevent或eventlet等包打补丁时存在的问题。这两个包都延迟了功能的修补，因此对导入模块的顺序非常敏感。要解决这个问题，至少对于Python标准库中的模块来说，要打补丁的time.sleep()函数不仅需要在time模块中进行修补，还需要在threading模块中进行修补。 有一些技术可以用来尝试和避免这些问题，但我将把这些解释推迟到以后的一段时间。在我的下一篇博客文章中，我想介绍一些使用使用猴子补丁示例，看看如何在进行测试时使用wrapt替代 mock 模块。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"wrapt","slug":"wrapt","permalink":"http://yoursite.com/tags/wrapt/"}]},{"title":"10 装饰类的性能","slug":"wrapt/python_decorator_10","date":"2018-05-31T16:00:00.000Z","updated":"2020-05-24T15:31:17.939Z","comments":true,"path":"2018/06/01/wrapt/python_decorator_10/","link":"","permalink":"http://yoursite.com/2018/06/01/wrapt/python_decorator_10/","excerpt":"在上一篇文章中，我们对作为函数闭包实现的装饰器与前文描述的通用装饰器进行了性能比较。本节我们继续我们的性能测试，看看装饰一个类方法时，不同实现方式的性能表现。","text":"在上一篇文章中，我们对作为函数闭包实现的装饰器与前文描述的通用装饰器进行了性能比较。本节我们继续我们的性能测试，看看装饰一个类方法时，不同实现方式的性能表现。 1. 装饰函数的性能比较在上一篇文章中，函数闭包实现的装饰器与前文描述的通用装饰器性能测试结果如下 对于2012年的MacBook Pro，直接调用函数的测试结果是: 10000000 loops, best of 3: 0.132 usec per loop 使用函数闭包实现的装饰器的测试结果是: 1000000 loops, best of 3: 0.326 usec per loop 最受，使用装饰器工厂函数的测试结果是: 1000000 loops, best of 3: 0.771 usec per loop 上述是代理对象，和 function wrapper 对象的Python实现测试结果，如果将它们以Python C扩展实现，可以降低至: 1000000 loops, best of 3: 0.382 usec per loop 这与使用函数闭包实现的装饰器，性能相差无几。 将装饰器应用在类方法会怎样？ 2. 必须绑定函数的开销将装饰器应用于类的方法的问题是，如果要遵守Python执行模型，则需要将装饰器实现为描述符，并在访问时正确地将方法绑定到类或类实例。在本系列文章中描述的装饰器中，我们正是实现了此机制，以便能够确定装饰器整被应用于与普通的函数、实例方法或类方法中的哪一个。 相比于使用函数闭包实现的装饰器不会遵守任何的Python 执行模型，这个绑定过程确保了正确的操作，但是也带来了额外的开销。为了查看发生了哪些额外的步骤，我们可以再次使用Python profile挂钩机制来跟踪修饰函数调用的执行。当前即跟踪实例方法的调用 首先，让我们来跟踪函数闭包实现的装饰器调用了哪些函数: 1234567891011121314151617181920def my_function_wrapper(wrapped): def _my_function_wrapper(*args, **kwargs): return wrapped(*args, **kwargs) return _my_function_wrapperclass Class(object): @my_function_wrapper def method(self): passinstance = Class()import sysdef tracer(frame, event, arg): print(frame.f_code.co_name, event)sys.setprofile(tracer)instance.method() 结果跟装饰器一个普通函数类似: 1234_my_function_wrapper call method call method return_my_function_wrapper return 因此，我们应该预期，当我们执行实际的时间测试时，开销不会有很大的不同。现在使用我们的装饰器工厂函数。为了提供上下文，我展示了完整的代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677class object_proxy(object): def __init__(self, wrapped): self.wrapped = wrapped try: self.__name__ = wrapped.__name__ except AttributeError: pass @property def __class__(self): return self.wrapped.__class__ def __getattr__(self, name): return getattr(self.wrapped, name)class bound_function_wrapper(object_proxy): def __init__(self, wrapped, instance, wrapper, binding, parent): super(bound_function_wrapper, self).__init__(wrapped) self.instance = instance self.wrapper = wrapper self.binding = binding self.parent = parent def __call__(self, *args, **kwargs): if self.binding == 'function': if self.instance is None: instance, args = args[0], args[1:] wrapped = functools.partial(self.wrapped, instance) return self.wrapper(wrapped, instance, args, kwargs) else: return self.wrapper(self.wrapped, self.instance, args, kwargs) else: instance = getattr(self.wrapped, '__self__', None) return self.wrapper(self.wrapped, instance, args, kwargs) def __get__(self, instance, owner): if self.instance is None and self.binding == 'function': descriptor = self.parent.wrapped.__get__(instance, owner) return bound_function_wrapper(descriptor, instance, self.wrapper, self.binding, self.parent) return selfclass function_wrapper(object_proxy): def __init__(self, wrapped, wrapper): super(function_wrapper, self).__init__(wrapped) self.wrapper = wrapper if isinstance(wrapped, classmethod): self.binding = 'classmethod' elif isinstance(wrapped, staticmethod): self.binding = 'staticmethod' else: self.binding = 'function' def __get__(self, instance, owner): wrapped = self.wrapped.__get__(instance, owner) return bound_function_wrapper(wrapped, instance, self.wrapper, self.binding, self) def __call__(self, *args, **kwargs): return self.wrapper(self.wrapped, None, args, kwargs)def decorator(wrapper): def _wrapper(wrapped, instance, args, kwargs): def _execute(wrapped): if instance is None: return function_wrapper(wrapped, wrapper) elif inspect.isclass(instance): return function_wrapper(wrapped, wrapper.__get__(None, instance)) else: return function_wrapper(wrapped, wrapper.__get__(instance, type(instance))) return _execute(*args, **kwargs) return function_wrapper(wrapper, _wrapper) 我们的装饰器实现如下: 123@decoratordef my_function_wrapper(wrapped, instance, args, kwargs): return wrapped(*args, **kwargs) 装饰实例方法的测试输出结果如下: 12345678910111213('__get__', 'call') # function_wrapper ('__init__', 'call') # bound_function_wrapper ('__init__', 'call') # object_proxy ('__init__', 'return') ('__init__', 'return')('__get__', 'return')('__call__', 'call') # bound_function_wrapper ('my_function_wrapper', 'call') ('method', 'call') ('method', 'return') ('my_function_wrapper', 'return')('__call__', 'return') 可以看到，由于方法与发生在 __get__() 中的类实例的绑定，现在发生了很多事情。因此，开销也会显著增加。 3. 执行类方法的开销与前面一样，不再使用上面的实现，而是再次使用wrapt库中的实际实现。这次我们的测试代码是: $ python -m timeit -s &#39;import benchmarks; c=benchmarks.Class()&#39; &#39;c.method()&#39; 没有被装饰的实例方法，直接运行的结果是: 10000000 loops, best of 3: 0.143 usec per loop 这比普通函数调用的情况要多一点，因为发生的了实例方法的绑定。 使用函数闭包实现的装饰器。测试结果如下: 1000000 loops, best of 3: 0.382 usec per loop 再一次，比未修饰的情况稍微多一点，与被应用到函数的装饰器相差无几。因此，当应用于普通函数与实例方法时，装饰器的开销并没有太大的差异。现在轮到我们的装饰器工厂函数和 function wrapper对象。首先测试Python 实现: 100000 loops, best of 3: 6.67 usec per loop 与使用函数闭包实现装饰器相比，这在运行时开销上增加了不少负担。虽然每次执行只需要额外的6个usec，但是您需要在上下文中考虑这个问题。特别是，如果在处理web请求的过程中对一个调用了1000次的函数应用了这样的装饰器，那么在该web请求的响应时间之上增加了6 ms。 在这一点上，许多人无疑会辩称，如果运行成本太高，那么正确是不值得的。但是，装饰函数和装饰器本身也不可能什么都不做，因此所产生的额外开销可能只是运行时成本的一小部分，因此在实践中并不明显。同样的，如果使用Python C扩展模块实现呢？对于作为C扩展实现的对象代理和函数包装器，结果是: 1000000 loops, best of 3: 0.836 usec per loop 所以不是6ms，而是小于1ms的额外开销如果修饰函数被调用1000次。它仍然比使用作为函数闭包实现的装饰器要多，但再次重申，在修饰类的方法时使用函数闭包不符合Python执行模型。 4. 需要大费周折么我是在吹毛求疵、过于迂腐地想把事情做好吗？当然，对于你现在所使用的装饰器，闭包实现可能工作的很好。但是当您开始使用函数包装器执行任意代码的猴子补丁时，情况就不一样了。如果你在做猴子补丁时不遵守Python的执行模型，那么你很容易以非常微妙和晦涩的方式打破第三方代码。客户可不会喜欢你破坏了他们的web应用程序。所以至少我现在所作的是很重要的。 在本文中，我只考虑了修饰类实例方法时的开销。我没有涵盖在修饰静态方法和类方法时的开销。如果您对它们的不同之处感到好奇，您可以在wrapt文档中查看完整的案例的基准。 在下一篇文章中，我将再次讨论性能开销问题，但也将讨论实现装饰器的一些替代方法，以便尝试并解决我在第一篇文章中提出的问题。这些内容将作为，对博客中描述的实现和 PyPi 模块中的实现的对比的一部分。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"wrapt","slug":"wrapt","permalink":"http://yoursite.com/tags/wrapt/"}]},{"title":"09 装饰器性能比较","slug":"wrapt/python_decorator_09","date":"2018-05-29T16:00:00.000Z","updated":"2020-05-24T15:31:12.738Z","comments":true,"path":"2018/05/30/wrapt/python_decorator_09/","link":"","permalink":"http://yoursite.com/2018/05/30/wrapt/python_decorator_09/","excerpt":"前面我们探讨了装饰器的实现方式，并实现了一个所谓的通用装饰器模式，并用它创建了一个类似 Java 的 @synchronized 装饰器作为使用示例。本节我们来看看不同的装饰器实现方式的性能问题。在这篇关于装饰器的实现性能这篇文章之后，我们将开始深入探讨如何实现代理，它是通用装饰器机制中的基础组件。","text":"前面我们探讨了装饰器的实现方式，并实现了一个所谓的通用装饰器模式，并用它创建了一个类似 Java 的 @synchronized 装饰器作为使用示例。本节我们来看看不同的装饰器实现方式的性能问题。在这篇关于装饰器的实现性能这篇文章之后，我们将开始深入探讨如何实现代理，它是通用装饰器机制中的基础组件。 1. 装饰一个普通函数在这篇文章中，我将只讨论用装饰器修饰一个普通函数的开销。相关的装饰器代码如下: 123456789101112131415161718192021222324class function_wrapper(object_proxy): def __init__(self, wrapped, wrapper): super(function_wrapper, self).__init__(wrapped) self.wrapper = wrapper ... def __get__(self, instance, owner): ... def __call__(self, *args, **kwargs): return self.wrapper(self.wrapped, None, args, kwargs) def decorator(wrapper): def _wrapper(wrapped, instance, args, kwargs): def _execute(wrapped): if instance is None: return function_wrapper(wrapped, wrapper) elif inspect.isclass(instance): return function_wrapper(wrapped, wrapper.__get__(None, instance)) else: return function_wrapper(wrapped, wrapper.__get__(instance, type(instance))) return _execute(*args, **kwargs) return function_wrapper(wrapper, _wrapper) 如果你想回忆完整的代码，你可以去查看之前的文章，那里有完整描述。使用装饰器工厂函数，创建装饰器，并装饰器一个普通函数可以像下面这样: 1234567@decoratordef my_function_wrapper(wrapped, instance, args, kwargs): return wrapped(*args, **kwargs) @my_function_wrapperdef function(): pass 这与使用函数闭包以更传统的方式创建的decorator不同。使用闭包创建一个函数装饰器如下所示: 12345678def my_function_wrapper(wrapped): def _my_function_wrapper(*args, **kwargs): return wrapped(*args, **kwargs) return _my_function_wrapper@my_function_wrapperdef function(): pass 在我们调用函数时function()，这两种情况各自会发生什么? 2. 追踪函数执行为了跟踪代码的执行，我们可以使用Python的profile hook机制。 1234567import sysdef tracer(frame, event, arg): print(frame.f_code.co_name, event)sys.setprofile(tracer)function() profile hook的目的是允许注册一个回调函数，该函数在所有函数的入口和出口调用。这样就可以追踪正在进行的函数调用的序列。对于函数闭包，输出如下: 1234_my_function_wrapper call function call function return_my_function_wrapper return 我们在这里看到的是函数闭包的嵌套函数被调用。这是因为在使用函数闭包的情况下，装饰器将函数替换为对嵌套函数的引用。当这个嵌套函数被调用时，它将依次调用原来的包装函数。对于我们的工厂函数，输出如下: 123456__call__ call my_function_wrapper call function call function return my_function_wrapper return__call__ return 这里的区别是，decorator 用 function wrapper 类的实例替换了函数。作为一个类，当它作为一个函数被调用时，__call__() 方法在类的实例上被调用。__call__() 方法随后调用用户提供的包装器函数，该函数反过来调用原始包装函数。 因此，结果是我们引入了额外的间接级别，或者换句话说，在执行路径中引入了额外的函数调用。记住，__call__()实际上是一个方法，而不仅仅是一个普通的函数。作为一种方法，实际上在幕后进行的工作要比普通的函数调用多得多。特别是，在调用未绑定方法之前，需要将其绑定到函数包装器类的实例。这不会出现在调用的跟踪中，但是它正在发生，并且会产生额外的开销。 3. 函数执行时间通过执行上面的跟踪，我们知道我们的解决方案会带来额外的方法调用开销。但是这会产生多少额外的开销呢？为了尝试度量每个解决方案中开销的增加，我们可以使用timeit模块来执行我们的函数调用。作为基线，我们首先需要知道在不应用任何修饰符的情况下对函数进行调用的时间开销。 123# benchmarks.pydef function(): pass 为记录时间，我们需要使用以下命令: $ python -m timeit -s &#39;import benchmarks&#39; &#39;benchmarks.function()&#39; 以这种方式使用的timeit模块时，它将执行适当的大量函数调用，将所有调用的总时间除以调用次数，最后得到单个调用的时间值。对于2012年款的MacBook Pro来说，输出如下: 10000000 loops, best of 3: 0.132 usec per loop 接下来测试函数闭包，输出如下: 1000000 loops, best of 3: 0.326 usec per loop 最后测试我们的装饰器工厂函数: 1000000 loops, best of 3: 0.771 usec per loop 在这个最后的例子中，我使用的是wrapt模块实现，而不是本系列博文中迄今为止给出的代码。这个实现的工作方式略有不同，因为它在描述的内容上有一些额外的功能，设计也有一些不同。即便是最轻量级的实现，性能开销也差不多。 4. 加速包装器的执行在这一点上毫无疑问会有人们想要指出,即使对于方法调用而言，它更加正确的实现了描述符协议，但是这所谓的的更好的方法实在是太慢，难以在实际生产环境中使用。因此，是否可以做些什么来加速实现呢? 此时可以采用的方法是将函数包装器和对象代理实现为Python C扩展模块。为了简单起见，我们可以将装饰器工厂函数本身作为纯Python代码来实现，因为工厂函数只在修饰符应用到函数时才调用，而不是修饰函数的每次调用时都会调用，因此它的时间开销并不重要。** 我绝对不会做的一件事是写博客，讨论如何将函数包装器和对象代理作为Python C扩展模块实现。不过请放心，它的工作方式与纯Python实现相同。显然，它的运行速度要快得多，因为它是使用Python C api实现的C代码，而不是纯粹的Python代码。 将函数包装器和对象代理作为Python C扩展模块实现的开销如何呢?测试如下: 1000000 loops, best of 3: 0.382 usec per loop 因此，尽管将函数包装器和对象代理作为Python C扩展模块实现需要付出更多的努力，但这些努力是值得的，结果时现在非常接近使用函数闭包的装饰器实现。 4. 装饰类方法性能到目前为止，我们只考虑了装饰一个普通函数的情况。正如预期的那样，与function wrapper作为一个类实现类似，由于引入了额外的间接层，因此开销明显更多。尽管如此，它仍然只有半微秒。 尽管如此，通过实现我们的函数包装器和对象代理作为C代码，我们还是能够将性能达到同一量级，在这里，作为函数闭包实现的装饰器工厂函数的开销可以忽略不计。 那么装饰类方法的性能如何呢。将在下一篇博客揭晓。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"wrapt","slug":"wrapt","permalink":"http://yoursite.com/tags/wrapt/"}]},{"title":"08 将 @synchronized 实现为上下文管理器","slug":"wrapt/python_decorator_08","date":"2018-05-28T16:00:00.000Z","updated":"2020-05-24T15:31:07.443Z","comments":true,"path":"2018/05/29/wrapt/python_decorator_08/","link":"","permalink":"http://yoursite.com/2018/05/29/wrapt/python_decorator_08/","excerpt":"在前一篇文章中，我们描述了如何使用新的通用装饰器模式来实现Python的 @synchronized 同步原语装饰器。在Java提供的两个同步机制中，同步方法和同步原语，目前为止我们只实现了同步方法。本文将描述如何将其扩展为上下文管理器，从而等效的实现Java的同步原语。","text":"在前一篇文章中，我们描述了如何使用新的通用装饰器模式来实现Python的 @synchronized 同步原语装饰器。在Java提供的两个同步机制中，同步方法和同步原语，目前为止我们只实现了同步方法。本文将描述如何将其扩展为上下文管理器，从而等效的实现Java的同步原语。 1. @synchronized 当前实现到目前为止，我们的@synchronized 装饰器的实现是。 123456789101112131415161718192021@decoratordef synchronized(wrapped, instance, args, kwargs): if instance is None: owner = wrapped else: owner = instance lock = vars(owner).get('_synchronized_lock', None) if lock is None: meta_lock = vars(synchronized).setdefault( '_synchronized_meta_lock', threading.Lock()) with meta_lock: lock = vars(owner).get('_synchronized_lock', None) if lock is None: lock = threading.RLock() setattr(owner, '_synchronized_lock', lock) with lock: return wrapped(*args, **kwargs) 通过确定装饰器被用于包装普通函数、实例方法或类的方法中的哪一个，我们可以在许多场景中使用同一一个装饰器。 123456789101112131415161718192021222324@synchronized # lock bound to function1def function1(): pass@synchronized # lock bound to function2def function2(): pass@synchronized # lock bound to Classclass Class(object): @synchronized # lock bound to instance of Class def function_im(self): pass @synchronized # lock bound to Class @classmethod def function_cm(cls): pass @synchronized # lock bound to function_sm @staticmethod def function_sm(): pass 我们现在想要实现的是让同步装饰器也能完成如下操作: 123456789class Object(object): @synchronized def function_im_1(self): pass def function_im_2(self): with synchronized(self): pass 也就是说，除了可以用作装饰器之外，它还能与with语句一起用作上下文管理器。通过这样做，它就能够对函数中的部分语句加锁，而不是整个函数。用作上下文管理器时，如果需要与实例方法同步，我们需要将把self参数或类实例传递给synchronized。如果需要与类方法同步，则传递类对象本身。 2. 将 function_wrapper 实现为上下文管里器在现有的synchronized实现上，当使用synchronized作为函数调用时，它将返回函数包装器类的一个实例。 12&gt;&gt;&gt; synchronized(None)&lt;__main__.function_wrapper object at 0x107b7ea10&gt; 这个函数包装器没有实现作为上下文管理器的对象所需的__enter__()和__exit__()函数。函数包装器是我们自己的类，所以我们只需要创建子类并为其添加这两个方法即可。同时这个函数包装器的创建是在@decorator的定义中绑定的，所以我们需要绕过@decorator并直接使用函数包装器。因此，第一步是重写我们的 @synchronized decorator，不使用@decorator。 123456789101112131415161718192021def synchronized(wrapped): def _synchronized_lock(owner): lock = vars(owner).get('_synchronized_lock', None) if lock is None: meta_lock = vars(synchronized).setdefault( '_synchronized_meta_lock', threading.Lock()) with meta_lock: lock = vars(owner).get('_synchronized_lock', None) if lock is None: lock = threading.RLock() setattr(owner, '_synchronized_lock', lock) return lock def _synchronized_wrapper(wrapped, instance, args, kwargs): with _synchronized_lock(instance or wrapped): return wrapped(*args, **kwargs) return function_wrapper(wrapped, _synchronized_wrapper) 这与我们最初的实现相同，但是我们现在可以访问到创建函数包装器对象 function_wrapper。因此我们可以创建一个满足上下文管里器协议的 function_wrapper 的子类来替换 function_wrapper。 12345678910111213141516171819202122232425262728293031def synchronized(wrapped): def _synchronized_lock(owner): lock = vars(owner).get('_synchronized_lock', None) if lock is None: meta_lock = vars(synchronized).setdefault( '_synchronized_meta_lock', threading.Lock()) with meta_lock: lock = vars(owner).get('_synchronized_lock', None) if lock is None: lock = threading.RLock() setattr(owner, '_synchronized_lock', lock) return lock def _synchronized_wrapper(wrapped, instance, args, kwargs): with _synchronized_lock(instance or wrapped): return wrapped(*args, **kwargs) class _synchronized_function_wrapper(function_wrapper): def __enter__(self): self._lock = _synchronized_lock(self.wrapped) self._lock.acquire() return self._lock def __exit__(self, *args): self._lock.release() return _synchronized_function_wrapper(wrapped, _synchronized_wrapper) 3. 两种调用方式当 synchronized 作为装饰器使用时，新的function wrapper子类被用于包装被包装函数和方法。当函数或类方法被调用时，function wrapper 基类中的 __call__ 方法被调用。装饰器将在尝试获取锁之后执行被包装函数。 当synchronized作为上下文管里器使用时。子类将用于包装类实例或类本身。没有方法会被调用，取而代之的是在进入上下文时，__enter__() 会获取锁，离开上下文时，__exit__() 会释放锁。 与在之前的文章中形容的复杂度相比，现在的实现简单明了。 4. 不只是个装饰器希望这能说明的一点是，尽管@decorator被用来创建自定义装饰器，但这并不总是最合适的方式。function wrapper 对象的单独存在为修改被包装对象的行为提供了很大的灵活性。在某些情况下，还可以直接删除和使用对象代理。所有这些都提供了一个通用的工具集，用于进行任何类型的包装或修补，而不仅仅是用于装饰。现在，我将开始将这一系列博客文章的焦点转移到更一般的包装和猴子补丁上。 在此之前，在下一篇文章中，我将首先讨论与使用函数闭包实现装饰器的更传统方式相比，使用 function wrapper 隐含的性能影响。以及使用Python C扩展实现完整的对象代理和 function wrapper 后，性能改善的大小。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"wrapt","slug":"wrapt","permalink":"http://yoursite.com/tags/wrapt/"}]},{"title":"07 实现 java 的 @synchronized 装饰器","slug":"wrapt/python_decorator_07","date":"2018-05-25T16:00:00.000Z","updated":"2020-05-24T15:31:03.129Z","comments":true,"path":"2018/05/26/wrapt/python_decorator_07/","link":"","permalink":"http://yoursite.com/2018/05/26/wrapt/python_decorator_07/","excerpt":"在之前的博客中，我们讨论了装饰器的实现，并实现了一个通用装饰器模式。作为这种模式的使用示例，本节我们来实现 java 中的 @synchronized 装饰器。","text":"在之前的博客中，我们讨论了装饰器的实现，并实现了一个通用装饰器模式。作为这种模式的使用示例，本节我们来实现 java 中的 @synchronized 装饰器。 1. Java @synchronized 装饰器java 的同步原语有两种形式，分别是同步方法和同步代码块。在Java 中创建同步方法，只需要在其定义时添加synchronized关键字即可。 123456789101112public class SynchronizedCounter &#123; private int c = 0; public synchronized void increment() &#123; c++; &#125; public synchronized void decrement() &#123; c--; &#125; public synchronized int value() &#123; return c; &#125;&#125; 使一个方法同步意味着不可能在同一个对象上同时调用多个同步方法。当一个线程正在执行一个对象的同步方法时，所有其他调用相同对象的同步方法的线程将阻塞直至当前同步方法调用完成。 换句话说，类的每个实例都有一个内在的锁对象，并且在进入一个方法时，锁会被获取，当方法返回时它会被释放。锁是所谓的重入锁，这意味着线程可以在它持有锁的同时，再次获得它，而不会阻塞。正因为如此，一个同步的方法可以调用同一个对象上的另一个同步方法。 在Java中创建同步代码的第二种方法是同步代码块。与同步方法不同，同步代码块必须指定提供内在锁的对象。 1234567public void addName(String name) &#123; synchronized(this) &#123; lastName = name; nameCount++; &#125; nameList.add(name);&#125; 值得注意的是，在Java中，可以使用任何对象作为锁的源，不需要创建特定锁类型的实例来同步。如果在类中需要更细粒度的锁，那么可以简单地创建或使用现有的任意对象进行同步。 12345678910111213141516public class MsLunch &#123; private long c1 = 0; private long c2 = 0; private Object lock1 = new Object(); private Object lock2 = new Object(); public void inc1() &#123; synchronized(lock1) &#123; c1++; &#125; &#125; public void inc2() &#123; synchronized(lock2) &#123; c2++; &#125; &#125;&#125; 这些同步原语使用起来相对简单，因此，如何才能通过装饰器在Python中让类似操作以同样简单的方式实现呢。 2.同步线程的互斥锁在Python中，不可能使用任意对象做同步。相反必要创建一个特定的锁对象，该对象内部持有一个线程互斥锁。锁对象提供了一个 acquire()和release() 方法来操作锁。同时由于上下文管理器被引入到 Python 中，所以锁也支持与with语句一起使用。使用这个特定的特性，用于实现Python的@synchronized 装饰器的典型实现是: 1234567891011121314def synchronized(lock=None): def _decorator(wrapped): @functools.wraps(wrapped) def _wrapper(*args, **kwargs): with lock: return wrapped(*args, **kwargs) return _wrapper return _decoratorlock = threading.RLock()@synchronized(lock)def function(): pass 使用此方法在一段时间后变得很烦人，因为对于需要同步的每个不同的函数，必须首先创建一个线程锁。替代方法是，为每个装饰器自动创建一个线程锁。 1234567891011def synchronized(wrapped): lock = threading.RLock() @functools.wraps(wrapped) def _wrapper(*args, **kwargs): with lock: return wrapped(*args, **kwargs) return _wrapper@synchronizeddef function(): pass 我们甚至可以使用前面描述的模式，为每次调用提供一个可选的参数 1234567891011121314151617181920def synchronized(wrapped=None, lock=None): if wrapped is None: return functools.partial(synchronized, lock=lock) if lock is None: lock = threading.RLock() @functools.wraps(wrapped) def _wrapper(*args, **kwargs): with lock: return wrapped(*args, **kwargs) return _wrapper@synchronizeddef function1(): passlock = threading.Lock()@synchronized(lock=lock)def function2(): pass 无论方法如何，基于函数闭包的装饰器都会遇到我们已经列出的所有问题。因此，我们可以采取的第一步是使用我们新的装饰器工厂函数替代它。 12345678910111213def synchronized(wrapped=None, lock=None): if wrapped is None: return functools.partial(synchronized, lock=lock) if lock is None: lock = threading.RLock() @decorator def _wrapper(wrapped, instance, args, kwargs): with lock: return wrapped(*args, **kwargs) return _wrapper(wrapped) 因为使用了我们的装饰器工厂函数，这意味着相同的代码可以安全的应在实例、类或静态方法上。需要强调的是在类方法上使用此装饰器看似简单，但并不是很有用。因为锁仅仅对被装饰的方法有用，并且会对类的所有实例在同一方法上施加同步锁。这并不是我们想要的，也不能同java的同步方法相对应。 在次重申我们要实现的目标是，被装饰器标识为同步的所有实例方法，我们希望每个类实例都有一个独立的同步锁来实现实例内的方法同步。不同类实例之间不要同步。 过去已经有一些文章描述了如何改进这一点，包括这个很复杂的尝试。个人觉得它的实现方式是相当笨拙的，甚至怀疑它实际上不是线程安全的，因为在创建一些锁的过程中有一个竞争条件。因为它使用了函数闭包，并且没有我们的通用装饰器的概念，所以还需要创建大量不同的装饰器，然后在一个装饰器入口点上尝试将它们整合在一起。显然，我们现在应该能够做得更好。 3. 将互斥锁存储在被包装对象上解决这个问题的关键在于我们可以在哪里存储线程锁。在被包装对象调用之间存储任何数据的惟一选项将是被包装对象本身，包括被包装的函数，类实例方法和类方法。因此相对于需要传入锁，或者在函数闭包中创建锁，让我们尝试在包装器本身中的创建和管理锁。** 首先考虑一个正常函数的情况。在这种情况下，我们所能做的就是将所需的线程锁存储在包装的函数对象本身上。 123456789101112131415@decoratordef synchronized(wrapped, instance, args, kwargs): lock = vars(wrapped).get('_synchronized_lock', None) if lock is None: lock = vars(wrapped).setdefault('_synchronized_lock', threading.RLock()) with lock: return wrapped(*args, **kwargs)@synchronizeddef function(): pass&gt;&gt;&gt; function()&gt;&gt;&gt; function._synchronized_lock&lt;_RLock owner=None count=0&gt; 我们要处理的一个关键问题是如何第一次创建线程锁。为此我们需要做的是查看线程锁是否已被创建。** lock = vars(wrapped).get(&#39;_synchronized_lock&#39;, None) 如果返回一个有效的线程锁对象，那么我们就可以继续尝试获取锁。如果锁不存在我们需要创建锁,但是我们必需小心避免竞态条件，因为当两个线程同时进入这部分代码时，它们都会判断需要第一次创建锁。我们用来解决这个问题的窍门是: lock = vars(wrapped).setdefault(&#39;_synchronized_lock&#39;, threading.RLock()) 当两个线程同时尝试创建锁时，它们都可能创建一个锁实例，但是由于使用了dict.setdefault()，只会有一个进程会成功。因为 dict.setdefault() 总是返回它第一次存储的值。所以所有的线程都会继续运行并且尝试获取相同的锁对象。其中一个线程对象会被丢弃也不存在任何问题，因为这只会在初始化并出现竞争条件时才会发生。 因此，我们已经成功地复制了最初的内容，不同之处在于线程锁存储在被包装的函数上，而不是存储在一个封闭函数的堆栈上。我们仍然有一个问题，即每个实例方法都有一个不同的锁。(而不是一个实例内的所有同步方法共用一个锁)。简单的解决方案是利用我们的通用装饰器，它提供了判断装饰器被使用的上下文的能力。 具体点说，我们需要判断当前是否在装饰一个类方法或实例方法，如果是，则将锁对象存储在 instance 参数上。 12345678910111213141516171819202122232425262728293031323334353637383940@decoratordef synchronized(wrapped, instance, args, kwargs): if instance is None: context = vars(wrapped) else: context = vars(instance) lock = context.get('_synchronized_lock', None) if lock is None: lock = context.setdefault('_synchronized_lock', threading.RLock()) with lock: return wrapped(*args, **kwargs)class Object(object): @synchronized def method_im(self): pass @synchronized @classmethod def method_cm(cls): passo1 = Object()o2 = Object()&gt;&gt;&gt; o1.method_im()&gt;&gt;&gt; o1._synchronized_lock&lt;_RLock owner=None count=0&gt;&gt;&gt;&gt; id(o1._synchronized_lock)4386605392&gt;&gt;&gt; o2.method_im()&gt;&gt;&gt; o2._synchronized_lock&lt;_RLock owner=None count=0&gt;&gt;&gt;&gt; id(o2._synchronized_lock)4386605456 这个简单的改变实际上已经达到了我们想要的结果。如果同步的装饰器被用于一个正常的函数，那么线程锁将被存储在函数本身上，并且它将单独存在，并且只在调用相同的函数之间进行同步。 对于实例方法，线程锁将被存储在类的实例上，实例方法会绑定到类，因此在该类上标记为同步的任何实例方法都将在该线程锁上同步，从而模拟Java的行为 那类方法呢。在这种情况下，instance 参数实际上是类。如果线程锁被存储在类上，那么结果将是，如果有多个类方法，并且它们都被标记为synchronized，那么它们将相互排斥。这种情况下线程锁的使用方式将不同于实例方法，但这实际上也是我们想要的。 代码是否对类方法有效? 12345678&gt;&gt;&gt; Object.method_cm()Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"test.py\", line 38, in __call__ return self.wrapper(self.wrapped, instance, args, kwargs) File \"synctest.py\", line 176, in synchronized lock = context.setdefault('_synchronized_lock'),AttributeError: 'dictproxy' object has no attribute 'setdefault' 很不幸，有错。这种情况的原因是，类 __dict__ 不是一个普通的字典，而是一个 dictproxy 。一个 dictproxy 不与普通的dict共享相同的方法，特别是它不提供setdefault()方法。因此，我们需要一种不同的方法来为类创建同步线程锁。dictproxy 还导致了另一个问题，即它不支持属性设置。但是类本身支持属性设置 1234&gt;&gt;&gt; vars(Object)['_synchronized_lock'] = threading.RLock()Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;TypeError: 'dictproxy' object does not support item assignment 123&gt;&gt;&gt; setattr(Object, '_synchronized_lock', threading.RLock())&gt;&gt;&gt; Object._synchronized_lock&lt;_RLock owner=None count=0&gt; 由于函数对象和类实例都可以，所以我们需要切换更新属性的方法。 4. 存储在装饰器上的元线程锁作为dict.setdefault()第一次设置锁的原子方式的替代方法，我们可以做的是使用存储在@synchronized 装饰器本身上的元线程锁。由于元线程锁的创建仍存在竞争条件，因此需要使用dict.setdefault()实现元线程锁的原子性创建。 123456789101112131415161718192021@decoratordef synchronized(wrapped, instance, args, kwargs): if instance is None: owner = wrapped else: owner = instance lock = vars(owner).get('_synchronized_lock', None) if lock is None: meta_lock = vars(synchronized).setdefault( '_synchronized_meta_lock', threading.Lock()) with meta_lock: lock = vars(owner).get('_synchronized_lock', None) if lock is None: lock = threading.RLock() setattr(owner, '_synchronized_lock', lock) with lock: return wrapped(*args, **kwargs) 请注意，由于对封装函数的锁存在的检查与创建元锁之间的间隙，在我们获得了元锁之后，我们需要再次检查锁是否存在。这是为了避免两个线程同时在尝试创建锁而发生竞争条件。 这里有一点很重要，我们仅仅在更新被包装对象上的锁时使用了属性访问方法。而在查找被包装对象上是否存在锁时，没有使用getattr()方法，而是继续在vars()返回的__dict__中查找它。这是必要的，因为当在类的实例上使用getattr()时，如果该属性在类的实例中不存在，那么查找规则意味着如果该属性在类本身上存在，那么将返回该属性。 如果一个同步的类方法是第一个被调用的，这会导致问题，因为它会在类本身上留下一个锁。当随后调用实例方法时，如果使用了getattr()，它会找到类类型的锁并返回它，并且会被错误地使用。因此，我们继续通过 __dict__ 寻找锁，因为它只包含实例中实际存在的内容。 有了这些修改，所有锁的创建都可以自动完成，并在不同的上下文中创建一个适当的锁。 12345678910111213141516171819202122232425262728@synchronizeddef function(): passclass Object(object): @synchronized def method_im(self): pass @synchronized @classmethod def method_cm(cls): passo = Object()&gt;&gt;&gt; function()&gt;&gt;&gt; id(function._synchronized_lock)4338158480&gt;&gt;&gt; Object.method_cm()&gt;&gt;&gt; id(Object._synchronized_lock)4338904656&gt;&gt;&gt; o.method_im()&gt;&gt;&gt; id(o._synchronized_lock)4338904592 代码也适用于在静态方法或类中使用@synchronized。综上所述，@synchronized 可以被应用的场景如下: 123456789101112131415161718192021222324@synchronized # lock bound to function1def function1(): pass@synchronized # lock bound to function2def function2(): pass@synchronized # lock bound to Classclass Class(object): @synchronized # lock bound to instance of Class def function_im(self): pass @synchronized # lock bound to Class @classmethod def function_cm(cls): pass @synchronized # lock bound to function_sm @staticmethod def function_sm(): pass 5. 实现同步代码块所以，我们已经完成了对同步方法的支持，如何实现同步代码块呢。要实现的目标是能按照下面的方式编写代码: 123456789class Object(object): @synchronized def function_im_1(self): pass def function_im_2(self): with synchronized(self): pass 也就是说，我们需要 synchronized 装饰器不仅可以用作装饰器，而且还可以作为上下文管理器使用。在synchronized作为上下文管理器时，类似于Java，需要提供给它执行同步操作的对象，对于实例方法而言，这个对象是 self 参数或者类的实例。为了解释我们如何做到这一点，需要等待下一篇文章。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"wrapt","slug":"wrapt","permalink":"http://yoursite.com/tags/wrapt/"}]},{"title":"06 装饰器的类实现","slug":"wrapt/python_decorator_06","date":"2018-05-24T16:00:00.000Z","updated":"2020-05-24T15:30:58.509Z","comments":true,"path":"2018/05/25/wrapt/python_decorator_06/","link":"","permalink":"http://yoursite.com/2018/05/25/wrapt/python_decorator_06/","excerpt":"上一篇文章中，我们讨论了如何实现一个带参数的装饰器，以及如何让装饰器可选的接收参数而不是必需输入参数。也讨论了如何让装饰器能在被包装函数的不同调用之间保持状态。保持状态的一种可用方法是使用类实现装饰器。然而我们实现的通用装饰器模式在使用类实现装饰器还存在一些问题，本文我们将来探讨问题出现的根源以及如何解决。","text":"上一篇文章中，我们讨论了如何实现一个带参数的装饰器，以及如何让装饰器可选的接收参数而不是必需输入参数。也讨论了如何让装饰器能在被包装函数的不同调用之间保持状态。保持状态的一种可用方法是使用类实现装饰器。然而我们实现的通用装饰器模式在使用类实现装饰器还存在一些问题，本文我们将来探讨问题出现的根源以及如何解决。 1. 装饰器工厂函数正如前文所述，我们通过类实现装饰器的模式如下 12345678910class with_arguments(object): def __init__(self, arg): self.arg = arg @decorator def __call__(self, wrapped, instance, args, kwargs): return wrapped(*args, **kwargs)@with_arguments(arg=1)def function(): pass 当我们这么做时，装饰器在被应用时发生了如下错误: 1234Traceback (most recent call last): File \"test.py\", line 483, in &lt;module&gt; @with_arguments(1)TypeError: _decorator() takes exactly 1 argument (2 given) _decorator() 是我们装饰器工厂函数的内部函数。 12345def decorator(wrapper): @functools.wraps(wrapper) def _decorator(wrapped): return function_wrapper(wrapped, wrapper) return _decorator 错误的原因是我们使用函数闭包实现装饰器工厂函数，却希望它能同时工作在普通函数和类方法上。当类方法被访问时，将触发描述符协议，绑定将会发生；类实例的引用将自动作为第一个参数传递给类方法。而 _decorator() 却没有被定义成同时接收 self和wrapped 作为参数，所以调用失败。我们可以创建一个仅用于类实例的装饰器工厂函数。但是这与我们之前要为类方法和函数创建统一的装饰器的初衷相违背。 解决问题的方法是，使用我们的 function_wrapper 作为装饰器工厂的返回对象，而不是函数闭包。 12345678910111213141516171819def decorator(wrapper): def _wrapper(wrapped, instance, args, kwargs): def _execute(wrapped): return function_wrapper(wrapped, wrapper) return _execute(*args, **kwargs) return function_wrapper(wrapper, _wrapper)class with_arguments(object): def __init__(self, arg): self.arg = arg @decorator def __call__(self, wrapped, instance, args, kwargs): return wrapped(*args, **kwargs)@with_arguments(arg=1)def function(): pass 这种方式特别巧妙，但是很不容易理解，我们再来看看整个调用的发生过程 with_arguments(arg=1) 带参数的装饰器被使用时，将创建一个类实例 ins 在 @decorator 装饰下, ins 的 __call__ 方法此时是 function_wrapper(__call__, _wrapper) 对象 @ 将 function 对象作为参数传递给创建的类实例，将调用 ins.__call__(function) 方法，此时将触发function_wrapper的描述符协议，并进一步调用 _wrapper(__call__, ins) 函数，functions 对象则通过 arg 传递给 _execute 函数，_execute 执行返回新的 function_wrapper(functions, __call__) 对象 装饰的最终结果是，我们现在不必担心 @decorator 被应用在普通函数，实例方法还是一个类方法上。因为在所有的情况下，被绑定的实例对象不会通过 args 被传递 细心的读者很快就会发现另一个问题，在 __call__ 在被调用时，需要传入装饰器类的实例即 self 参数，而在上述的实现中并没有此步骤。(不过我没懂为什么作者在 _wrapper 内多嵌套一层_execute函数，应该是想说名这是要被执行的部分。) 2. 类的绑定要求更改之后，重新进行测试，我们遇到了一个新的问题。这次发生在被被包装函数被调用的时候。 123456&gt;&gt;&gt; function()Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"test.py\", line 243, in __call__ return self.wrapper(self.wrapped, None, args, kwargs)TypeError: __call__() takes exactly 5 arguments (4 given) 现在这个问题是__call__()方法传递给@decorator发生在 类初始化，此时它是未绑定方法，任何类实例远还没被创建。通常情况下，类实例的引用在方法被绑定时被提供，但是因为我们的装饰器实际是一个工厂函数，因此这里涉及到了两层绑定。外部包装函数的类实例被传递给工厂函数内部的 _wrapper 函数的instance参数。但是它在 function wrapper 对象被创建的时候，完全没有被使用。为了解决这个问题，我们需要根据是否绑定了一个实例方法，显示使用类实例绑定我们的包装函数 1234567891011def decorator(wrapper): def _wrapper(wrapped, instance, args, kwargs): def _execute(wrapped): if instance is None: return function_wrapper(wrapped, wrapper) elif inspect.isclass(instance): return function_wrapper(wrapped, wrapper.__get__(None, instance)) else: return function_wrapper(wrapped, wrapper.__get__(instance, type(instance))) return _execute(*args, **kwargs) return function_wrapper(wrapper, _wrapper) 在这个示例中，有三种情况需要我们处理。 第一种情况是 instance 为 None。这对应于decorator函数被应用在普通函数，类静态方法或一个类上 第二种情况是 instance 不为 None，但是是一个类对象。这对应用于一个类方法。这种情况下，我们需要通过包装函数的get()将包装函数显示绑定到一个类对象。 第三种即最后一种情况下，instance 不是None，也不是一个类对象。这对应于实例方法。在这种情况我们仍然需要绑定包装函数，只不过这次绑定的是类实例。 3. 总结改进之后，我们解决了所有问题，而且很大程度上完善了我们的装饰器模式。所以，目前我们的通用装饰器解决方案如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869class object_proxy(object): def __init__(self, wrapped): self.wrapped = wrapped try: self.__name__ = wrapped.__name__ except AttributeError: pass @property def __class__(self): return self.wrapped.__class__ def __getattr__(self, name): return getattr(self.wrapped, name)class bound_function_wrapper(object_proxy): def __init__(self, wrapped, instance, wrapper, binding, parent): super(bound_function_wrapper, self).__init__(wrapped) self.instance = instance self.wrapper = wrapper self.binding = binding self.parent = parent def __call__(self, *args, **kwargs): if self.binding == 'function': if self.instance is None: instance, args = args[0], args[1:] wrapped = functools.partial(self.wrapped, instance) return self.wrapper(wrapped, instance, args, kwargs) else: return self.wrapper(self.wrapped, self.instance, args, kwargs) else: instance = getattr(self.wrapped, '__self__', None) return self.wrapper(self.wrapped, instance, args, kwargs) def __get__(self, instance, owner): if self.instance is None and self.binding == 'function': descriptor = self.parent.wrapped.__get__(instance, owner) return bound_function_wrapper(descriptor, instance, self.wrapper, self.binding, self.parent) return self class function_wrapper(object_proxy): def __init__(self, wrapped, wrapper): super(function_wrapper, self).__init__(wrapped) self.wrapper = wrapper if isinstance(wrapped, classmethod): self.binding = 'classmethod' elif isinstance(wrapped, staticmethod): self.binding = 'staticmethod' else: self.binding = 'function' def __get__(self, instance, owner): wrapped = self.wrapped.__get__(instance, owner) return bound_function_wrapper(wrapped, instance, self.wrapper, self.binding, self) def __call__(self, *args, **kwargs): return self.wrapper(self.wrapped, None, args, kwargs)def decorator(wrapper): def _wrapper(wrapped, instance, args, kwargs): def _execute(wrapped): if instance is None: return function_wrapper(wrapped, wrapper) elif inspect.isclass(instance): return function_wrapper(wrapped, wrapper.__get__(None, instance)) else: return function_wrapper(wrapped, wrapper.__get__(instance, type(instance))) return _execute(*args, **kwargs) return function_wrapper(wrapper, _wrapper) 尽管在之前的文章中提到过。这里给出的对象代理实现并不是一个完美实现。因此，不要使用这段代码。如果你使用了，就会发现。在被包装函数上的部分内省操作不会按照我们所预期的执行。特别的，访问函数的doc属性总是返回 None。类似Python3中的新增变量 __qualname__ 和 __module__ 也不能正确显示。 正确处理像__doc__这样的内置属性是比较费劲的，因为内置属性的获取逻辑与普通属性有时候并不相同。上述实现中我们期望的是，无论从代理对象还是代理对象的子类，我们都是从被包装函数获取并返回属性值，但是对于__doc__属性，即便是代理对象的子类没有__doc__属性，它也同样会覆盖父类的__doc__，结果是代理对象的子类拦截了对 __doc__ 属性的获取。所以这里展示的代理对象仅仅是一个参照实现。 大体上说，这里所有的代码都仅仅是参照实现。目的不是使用而是展示如何实现一个更加通用的装饰器。它只是提供给你一个学习的途径。不要期望通过简单的几行代码就能实现，事情不会那么简单。 4. wrapt 模块如果我告诉你不要使用这里的代码，那你应该怎么做呢？答案是在PyPi上已经有现成的 wrapt 模块。wrapt 模块已经上线几个月了，但是目前为止并没有广为人知。它实现了这里描述的所有细节，甚至更多。这个模块实现了一个完整的代理对象，能使所有代码正确执行。并且提供了很多和装饰器工厂函数相关的特性，也提供了很多和猴子补丁相关的特性。 虽然我指出了wrapt 模块的存在，但是博客内容不会就此停止，因为我还有其他一些主题想要阐述。这些内容包括通用装饰器的应用，启用和关闭装饰器，装饰器执行性能问题，以及代理对象，猴子补丁的实现问题等等。 接下来的博客，我将举一个通用装饰器应用的特殊示例，来说明Python 装饰器如此强大，为什么Pyhton不提供一个@synchronized装饰器。在装饰器第一次被引入编程语言时，这个装饰器被当作是如何使用装饰器的经典示例。然而我能找到的所有实现都是半成品，很少在现实世界中被使用。我相信这里的通用装饰器能帮助我们实现一个可用的@synchronized装饰器。我将在下一篇博客中详述它。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"wrapt","slug":"wrapt","permalink":"http://yoursite.com/tags/wrapt/"}]},{"title":"05 带参数的装饰器","slug":"wrapt/python_decorator_05","date":"2018-05-23T16:00:00.000Z","updated":"2020-05-24T15:30:54.170Z","comments":true,"path":"2018/05/24/wrapt/python_decorator_05/","link":"","permalink":"http://yoursite.com/2018/05/24/wrapt/python_decorator_05/","excerpt":"在之前的博客，通过使用代理对象，装饰器工厂函数等技术，我们已经实现了一个通用装饰器。在这篇文章中，我们将使用前面文章中描述的装饰器工厂函数，介绍如何使用它来实现接受参数的装饰器，包括强制参数和可选的接收参数。","text":"在之前的博客，通过使用代理对象，装饰器工厂函数等技术，我们已经实现了一个通用装饰器。在这篇文章中，我们将使用前面文章中描述的装饰器工厂函数，介绍如何使用它来实现接受参数的装饰器，包括强制参数和可选的接收参数。 1. 装饰器创建模式前面文章中描述的关键组件是一个函数包装器对象。我不打算复制代码，所以请参阅前面的帖子。简而言之，它是一个类类型，它接受要被包装的函数和一个用户提供的包装器函数。所得到的函数包装器对象的实例被用来代替被包装函数，当调用时，会将被包装函数的调用委托给用户提供的包装器函数。这允许用户修改调用的方式，在调用被包装函数之前或之后执行操作，或者修改输入参数或结果。function_wrapper 和装饰器工厂一起使用创建装饰器的方式如下:** 12345678910111213141516171819# 装饰器工厂函数def decorator(wrapper): @functools.wraps(wrapper) def _decorator(wrapped): return function_wrapper(wrapped, wrapper) return _decorator# 使用工厂函数创建的装饰器@decoratordef my_function_wrapper(wrapped, instance, args, kwargs): print('INSTANCE', instance) print('ARGS', args) print('KWARGS', kwargs) return wrapped(*args, **kwargs)# 应用装饰器包装函数@my_function_wrapperdef function(a, b): pass 在本例中，创建的最终装饰器不接受任何参数，但如果我们希望装饰器能够接受参数，在调用用户提供的包装器函数时可访问传入的参数，那么我们该如何做呢？ 2. 使用函数闭包收集参数最简单的实现一个能接收参数的装饰器的方式是使用函数闭包 123456789def with_arguments(arg): @decorator def _wrapper(wrapped, instance, args, kwargs): return wrapped(*args, **kwargs) return _wrapper@with_arguments(arg=1)def function(): pass 实际上，外部函数本身是一个工厂函数，可根据传入的参数，返回不同的装饰器实例。因此，当外部工厂函数被应用到一个具有特定参数的函数时，它返回内部装饰器函数，实际上它是应用于被包装的函数。当包装器函数最终被调用时，它会调用被包装函数，并通过作为函数闭包的一部分来访问传递给外部工厂函数的原始参数。** 位置或关键字参数可以与外部装饰器工厂函数一起使用，但是我认为关键字参数可能是一个更好的惯例，我稍后会展示。现在，如果带有参数的装饰器具有默认值，使用这种方法来实现装饰器，即使不传递参数，也必需将其作为一个不同的调用来使用。也就是说，仍然需要提供空括号。 123456789def with_arguments(arg='default'): @decorator def _wrapper(wrapped, instance, args, kwargs): return wrapped(*args, **kwargs) return _wrapper@with_arguments()def function(): pass 尽管这只是一个特例，但看起来不优雅。大多数更喜欢当所有参数都是可选，并没有被显示传递参数时，括号时可选的。换句话说，当没有参数被传递时，可以被写成 123@with_argumentsdef function(): pass 当我们从另一个角度看问题时，这个想法实际上是有价值的。如果一个装饰器最初不接收参数，但是之后又需要可选的接收参数。如果括号是可选的，那么原来不带参数调用装饰器的代码也无需改变。 3. 带可选参数的装饰器允许装饰器添加可选参数，可以将上面的方法更改为: 12345678910111213141516def optional_arguments(wrapped=None, arg=1): if wrapped is None: return functools.partial(optional_arguments, arg=arg) @decorator def _wrapper(wrapped, instance, args, kwargs): return wrapped(*args, **kwargs) return _wrapper(wrapped)@optional_arguments(arg=2)def function1(): pass@optional_argumentsdef function2(): pass 当具有默认的可选参数时，外部工厂函数将被包装函数作为第一个参数并默认为 None。第一次调用时，被包装函数是 None，通过 partical 函数再一次返回装饰器工厂函数。第二次调用，被包装函数将被传入并被装饰器包装。 将装饰器被直接装饰函数时，因为默认参数的存在，我们不需要显示传递参数。因为 wrapped 惨数值不是None，装饰器直接返回工厂函数，直接装饰函数。 此时工厂函数的参数必需是关键词参数，Python 3允许您使用新的关键字参数语法来强制使用关键词参数。 123456789def optional_arguments(wrapped=None, *, arg=1): if wrapped is None: return functools.partial(optional_arguments, arg=arg) @decorator def _wrapper(wrapped, instance, args, kwargs): return wrapped(*args, **kwargs) return _wrapper(wrapped) 这样，就可以避免有人不小心将装饰器参数作为位置参数传递给 wrapped。对于一致性，关键字参数也可以被强制执行，即使它不是必需的。 12345def required_arguments(*, arg): @decorator def _wrapper(wrapped, instance, args, kwargs): return wrapped(*args, **kwargs) return _wrapper 4. 在调用之间保持状态某些时候，装饰器可能需要在函数调用之间保持状态。一个典型的例子是缓存装饰器。此时，由于包装器函数本身没有任何状态收集器，所以只能借助于装饰器能够访问到的外部数据结构作为状态收集器进行状态保持。 有几种方法可以做到这一点。 第一个是将保持状态的对象作为显式参数传递给装饰器 12345678910111213141516def cache(d): @decorator def _wrapper(wrapped, instance, args, kwargs): try: key = (args, frozenset(kwargs.items())) return d[key] except KeyError: result = wrapped(*args, **kwargs) return result return _wrapper_d = &#123;&#125;@cache(_d)def function(): return time.time() 除非有特定的需要能够传入状态对象，否则第二个更好的方法是在外部函数的调用中创建状态对象。 1234567891011121314151617def cache(wrapped): d = &#123;&#125; @decorator def _wrapper(wrapped, instance, args, kwargs): try: key = (args, frozenset(kwargs.items())) return d[key] except KeyError: result = d[key] = wrapped(*args, **kwargs) return result return _wrapper(wrapped)@cachedef function(): return time.time() 这种情况下，外部包装函数在函数内部自定状态对象，而不是通过参数显示传递。如果这是一个合理的默认值，但是在某些情况下，仍然需要将状态对象作为参数传递进来，那么可以使用可选的装饰数参数。 12345678910111213141516171819202122232425262728293031def cache(wrapped=None, d=None): if wrapped is None: return functools.partial(cache, d=d) if d is None: d = &#123;&#125; @decorator def _wrapper(wrapped, instance, args, kwargs): try: key = (args, frozenset(kwargs.items())) return d[key] except KeyError: result = d[key] = wrapped(*args, **kwargs) return result return _wrapper(wrapped)@cachedef function1(): return time.time()_d = &#123;&#125;@cache(d=_d)def function2(): return time.time()@cache(d=_d)def function3(): return time.time() 5. 使用类创建装饰器在第一篇文章中，我们说过可以使用类实现装饰器。 1234567class function_wrapper(object): def __init__(self, wrapped): self.wrapped = wrapped def __call__(self, *args, **kwargs): return self.wrapped(*args, **kwargs) 就像之前已经阐述的，这种通过类实现的装饰器存在缺陷，但是作为一种替代模式，这种原始的方法也能保持状态。具体地说，类的构造函数可以将状态对象连同被包装函数保存为类实例的属性。 1234567891011121314151617class cache(object): def __init__(self, wrapped): self.wrapped = wrapped self.d = &#123;&#125; def __call__(self, *args, **kwargs): try: key = (args, frozenset(kwargs.items())) return self.d[key] except KeyError: result = self.d[key] = self.wrapped(*args, **kwargs) return result@cachedef function(): return time.time() 在装饰器逻辑特别复杂时，这种通过类实现的装饰器也存在一些好处。可以拆分封装在不同的类方法中。那么使用我们的新函数包装器和装饰器工厂，能否将装饰器实现为类呢？一种可能的方式是这样: 123456789101112class with_arguments(object): def __init__(self, arg): self.arg = arg @decorator def __call__(self, wrapped, instance, args, kwargs): return wrapped(*args, **kwargs)@with_arguments(arg=1)def function(): pass 装饰器执行逻辑是这样的，当带参数的装饰器被使用时，将创建一个类实例。在被包装函数被调用时，将调用 @decorator 装饰的实例方法 __call__()，__call__()进而调用被包装函数。因为__call__()是实例的绑定方法，所以能够访问到类实例拥有的状态对象。 那么事实上是否能正常运行呢？ 1234Traceback (most recent call last): File \"test.py\", line 483, in &lt;module&gt; @with_arguments(1)TypeError: _decorator() takes exactly 1 argument (2 given) 理想很丰满，显示很骨干。失败的原因就在于装饰器工厂函数的实现方式，我们将在下一篇文章种解释并解决这个特别的问题。 12345def decorator(wrapper): @functools.wraps(wrapper) def _decorator(wrapped): return function_wrapper(wrapped, wrapper) return _decorator 作为另一种一种替代方式是，仍然使用类封装所需的逻辑，并在函数闭包类创建实例供包装函数使用。装饰器将功能委托给类实例，但是本身不作为类实现。这种方式需要额外创建一个类，使用起来并不优雅。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"wrapt","slug":"wrapt","permalink":"http://yoursite.com/tags/wrapt/"}]},{"title":"04 实现一个通用装饰器","slug":"wrapt/python_decorator_04","date":"2018-05-21T16:00:00.000Z","updated":"2020-05-24T15:30:48.440Z","comments":true,"path":"2018/05/22/wrapt/python_decorator_04/","link":"","permalink":"http://yoursite.com/2018/05/22/wrapt/python_decorator_04/","excerpt":"本节我们将实现一个”通用装饰器”，它能够让用户提供的包装函数通过传入的参数判断其被使用的上下文，即确定，它是被应用在函数，实例方法，类方法，类对象中的哪一个。因为装饰器不是在各个环境种被单独实现，而是以一种更加统一的方式创建，所以将这种能确定上下文的装饰器称为通用装饰器。","text":"本节我们将实现一个”通用装饰器”，它能够让用户提供的包装函数通过传入的参数判断其被使用的上下文，即确定，它是被应用在函数，实例方法，类方法，类对象中的哪一个。因为装饰器不是在各个环境种被单独实现，而是以一种更加统一的方式创建，所以将这种能确定上下文的装饰器称为通用装饰器。 1. 内容回顾到目前为止，我们创建装饰器的方式已经经过了几次迭代: 第一篇博客中我们介绍使用函数创建装饰器的传统方式，这种方式存在几个重大问题 为解决函数创建装饰器的问题，我们在第二篇博客中使用了代理对象，并将装饰器实现成了描述符，这种方式有效的解决了之前的问题，但是存在大量的样板代码 为了提高创建装饰器的效率，第三篇博客中我们使用了装饰器工厂函数，抽象了装饰器的创建过程，用户只需提供一个执行所需的包装函数即可。我们的目的是实现一个通用装饰器，能够让用户的包装函数通过传入参数确定其被使用的上下文。 12345678910111213# 包装函数通过传入参数确定其被使用的上下文@decoratordef universal(wrapped, instance, args, kwargs): if instance is None: if inspect.isclass(wrapped): # class. else: # function or staticmethod. else: if inspect.isclass(instance): # classmethod. else: # instancemethod. 目前为止我们已经能够区分装饰器是被用于普通函数和还是实例方法，但是当通过类调用类方法和静态方法时将出现问题。本文我们将继续探索如何调整我们的装饰器工厂函数，以区分类方法和静态方法，以便找到实现通用装饰器的模式 2. 区分普通函数和实例方法目前为止，我们的通用装饰器模式实现如下: 123456789101112131415161718192021222324252627282930313233class bound_function_wrapper(object_proxy): def __init__(self, wrapped, instance, wrapper): super(bound_function_wrapper, self).__init__(wrapped) self.instance = instance self.wrapper = wrapper def __call__(self, *args, **kwargs): if self.instance is None: instance, args = args[0], args[1:] wrapped = functools.partial(self.wrapped, instance) return self.wrapper(wrapped, instance, args, kwargs) return self.wrapper(self.wrapped, self.instance, args, kwargs)class function_wrapper(object_proxy): def __init__(self, wrapped, wrapper): super(function_wrapper, self).__init__(wrapped) self.wrapper = wrapper def __get__(self, instance, owner): wrapped = self.wrapped.__get__(instance, owner) return bound_function_wrapper(wrapped, instance, self.wrapper) def __call__(self, *args, **kwargs): return self.wrapper(self.wrapped, None, args, kwargs)# 装饰器工厂函数def decorator(wrapper): @functools.wraps(wrapper) def _decorator(wrapped): return function_wrapper(wrapped, wrapper) return _decorator 为了测试当前的模式能在任何情况下都能工作，我们需要使用装饰器工厂创建一个装饰器，它能在执行时打印绑定的 instance 对象，以及传递进来的参数。 12345@decoratordef my_function_wrapper(wrapped, instance, args, kwargs): print('INSTANCE', instance) print('ARGS', args) return wrapped(*args, **kwargs) 当装饰器被应用到一个正常的函数和实例方法时，包括通过显式传入实例调用实例方法时，我们能够得到符合预期的结果 12345678910111213141516171819202122@my_function_wrapperdef function(a, b): pass&gt;&gt;&gt; function(1, 2)INSTANCE NoneARGS (1, 2)class Class(object): @my_function_wrapper def function_im(self, a, b): passc = Class()&gt;&gt;&gt; c.function_im(1, 2)INSTANCE &lt;__main__.Class object at 0x1085ca9d0&gt;ARGS (1, 2)&gt;&gt;&gt; Class.function_im(c, 1, 2)INSTANCE &lt;__main__.Class object at 0x1085ca9d0&gt;ARGS (1, 2) 但是当装饰起被应用到类方法以及静态方法时，参数传递发生了错误。instance 按预期要么为空，要么接收的是类实例或类对象，现在却是传递给函数的第一个实参。并不符合我们通用装饰器的要求 。 12345678910111213141516171819class Class(object): @my_function_wrapper @classmethod def function_cm(self, a, b): pass @my_function_wrapper @staticmethod def function_sm(a, b): pass&gt;&gt;&gt; Class.function_cm(1, 2)INSTANCE 1ARGS (2,)&gt;&gt;&gt; Class.function_sm(1, 2)INSTANCE 1ARGS (2,) 3. 区分类方法和静态方法因此，我们要指出的是，在实例被传递为None的情况下，我们需要能够区分这三种情况: 通过类直接调用实例方法 类方法被调用 静态方法被调用 一种判断方法是查看绑定函数的__self__属性。该属性保存了函数在特定时间点绑定到的对象类型信息。我们先来看看通过类调用不同方法时，此属性的值。 123456789101112&gt;&gt;&gt; print(Class.function_im.__self__)None&gt;&gt;&gt; print(Class.function_cm.__self__)&lt;class '__main__.Class'&gt;&gt;&gt;&gt; print(Class.function_sm.__self__)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"test.py\", line 19, in __getattr__ return getattr(self.wrapped, name)AttributeError: 'function' object has no attribute '__self__' 通过类调用实例方法的情况，__self__ 值为 None，对于类方法，它将是类对象，在静态方法的情况下，不存在 __self__ 属性。似乎检查 __self__ 是一个有效的判断方法 在我们编写一个基于此的解决方案之前，我们先检查一下Python 3，以确保我们在那里没问题，并且没有任何变化。 12345678910111213141516&gt;&gt;&gt; print(Class.function_im.__self__)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"dectest.py\", line 19, in __getattr__ return getattr(self.wrapped, name)AttributeError: 'function' object has no attribute '__self__'&gt;&gt;&gt; print(Class.function_cm.__self__)&lt;class '__main__.Class'&gt;&gt;&gt;&gt; print(Class.function_sm.__self__)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"test.py\", line 19, in __getattr__ return getattr(self.wrapped, name)AttributeError: 'function' object has no attribute '__self__' Python 3 与 Python 2 表现并不相同，此方法无效。但是为什么会出现这种情况？发生这种情况的原因是，Pyhton3 已经没有未绑定对象这个对象，通过类直接调用实例方法时返回的也是函数。而Python2中通过类调用实例的返回值类型依赖于 __self__ 是否为None，所以Python3中删除了此属性。因此，我们现在不能区分通过类调用实例方法和调用静态方法这两种情况。 另一个方法是在 function_wrapper 构造函数内，检查被包装对象的类型，并确定它是类方法还是静态方法。然后，将判定信息传递到 bound function wrapper 并进行进一步检查。 12345678910111213141516171819202122232425262728293031323334class bound_function_wrapper(object_proxy): def __init__(self, wrapped, instance, wrapper, binding): super(bound_function_wrapper, self).__init__(wrapped) self.instance = instance self.wrapper = wrapper self.binding = binding def __call__(self, *args, **kwargs): if self.binding == 'function' and self.instance is None: instance, args = args[0], args[1:] wrapped = functools.partial(self.wrapped, instance) return self.wrapper(wrapped, instance, args, kwargs) return self.wrapper(self.wrapped, self.instance, args, kwargs)class function_wrapper(object_proxy): def __init__(self, wrapped, wrapper): super(function_wrapper, self).__init__(wrapped) self.wrapper = wrapper if isinstance(wrapped, classmethod): self.binding = 'classmethod' elif isinstance(wrapped, staticmethod): self.binding = 'staticmethod' else: self.binding = 'function' def __get__(self, instance, owner): wrapped = self.wrapped.__get__(instance, owner) return bound_function_wrapper(wrapped, instance, self.wrapper, self.binding) def __call__(self, *args, **kwargs): return self.wrapper(self.wrapped, None, args, kwargs) 如果有人实际上在他们的decorator中实现了描述符协议，那么希望他们也可以在这里使用对象代理。因为对象代理拥有class属性，它将返回被包装对象的类，这意味着isinstance()检查仍然会成功，因为isinstance()会优先考虑class的返回结果，而不是对象的实际类型。 无论如何，更改后，我们重新测试如下 1234567891011121314151617181920212223&gt;&gt;&gt; c.function_im(1,2)INSTANCE &lt;__main__.Class object at 0x101f973d0&gt;ARGS (1, 2)&gt;&gt;&gt; Class.function_im(c, 1, 2)INSTANCE &lt;__main__.Class object at 0x101f973d0&gt;ARGS (1, 2)&gt;&gt;&gt; c.function_cm(1,2)INSTANCE &lt;__main__.Class object at 0x101f973d0&gt;ARGS (1, 2)&gt;&gt;&gt; Class.function_cm(1, 2)INSTANCE NoneARGS (1, 2)&gt;&gt;&gt; c.function_sm(1,2)INSTANCE &lt;__main__.Class object at 0x101f973d0&gt;ARGS (1, 2)&gt;&gt;&gt; Class.function_sm(1, 2)INSTANCE NoneARGS (1, 2) 成功，我们已经修复了调用类方法和静态方法时参数列表问题。现在的问题是，虽然对通过实例调用方法时， instance 参数没有问题。但是无论是通过实例还是类，传递给类方法和静态方法的 instance 参数都没有什么用。并且我们不能将它同其他情形区别开。理想情况下，我们希望调用类方法时 instance 参数始终为类对象，而调用静态方法时，则使用为 None。因此 对于静态方法，我们只需要在检查被包装类型时，判断 ‘staticmethod’ 即可 对于类方法的情况，如果我们回头看一下我们的测试，看看是否可以使用__self__属性，我们发现，对于类方法，__self__是类实例，对于静态方法，属性不存在。因此，我们可以做的是，如果包装对象的类型不是一个函数，那么我们可以查找__self__的值，如果它不存在的话，就会默认为None。这将满足这两种情况。进一步改进后如下 1234567891011121314151617181920212223class bound_function_wrapper(object_proxy): def __init__(self, wrapped, instance, wrapper, binding): super(bound_function_wrapper, self).__init__(wrapped) self.instance = instance self.wrapper = wrapper self.binding = binding def __call__(self, *args, **kwargs): if self.binding == 'function': # 通过类调用的实例方法 if self.instance is None: instance, args = args[0], args[1:] wrapped = functools.partial(self.wrapped, instance) return self.wrapper(wrapped, instance, args, kwargs) else: # 通过实例调用的实例方法 return self.wrapper(self.wrapped, self.instance, args, kwargs) else: # 调用静态方法，__self__ 属性不存在，instance 为 None # 调用类方法时，__self__ 为类对象， instance 为类对象 instance = getattr(self.wrapped, '__self__', None) return self.wrapper(self.wrapped, instance, args, kwargs) 如果我们重新测试一次，我们将得到我们想要得结果 1234567891011121314151617181920212223&gt;&gt;&gt; c.function_im(1,2)INSTANCE &lt;__main__.Class object at 0x10c2c43d0&gt;ARGS (1, 2)&gt;&gt;&gt; Class.function_im(c, 1, 2)INSTANCE &lt;__main__.Class object at 0x10c2c43d0&gt;ARGS (1, 2)&gt;&gt;&gt; c.function_cm(1,2)INSTANCE &lt;class '__main__.Class'&gt;ARGS (1, 2)&gt;&gt;&gt; Class.function_cm(1, 2)INSTANCE &lt;class '__main__.Class'&gt;ARGS (1, 2)&gt;&gt;&gt; c.function_sm(1,2)INSTANCE NoneARGS (1, 2)&gt;&gt;&gt; Class.function_sm(1, 2)INSTANCE NoneARGS (1, 2) 现在万事大吉了？可惜并不是。 4. 多层绑定还有一个我们还没有考虑到的特殊情况，即为方法创建别名，并通过别名调用时。 1234567891011121314151617181920212223&gt;&gt;&gt; Class.function_rm = Class.function_im&gt;&gt;&gt; c.function_rm(1, 2)INSTANCE 1ARGS (2,)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"test.py\", line 132, in __call__ return self.wrapper(wrapped, instance, args, kwargs) File \"test.py\", line 58, in my_function_wrapper return wrapped(*args, **kwargs)TypeError: unbound method function_im() must be called with Class instance as first argument (got int instance instead)&gt;&gt;&gt; Class.function_rm = Class.function_cm&gt;&gt;&gt; c.function_rm(1, 2)INSTANCE &lt;class '__main__.Class'&gt;ARGS (1, 2)&gt;&gt;&gt; Class.function_rm = Class.function_sm&gt;&gt;&gt; c.function_rm(1, 2)INSTANCE NoneARGS (1, 2) 对于类方法或静态方法来说，一切都很好，但是对于实例方法来说却失败了。这里的问题是由于在第一次访问实例方法时，它将返回绑定的bound_function wrapper对象。然后把它作为类的属性分配回来。当通过新名称进行后续查找时，在正常情况下，绑定将再次发生，以将其绑定到实际实例。在我们的绑定函数包装器的实现中，我们不提供__get__()方法，因此不会发生这种重新绑定。结果是，在随后的调用中，它全部崩溃。 Class.function_rm = Class.function_im 设置别名时，发生第一次描述符协议，function_rm 绑定得是 bound_function_wrapper 对象，第二次通过别名调用实例方法时会发生第二次描述符协议，进行第二次绑定。 因此，解决方案是我们需要向 bound_function_wrapper 添加__get__()方法，为其提供了执行进一步绑定的能力。我们只希望在实例为None的地方执行这个操作，这表明我们处理的是实例方法，而不是类方法或静态方法。 (注: Class.function_rm = Class.function_im 第一次绑定时，self.binding 为 function，并且由于时通过类直接调用实例方法，因此 instance 参数是 None。包装普通函数时也符合此类情况，但是不会触发描述符协议，只有通过实例调用发生第二次绑定时，才会调用bound_function_wrapper 的__get__方法) 另一个问题是，我们需要绑定的是原始的被包装函数，而不是绑定后的包装函数。最简单的处理方法是将对原始函数包装器 function_wrapper 的引用传递给绑定的函数包装器bound_function_wrapper，并通过它获得原始的被包装函数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class bound_function_wrapper(object_proxy): def __init__(self, wrapped, instance, wrapper, binding, parent): super(bound_function_wrapper, self).__init__(wrapped) self.instance = instance self.wrapper = wrapper self.binding = binding self.parent = parent # 目的是获取原始的被包装函数 def __call__(self, *args, **kwargs): if self.binding == 'function': if self.instance is None: instance, args = args[0], args[1:] wrapped = functools.partial(self.wrapped, instance) return self.wrapper(wrapped, instance, args, kwargs) else: return self.wrapper(self.wrapped, self.instance, args, kwargs) else: instance = getattr(self.wrapped, '__self__', None) return self.wrapper(self.wrapped, instance, args, kwargs) def __get__(self, instance, owner): # 仅在通过类调用实例方法时才会发生第二次绑定 if self.instance is None and self.binding == 'function': descriptor = self.parent.wrapped.__get__(instance, owner) # instance 是第二次绑定传入的实例对象 return bound_function_wrapper(descriptor, instance, self.wrapper, self.binding, self.parent) return selfclass function_wrapper(object_proxy): def __init__(self, wrapped, wrapper): super(function_wrapper, self).__init__(wrapped) self.wrapper = wrapper if isinstance(wrapped, classmethod): self.binding = 'classmethod' elif isinstance(wrapped, staticmethod): self.binding = 'staticmethod' else: self.binding = 'function' def __get__(self, instance, owner): wrapped = self.wrapped.__get__(instance, owner) return bound_function_wrapper(wrapped, instance, self.wrapper, self.binding, self) def __call__(self, *args, **kwargs): return self.wrapper(self.wrapped, None, args, kwargs) 再次运行测试得到如下所示 1234567891011121314151617&gt;&gt;&gt; Class.function_rm = Class.function_im&gt;&gt;&gt; c.function_rm(1, 2)INSTANCE &lt;__main__.Class object at 0x105609790&gt;ARGS (1, 2)# 不会发生二次绑定&gt;&gt;&gt; Class.function_rm = Class.function_cm&gt;&gt;&gt; c.function_rm(1, 2)INSTANCE &lt;class '__main__.Class'&gt;ARGS (1, 2)# 不会发生二次绑定&gt;&gt;&gt; Class.function_rm = Class.function_sm&gt;&gt;&gt; c.function_rm(1, 2)INSTANCE NoneARGS (1, 2) 5. 装饰器应用顺序目前为止，我们的装饰器一直被放置在将方法标记为类方法或静态方法的装饰器之外。如果我们颠倒顺序会怎样？ 1234567891011121314151617181920212223242526272829class Class(object): @classmethod @my_function_wrapper def function_cm(self, a, b): pass @staticmethod @my_function_wrapper def function_sm(a, b): passc = Class()&gt;&gt;&gt; c.function_cm(1,2)INSTANCE NoneARGS (&lt;class '__main__.Class'&gt;, 1, 2)&gt;&gt;&gt; Class.function_cm(1, 2)INSTANCE NoneARGS (&lt;class '__main__.Class'&gt;, 1, 2)&gt;&gt;&gt; c.function_sm(1,2)INSTANCE NoneARGS (1, 2)&gt;&gt;&gt; Class.function_sm(1, 2)INSTANCE NoneARGS (1, 2) 静态方法按预期运行，但是类方法不行。在这个特殊的例子中，它实际上可以被看作是Python本身的一个bug。具体地说，classmethod 装饰器本身并不能对它包装的所有对象都遵守描述符协议。这也是为什么当使用闭包实现装饰器会发生错误的原因。如果classmethod 装饰器能正常工作，一起都是OK 的。对于那些对细节感兴趣的人，您可以在Python bug跟踪器中查看19072。 6. 装饰器一个类除了与类方法的装饰器顺序之外，我们实现的通用装饰器的模式看起来很好。我在上一篇文章中提到过，我们的目标是，我们也可以区分什么时候装饰器被应用到一个类中。所以让我们试试 1234567@my_function_wrapperclass Class(object): pass&gt;&gt;&gt; c = Class()INSTANCE NoneARGS () 基于此，我们无法将其与普通函数或类方法区分开来。如果我们再考虑一下，在这个例子中传递给包装器函数的包装对象将是类本身。让我们输出传递给用户包装函数的 wrapped参数，看看是否能区分出这种情景 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374@decoratordef my_function_wrapper(wrapped, instance, args, kwargs): print('WRAPPED', wrapped) print('INSTANCE', instance) print('ARGS', args) return wrapped(*args, **kwargs)@my_function_wrapperdef function(a, b): pass&gt;&gt;&gt; function(1, 2)WRAPPED &lt;function function at 0x10e13bb18&gt;INSTANCE NoneARGS (1, 2)class Class(object): @my_function_wrapper def function_im(self, a, b): pass @my_function_wrapper @classmethod def function_cm(self, a, b): pass @my_function_wrapper @staticmethod def function_sm(a, b): passc = Class()&gt;&gt;&gt; c.function_im(1,2)WRAPPED &lt;bound method Class.function_im of &lt;__main__.Class object at 0x107e90950&gt;&gt;INSTANCE &lt;__main__.Class object at 0x107e90950&gt;ARGS (1, 2)&gt;&gt;&gt; Class.function_im(c, 1, 2)WRAPPED &lt;functools.partial object at 0x107df3208&gt;INSTANCE &lt;__main__.Class object at 0x107e90950&gt;ARGS (1, 2)&gt;&gt;&gt; c.function_cm(1,2)WRAPPED &lt;bound method type.function_cm of &lt;class '__main__.Class'&gt;&gt;INSTANCE &lt;class '__main__.Class'&gt;ARGS (1, 2)&gt;&gt;&gt; Class.function_cm(1, 2)WRAPPED &lt;bound method type.function_cm of &lt;class '__main__.Class'&gt;&gt;INSTANCE &lt;class '__main__.Class'&gt;ARGS (1, 2)&gt;&gt;&gt; c.function_sm(1,2)WRAPPED &lt;function function_sm at 0x107e918c0&gt;INSTANCE NoneARGS (1, 2)&gt;&gt;&gt; Class.function_sm(1, 2)WRAPPED &lt;function function_sm at 0x107e918c0&gt;INSTANCE NoneARGS (1, 2)@my_function_wrapperclass Class(object): passc = Class()&gt;&gt;&gt; c = Class()WRAPPED &lt;class '__main__.Class'&gt;INSTANCE NoneARGS () 答案是肯定的，因为它是唯一一个被包装对象是类型对象的情况。 7. 通用装饰器结构我们的目标是，一个装饰器能同时被应用在普通函数，示例方法，类方法以及类上。比较特殊的是静态方法，但是实践中，静态方法与函数并没有本质上的不同，只是它被放在不同的地方。在装饰器的执行过程中区分出静态方法是必要的，但是静态方法不会包含任何连接到它所在的类的参数。如果需要，在开始更应该创建一个类方法。最后我们的通用装饰器可以被展示如下: 12345678910111213141516@decoratordef universal(wrapped, instance, args, kwargs): if instance is None: if inspect.isclass(wrapped): # Decorator was applied to a class. return wrapped(*args, **kwargs) else: # Decorator was applied to a function or staticmethod. return wrapped(*args, **kwargs) else: if inspect.isclass(instance): # Decorator was applied to a classmethod. return wrapped(*args, **kwargs) else: # Decorator was applied to an instancemethod. return wrapped(*args, **kwargs) 这样的通用装饰器有实际用途吗?我相信有一些很好的例子，我将在随后的博客文章中特别提到其中一个。其他一些框架，比如Django，也使用了一些技巧来创建同时适用于函数和实例方法的装饰器。事实证明，他们使用的方法是不正确的，因为它不遵守描述符协议。如果您对此感兴趣，请参见Django bug跟踪器中的第21247号问题。下一篇博客中将介绍一些具有可选参数的装饰器的问题，通用装饰器的使用实例留在以后展示。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"wrapt","slug":"wrapt","permalink":"http://yoursite.com/tags/wrapt/"}]},{"title":"03 使用工厂函数创建装饰器","slug":"wrapt/python_decorator_03","date":"2018-05-11T16:00:00.000Z","updated":"2020-05-24T15:30:43.340Z","comments":true,"path":"2018/05/12/wrapt/python_decorator_03/","link":"","permalink":"http://yoursite.com/2018/05/12/wrapt/python_decorator_03/","excerpt":"上一篇文章描述了一种基于代理对象创建装饰器的模式，并且通过将装饰器实现为一个描述符，解决了当装饰器应用于类方法时，对象绑定问题。代理对象和描述符的组合自动确保了内省机制能正常进行。现在的问题是如何消除样本代码来解决代码复用的问题。","text":"上一篇文章描述了一种基于代理对象创建装饰器的模式，并且通过将装饰器实现为一个描述符，解决了当装饰器应用于类方法时，对象绑定问题。代理对象和描述符的组合自动确保了内省机制能正常进行。现在的问题是如何消除样本代码来解决代码复用的问题。本文我们将进一步改进创建装饰器的方式，通过使用装饰器工厂函数，来抽象装饰器的创建，用户只需提供一个执行所需功能的的包装函数即可。 1. 装饰器的实现模式如前所述，我们需要一个代理对象，其实现如下 123456789101112131415class object_proxy(object): def __init__(self, wrapped): self.wrapped = wrapped try: self.__name__= wrapped.__name__ except AttributeError: pass @property def __class__(self): return self.wrapped.__class__ def __getattr__(self, name): return getattr(self.wrapped, name) 正如最后一次指出的那样，这是对它所做事情的最小表示。一个通用的对象代理需要做更多的工作。 描述符本身将按照如下模式实现 12345678910111213141516171819class bound_function_wrapper(object_proxy): def __init__(self, wrapped): super(bound_function_wrapper, self).__init__(wrapped) def __call__(self, *args, **kwargs): return self.wrapped(*args, **kwargs)class function_wrapper(object_proxy): def __init__(self, wrapped): super(function_wrapper, self).__init__(wrapped) def __get__(self, instance, owner): wrapped = self.wrapped.__get__(instance, owner) return bound_function_wrapper(wrapped) def __call__(self, *args, **kwargs): return self.wrapped(*args, **kwargs) 当将装饰器应用于一个正常的函数时，将使用包装器的 __call__()方法。如果将包装器应用于类的方法，则在属性访问时调用 __get__() 方法，返回一个新的绑定对象之后的装饰器，并在被调用时调用新的装饰器的__call__()方法。这使得我们的包装器能作为描述符来传递描述符协议，以根据需要对包装的对象进行绑定。 2. 创建装饰器的装饰器正常工作的装饰器有一个固定的实现模式，因此，我们可以使用工场函数抽象装饰器创建的过程，工厂函数可以作为一个装饰器使用，创建一个装饰器的过程如下: 1234567@decoratordef my_function_wrapper(wrapped, args, kwargs): return wrapped(*args, **kwargs)@my_function_wrapperdef function(): pass 这个装饰器工厂函数 decorator 应该怎么实现呢？就像表现的一样，我们的装饰器工厂函数是非常简单的，与partial()函数并没有很大不同，在装饰器定义时接收用户提供的包装函数，在装饰器应用时接收被包装函数，并将他们传递到function wrapper对象中。 12345def decorator(wrapper): @functools.wraps(wrapper) def _decorator(wrapped): return function_wrapper(wrapped, wrapper) return _decorator 我们现在只需要修改我们的装饰器 function wrapper 对象的实现，将包装对象的实际执行委托给用户提供的包装函数。 123456789101112131415161718192021class bound_function_wrapper(object_proxy): def __init__(self, wrapped, wrapper): super(bound_function_wrapper, self).__init__(wrapped) self.wrapper = wrapper def __call__(self, *args, **kwargs): return self.wrapper(self.wrapped, args, kwargs)class function_wrapper(object_proxy): def __init__(self, wrapped, wrapper): super(function_wrapper, self).__init__(wrapped) self.wrapper = wrapper def __get__(self, instance, owner): wrapped = self.wrapped.__get__(instance, owner) return bound_function_wrapper(wrapped, self.wrapper) def __call__(self, *args, **kwargs): return self.wrapper(self.wrapped, args, kwargs) function_wrapper 和 bound_function_wrapper 同时接收包装函数，和被包装函数，并将 __call__() 实际执行委托给用户提供的包装函数，由用户调用被包装函数并返回值。 因此，我们可以使用工厂来简化创建装饰器的过程。现在让我们来检查一下，在所有的情况下，这将在实际工作中发挥作用，并且看看我们还能找到什么其他的问题，以及我们是否能在这些情况下改进。 3. 装饰类方法第一个可能导致问题的领域是创建一个单独的decorator，它可以同时处理类的正常函数和实例方法。为了测试我们的新decorator是如何工作的，我们可以在调用包装函数时打印传递给包装器的args，并可以比较结果。 1234@decoratordef my_function_wrapper(wrapped, args, kwargs): print('ARGS', args) return wrapped(*args, **kwargs) 首先让我们尝试包装一个普通函数: 123456@my_function_wrapperdef function(a, b): pass&gt;&gt;&gt; function(1, 2)ARGS (1, 2) 正如所期望的那样，在函数被调用时，只有两个参数被输出。包装一个实例方法会如何？ 123456789class Class(object): @my_function_wrapper def function_im(self, a, b): passc = Class()&gt;&gt;&gt; c.function_im()ARGS (1, 2) 同样，当调用实例方法时传入的两个参数被输出。因此，装饰器对正常函数和实例方法的工作方式是相同的。 这里的问题是，用户如何在他们的包装函数中获取类的实例。当函数被绑定到类的实例时，我们丢失了这个信息，因为类实例现在与传入的绑定函数关联，而不是参数列表。要解决这个问题，我们可以记住在调用绑定函数时传递给 __get__() 方法的实例是什么。在 bound wrapper被创建，作为参数传递给bound wrapper。 12345678910111213141516171819202122class bound_function_wrapper(object_proxy): def __init__(self, wrapped, instance, wrapper): super(bound_function_wrapper, self).__init__(wrapped) self.instance = instance self.wrapper = wrapper def __call__(self, *args, **kwargs): return self.wrapper(self.wrapped, self.instance, args, kwargs)class function_wrapper(object_proxy): def __init__(self, wrapped, wrapper): super(function_wrapper, self).__init__(wrapped) self.wrapper = wrapper def __get__(self, instance, owner): wrapped = self.wrapped.__get__(instance, owner) return bound_function_wrapper(wrapped, instance, self.wrapper) def __call__(self, *args, **kwargs): return self.wrapper(self.wrapped, None, args, kwargs) 在bound wrapper中，类实例作为额外的参数传给用户创建的包装函数。对于普通函数，在顶级包装器中，对于这个新的实例参数，我们没有传递任何内容。现在，我们可以修用户的包装函数，以输出实例和传递的参数。 12345678910111213@decoratordef my_function_wrapper(wrapped, instance, args, kwargs): print('INSTANCE', instance) print('ARGS', args) return wrapped(*args, **kwargs)&gt;&gt;&gt; function(1, 2)INSTANCE NoneARGS (1, 2)&gt;&gt;&gt; c.function_im(1, 2)INSTANCE &lt;__main__.Class object at 0x1085ca9d0&gt;ARGS (1, 2) 因此，这种变化能让我们在包装器函数中区分出一个普通函数调用和一个的实例方法调用。对实例的引用甚至是单独传递的，在调用原始被包装函数时，我们不必为一个实例方法去判断并移除额外的类实例参数。对于类，原始的被包装函数已经是绑定对象，所以不能在传入类实例对象。 需要注意的是实例方法可以通过类，显示传递类实例来调用，我们需要验证这种情况是否仍然符合我们的要求。 123&gt;&gt;&gt; Class.function_im(c, 1, 2)INSTANCE NoneARGS (&lt;__main__.Class object at 0x1085ca9d0&gt;, 1, 2) 不幸的是，将实例显式地传递给类中的函数作为参数时，类实例没有通过 instance 传递给包装函数，而是作为 arg 的第一个参数被传递。这并不是一个理想的结果 为了处理这种变化，我们可以在调用bound_function_wrapper.__call__()之前检查实例，并从参数列表的开头弹出实例。然后使用 partcial 函数将实例绑定到被包装函数上，并调用用户的包装函数。 1234567891011121314class bound_function_wrapper(object_proxy): def __call__(self, *args, **kwargs): if self.instance is None: instance, args = args[0], args[1:] wrapped = functools.partial(self.wrapped, instance) return self.wrapper(wrapped, instance, args, kwargs) return self.wrapper(self.wrapped, self.instance, args, kwargs)# We then get the same result no matter whether the instance method is called via the class or not.&gt;&gt;&gt; Class.function_im(c, 1, 2)INSTANCE &lt;__main__.Class object at 0x1085ca9d0&gt;ARGS (1, 2) 对于实例方法，一切都可以正常执行，被包装函数无论是实例方法和还是普通函数接收参数完全相同。得益与 instance 参数，在将装饰器应用于实例方法时，我们可以按需调用类方法。 对于类可以拥有的其他方法类型，特别是类方法和静态方法会怎样？ 12345678910class Class(object): @my_function_wrapper @classmethod def function_cm(cls, a, b): pass&gt;&gt;&gt; Class.function_cm(1, 2)INSTANCE 1ARGS (2,) 正如所看见得，装饰器对类方法和静态方法有非常严重得问题。这两种情况下，在函数被绑定时，instance 参数将为空。此时传递给函数的第一实参将被传递给 instance，这显然是不正确的，应该怎么做？ 4. 通用装饰器所以我们并没有完成一个通用的装饰器，但我们到底想要达到什么目的呢?我们最初的装饰模式有什么问题?这里的终极目标是我所说的“通用装饰器”。一个可以应用于普通函数、实例方法、类方法、静态方法甚至是类的修饰符，修饰符能够在使用的时候自动适用它被使用的上下文。 目前为止，实现装饰器的所有方法想达到上述目标是不可能了。只能通过复制代码，或者通过某种技巧转换装饰器，以便装饰器能在不同的上下文中使用。我的目标是能实现如下功能: 123456789101112@decoratordef universal(wrapped, instance, args, kwargs): if instance is None: if inspect.isclass(wrapped): # class. else: # function or staticmethod. else: if inspect.isclass(instance): # classmethod. else: # instancemethod. 本文中，我们已经实现了让装饰器在普通函数和实例方法上正确执行，我们现在需要了解如何处理类方法、静态方法以及将装饰器应用于类的场景。本系列的下一篇文章将继续追求这个目标，并描述如何进一步调整我们的装饰器。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"wrapt","slug":"wrapt","permalink":"http://yoursite.com/tags/wrapt/"}]},{"title":"02 装饰器与描述符协议","slug":"wrapt/python_decorator_02","date":"2018-05-07T16:00:00.000Z","updated":"2020-05-24T15:30:38.513Z","comments":true,"path":"2018/05/08/wrapt/python_decorator_02/","link":"","permalink":"http://yoursite.com/2018/05/08/wrapt/python_decorator_02/","excerpt":"上一篇文章说明了普通函数实现的装饰器存在的问题。本文我们将着眼于之前阐述的最后一个问题，如何将装饰器应用到一个描述符上。","text":"上一篇文章说明了普通函数实现的装饰器存在的问题。本文我们将着眼于之前阐述的最后一个问题，如何将装饰器应用到一个描述符上。 1. 描述符协议有关 Python 的对象模型和底层设计原理推荐大家读一读《流畅的Python》，这里不会详细解释描述符是什么以及他们的工作原理。简而言之，描述符就是存在绑定行文的对象，即属性访问会被描述符协议实现的方法所覆盖。实现描述符协议的特殊方法包括 __get__(), __set__(), 和 __delete__()。如果任意一中方法在一个对象中被定义，就可以说该对象是一个描述符** 123obj.attribute attribute.__get_(obj.type(obj))obj.attribute = value attribute.__set_(obj, value)del obj.attribute attribute.__delete_(obj, value) 上述描述的是，如果一个类的属性包含上述任意一中特殊方法，当相应操作在类属性被执行时，这些特殊方法将取代默认方法被调用。这就允许一个属性去覆盖将发生默认操作。 也许你以为你从未使用过描述符，事实上，函数对象就是描述符。当在类中定义函数时，函数就是普通的函数。当你通过’.’属性访问函数时，你将调用函数的 __get__()方法，将函数与一个类实例绑定，进而返回一个绑定方法对象** 123456789101112def f(obj): pass&gt;&gt;&gt; hasattr(f, '__get__')True&gt;&gt;&gt; f&lt;function f at 0x10e963cf8&gt;&gt;&gt;&gt; obj = object()&gt;&gt;&gt; f.__get__(obj, type(obj))&lt;bound method object.f of &lt;object object at 0x10e8ac0b0&gt;&gt; 所以当你调用类方法时，调用的不是原始函数的 __call__()，而是访问函数时临时创建的绑定方法对象的 __call__() 方法，当然，你通常不会看到所有这些中间步骤，只看到结果。 1234567&gt;&gt;&gt; class Object(object):... def f(self): pass&gt;&gt;&gt; obj = Object()&gt;&gt;&gt; obj.f&lt;bound method Object.f of &lt;__main__.Object object at 0x10abf29d0&gt;&gt; 现在回想一下在第一个博客文章中给出的例子，当我们对一个类方法应用了装饰器时，我们遇到了如下错误: 12345678910111213class Class(object): @function_wrapper @classmethod def cmethod(cls): pass&gt;&gt;&gt; Class.cmethod()Traceback (most recent call last): File \"classmethod.py\", line 15, in &lt;module&gt; Class.cmethod() File \"classmethod.py\", line 6, in _wrapper return wrapped(*args, **kwargs)TypeError: 'classmethod' object is not callable 示例中的问题在于 @classmethod 装饰器返回的 classmethod 对象本身并没有 __call__() 方法，__call__() 方法仅存在于 classmethod 对象__get__()被调用时返回的结果中。 更具体的说， 人们使用的简单装饰器，并没有对被包装的描述符对象执行描述符协议以产生的一个可调用对象。想反，只是简单的直接调用被包装对象。因为其没有 __call__() 方法，结果当然会失败。 那为什么将装饰器应用在普通的实例方法上仍然可以运行呢？原因是一个普通函数本身具有 __call__() 方法，包装函数直接调用的是此方法。而且尽管绑定步骤被跳过，但是包装函数将 self 包含的实例对象通过第一参数显示传递给了原始的未绑定函数对象。因此对于一个普通的实例方法包装前后调用实际上是相同的，只有当被包装的对象(如@classmethod)依赖于正确应用的描述符协议时，才会崩溃。 2. 包装描述符对象解决包装器不能在类方法执行描述符协议获取绑定对象的方法是，让包装器也成为一个描述符对象。 1234567891011121314class bound_function_wrapper(object): def __init__(self, wrapped): self.wrapped = wrapped def __call__(self, *args, **kwargs): return self.wrapped(*args, **kwargs)class function_wrapper(object): def __init__(self, wrapped): self.wrapped = wrapped def __get__(self, instance, owner): wrapped = self.wrapped.__get__(instance, owner) return bound_function_wrapper(wrapped) def __call__(self, *args, **kwargs): return self.wrapped(*args, **kwargs) 如果将包装器应用于一个正常的函数，则使用包装器的 __call__()方法。如果将包装器应用于类的方法，则调用__get__()方法，该方法返回一个新的绑定包装器，并调用该方法的 __call__() 方法。这样我们的包装器就可以在描述符的传播过程中使用。 因为将装饰器实现为一个描述符对象时，使用闭包总是会失败，因此这种情况下为了让所有的事都能正常工作，我们必需总是使用类实现装饰器。装饰器类将实现描述符协议，如上所式。 现在的问题是，我们如何解决我们列出的其他问题。我们使用functools.wrap() 和 functools.update_wrapper() 解决命名问题，现在我们应该怎么做以便继续使用他们。因为 functools.wrap() 内部使用 update_wrapper(),所以我们只需要看看它如何实现。 12345678910111213141516171819WRAPPER_ASSIGNMENTS = ('__module__', '__name__', '__qualname__', '__doc__', '__annotations__')WRAPPER_UPDATES = ('__dict__',)def update_wrapper(wrapper, wrapped, assigned = WRAPPER_ASSIGNMENTS, updated = WRAPPER_UPDATES): wrapper.__wrapped__ = wrapped for attr in assigned: try: value = getattr(wrapped, attr) except AttributeError: pass else: setattr(wrapper, attr, value) for attr in updated: getattr(wrapper, attr).update( getattr(wrapped, attr, &#123;&#125;)) 如上展示的是Python3.3中的代码，事实上它还存在一个bug，在Python3.4中已经修复。 在函数体中，3件事需要被做。 第一件是将被包装函数保存为包装函数的__wrapped__属性。这就是那个bug，因为它应该在最后实现 第二步，复制诸如 __name__ 和 __doc__ 属性； 最后一步，复制被包装函数dict属性值到包装函数，结果是很多对象需要被复制 如果我们使用的是一个函数闭包或直接的类包装器，那么这个复制就可以在decorator应用的时候完成。当装饰器被实现为描述符时，也需要在 bound wrapper 中完成上述工作。 123456789class bound_function_wrapper(object): def __init__(self, wrapped): self.wrapped = wrapped functools.update_wrapper(self, wrapped)class function_wrapper(object): def __init__(self, wrapped): self.wrapped = wrapped functools.update_wrapper(self, wrapped) 因为bound wrapper 在包装器每次被作为类的绑定方法调用时都会被创建，所有将非常慢。我们需要更高效的方式处理它。 2. 代理对象性能问题的解决方法是，使用代理对象。这是一个特殊的包装类，因为它的行为跟它包装的东西看起来很像。 123456789101112131415class object_proxy(object): def __init__(self, wrapped): self.wrapped = wrapped try: self.__name__= wrapped.__name__ except AttributeError: pass @property def __class__(self): return self.wrapped.__class__ def __getattr__(self, name): return getattr(self.wrapped, name) 一个完全透明的对象代理本身就是一个复杂的怪物，所以我打算暂时把细节掩盖起来，并在一个单独的博客文章中讨论它。上面的例子是它所做事情的最小表示。实际上，它实际上需要做更多的工作。简而言之，它将有限的属性从包装的对象复制到自身，并使用特殊的方法、属性和 __getattr__() 来从包装对象中获取属性，从而避免需要复制许多可能永远不会被访问的属性。 我们现在要做的是从对象代理中派生出包装器类，并取消调用update_wrapper()。 12345678910111213141516171819class bound_function_wrapper(object_proxy): def __init__(self, wrapped): super(bound_function_wrapper, self).__init__(wrapped) def __call__(self, *args, **kwargs): return self.wrapped(*args, **kwargs) class function_wrapper(object_proxy): def __init__(self, wrapped): super(function_wrapper, self).__init__(wrapped) def __get__(self, instance, owner): wrapped = self.wrapped.__get__(instance, owner) return bound_function_wrapper(wrapped) def __call__(self, *args, **kwargs): return self.wrapped(*args, **kwargs) 现在从包装器中查询像 __name__ 和 __doc__ 这样的属性时，将从被包装函数直接返回。使用透明的对象代理也意味着像 inspect.getargspec() 和 inspection.getsource() 这样的调用也将按照预期正常工作。 3. 代码复用尽管这种模式解决了最初确定的问题，但它包含了大量的重复样板代码。此外，在现在的代码中有两个位置，调用被包装函数。因而需要在两个地方重复实现包装逻辑。因此，每次需要实现一个装饰器时都要复制这一点，因此会有点痛苦。 我们可以做的是将整个过程打包到一个装饰器工厂函数中，从而避免每次都需要手工完成这一切。如何做到这一点将成为本系列下一篇博客文章的主题。从这一点开始，我们可以开始研究如何进一步改进功能，并引入新的功能，这些都是使用常规的装饰器实现方法难以实现的。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"wrapt","slug":"wrapt","permalink":"http://yoursite.com/tags/wrapt/"}]},{"title":"01 如何实现一个 Python 装饰器","slug":"wrapt/python_decorator_01","date":"2018-05-03T16:00:00.000Z","updated":"2020-05-24T15:30:33.990Z","comments":true,"path":"2018/05/04/wrapt/python_decorator_01/","link":"","permalink":"http://yoursite.com/2018/05/04/wrapt/python_decorator_01/","excerpt":"稍微对 Python 有所了解的程序员一定知道 Python 装饰器和函数闭包。我曾经也以为很了解，直到在《流畅的Python》中看到了 Wrapt 模块。","text":"稍微对 Python 有所了解的程序员一定知道 Python 装饰器和函数闭包。我曾经也以为很了解，直到在《流畅的Python》中看到了 Wrapt 模块。 Wrapt 模块的作者 Graham Dumpleton 先生写了 14 篇博客详细讲解了如何在 Python 中实现一个能同时包装函数，类方法，实例方法的通用装饰器。本文以及接下来几篇文章是我对那 14 篇博客的整理和笔记。 Graham Dumpleton 先生的博文 和 Wrapt 模块请参阅: GrahamDumpleton wrapt blog wrapt 1.10.11 documentation 1. 通过函数闭包实现装饰器装饰器的典型目的是为被包装函数附加的额外的处理逻辑。我遇到的使用装饰器的最典型场景是，大多数数据库对一次查询可设置的查询的条件有数量限制，大量查询时需要将多个查询条件分组进行多次查询在合并查询结果。比如我有100000 用户需要根据ID 查询其性别，查询条件太多，只能分批多次查询，然后将查询结果合并。这种分批查询不仅对 mysql，对其他任何数据库都适用，所以非常适用用装饰器将分批查询再合并的功能抽象出来。 1.1 实现原理大多数人(我)都是通过闭包来创建一个装饰器，就像下面这样。1234567891011def function_wrapper(wrapped): def _wrapper(*args, **kwargs): return wrapped(*args, **kwargs) return _wrapper# @ 符应用一个装饰器在Python2.4 中被加入。它仅仅是如下方式的语法糖@function_wrapperdef function(): passfunction = function_wrapper(function) 整个包装的执行过程如下: 包装函数(function_wrapper)接收被包装函数(wrapped)作为参数，并将内部的另一个内部函数(_wrapper) 作为返回值 通过@装饰器或函数的调用赋值，使用 _wrapper 替换 wrapped，这样对 wrapped 的调用实际是调用的 _wrapped _wrapped 通过函数闭包保留了对 wrapped 函数的引用，这样它就可以在内部调用 wrapped 函数并返回调用结果。 _wrapped 在调用 wrapped 之前或之后可以添加其他处理逻辑，以达到为 wrapped 附加功能的目的。 虽然通常都是适用函数闭包实现装饰器，但是能展示它工作原理的更好的示例是使用一个类实现它: function_wrapper 类通过属性保留对被包装函数的引用 当被包装函数被调用时，包装类的 __call__ 方法被调用，并进而调用原始的被包装函数 __call__ 包含了附加的通用处理逻辑。 12345678class function_wrapper(object): def __init__(self, wrapped): self.wrapped = wrapped def __call__(self, *args, **kwargs): return self.wrapped(*args, **kwargs)@function_wrapperdef function(): pass 1.2 局限尽管通过闭包实现装饰器很简单，但是这种方式存在很多局限，其中最重要的是打断了 Python 内部的自省，也没有遵循 Python 对象模型的执行方式。 猴子补丁与装饰器十分相似的一个技术是 monkey patching(猴子打补丁)，猴子打补丁会进入并修改其他人的代码。二者不同的是装饰器作用的时间是函数定义完成之后，而猴子补订在函数导入模块时被应用。为了能同时使用函数包装器和猴子补丁，函数包装器必需是透明的，并且内部维护了一个堆，以便多个装饰器，猴子补订能按照预期的顺序执行。 2. 自省丢失当我们讨论函数闭包时，我们会预期函数的自省属性和函数的外在表现相一致。这些包括__name__，__doc__ 属性。但是当使用函数闭包时，原函数的自省属性会被内嵌函数所替代，因为函数闭包返回的是内嵌函数。 1234567891011def function_wrapper(wrapped): def _wrapper(*args, **kwargs): return wrapped(*args, **kwargs) return _wrapper@function_wrapperdef function(): pass&gt;&gt;&gt; print(function.__name__)_wrapper 当使用类实现闭包时，类实例没有 __name__ 属性，访问此属性时，会导致 AttributeError 异常 1234567891011121314class function_wrapper(object): def __init__(self, wrapped): self.wrapped = wrapped def __call__(self, *args, **kwargs): return self.wrapped(*args, **kwargs)@function_wrapperdef function(): pass&gt;&gt;&gt; print(function.__name__)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;AttributeError: 'function_wrapper' object has no attribute '__name__' 此处的解决方式是，在函数闭包内，将被包装函数的内省属性复制到内嵌函数上。这样函数名称和文档字符串属性就能表现正常 12345678910111213def function_wrapper(wrapped): def _wrapper(*args, **kwargs): return wrapped(*args, **kwargs) _wrapper.__name__ = wrapped.__name__ _wrapper.__doc__ = wrapped.__doc__ return _wrapper@function_wrapperdef function(): pass&gt;&gt;&gt; print(function.__name__)function 手动复制属性是费劲的，如果未来扩展了其他自省属性，代码需要被更新。例如需要复制 __module__ 属性，在Python3 中需要复制 __qualname__ 和 __annotations__ 属性。为了避免这么做，Python 标准库为我们提供了 functools.wraps() 装饰器，完成自省属性的复制 1234567891011121314import functoolsdef function_wrapper(wrapped): @functools.wraps(wrapped) def _wrapper(*args, **kwargs): return wrapped(*args, **kwargs) return _wrapper@function_wrapperdef function(): pass&gt;&gt;&gt; print(function.__name__)function 使用类实现装饰器时，我们需要使用 functools.update_wrapper() 函数 12345678import functoolsclass function_wrapper(object): def __init__(self, wrapped): self.wrapped = wrapped functools.update_wrapper(self, wrapped) def __call__(self, *args, **kwargs): return self.wrapped(*args, **kwargs) 或许你已经认为通过 functolls.wraps 函数我们能确保函数的自省属性是正确的，但事实上它并不能一直有效。假如我们去访问函数的参数信息，返回的将是包装函数的参数信息而不是被包装函数的。即，在使用闭包的方式中，内嵌函数的参数信息被返回。因此包装器没能保留函数签名信息 123456789import inspectdef function_wrapper(wrapped): ...@function_wrapperdef function(arg1, arg2): pass&gt;&gt;&gt; print(inspect.getargspec(function))ArgSpec(args=[], varargs='args', keywords='kwargs', defaults=None) 类包装器更加严重，因为会触发异常，并解释称被包装函数不是一个函数。我们完全不能获取函数签名信息，即使被包装函数是可调用的 123456789101112class function_wrapper(object): ...@function_wrapperdef function(arg1, arg2): pass&gt;&gt;&gt; print(inspect.getargspec(function))Traceback (most recent call last): File \"...\", line XXX, in &lt;module&gt; print(inspect.getargspec(function)) File \".../inspect.py\", line 813, in getargspec raise TypeError('&#123;!r&#125; is not a Python function'.format(func))TypeError: &lt;__main__.function_wrapper object at 0x107e0ac90&gt; is not a Python function 另外一个自省的示例是使用 inspect.getsource() 获取函数源代码。闭包装饰器返回的是内嵌函数的源代码，而类装饰器则会触发异常 3.描述符协议同函数类似，装饰器也可以应用于类方法。Python 包含了两个特殊的装饰器@classmethod 和 @staticmethod 将实例方法转换为特殊的类方法。装饰器应用于类方法同样隐含着几个问题 12345678910111213class Class(object): @function_wrapper def method(self): pass @classmethod def cmethod(cls): pass @staticmethod def smethod(): pass 第一即使使用了 functools.wraps 或者 functools.update_wrapper，当装饰器被用在 @classmethod，@staticmethod 上时，仍然会导致异常。这是因为这两个特殊的装饰器没能将一些必要的属性复制过来。这是一个Python2 的bug，并在Python3中通过忽略丢失的属性修复了 12345678910111213class Class(object): @function_wrapper @classmethod def cmethod(cls): passTraceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"&lt;stdin&gt;\", line 3, in Class File \"&lt;stdin&gt;\", line 2, in wrapper File \".../functools.py\", line 33, in update_wrapper setattr(wrapper, attr, getattr(wrapped, attr))AttributeError: 'classmethod' object has no attribute '__module__' 即使我们运行在 Python3 上，我们依然会遇到问题。这是因为所有类型的装饰器都假设被包装函数是直接可调用的。事实上并非如此。Python classmethod 装饰器返回一个描述符，这个描述符不是直接可调用的，但是装饰器假设被包装函数直接可调用，因此会出错。 12345678910111213class Class(object): @function_wrapper @classmethod def cmethod(cls): pass&gt;&gt;&gt; Class.cmethod()Traceback (most recent call last): File \"classmethod.py\", line 15, in &lt;module&gt; Class.cmethod() File \"classmethod.py\", line 6, in _wrapper return wrapped(*args, **kwargs)TypeError: 'classmethod' object is not callable 4. 总结函数闭包实现的装饰器存在以下问题: 无法保留函数的自省属性 无法获取函数签名信息 无法获取函数源代码 无法将装饰器应用于另一个为实现描述符的装饰器之上.简单的装饰器实现不会遵守被包装对象的描述符协议，因而破坏了Python对象的执行模型 使用 functools.wraps() 和 functools.update_wrapper() 能保留常规的自省属性，但依旧无法保留函数签名信息和源代码，而且由于 Python2 的bug，无法将装饰器直接应用于类方法和静态方法(导入时即报错) 确实存在第三方包，尝试解决这些问题，例如PyPi上的decorator模块。这个模块虽然对前两类问题有所帮助，但仍然存在一些潜在的问题，当尝试通过猴子补丁动态应用函数包装时，可能会导致问题 这并不意味着这些问题是不可解决的，而且可以以一种不牺牲性能的方式解决。现在已经说明了要解决的问题，在随后的文章将会解释如何解决这些问题，以及提供哪些额外的功能。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"wrapt","slug":"wrapt","permalink":"http://yoursite.com/tags/wrapt/"}]},{"title":"Python 单元测试","slug":"unittest/unittest_01","date":"2017-09-21T16:00:00.000Z","updated":"2020-05-24T15:40:20.149Z","comments":true,"path":"2017/09/22/unittest/unittest_01/","link":"","permalink":"http://yoursite.com/2017/09/22/unittest/unittest_01/","excerpt":"至关重要的单元测试","text":"至关重要的单元测试 1. 单元测试Python 中有多个单元测试框架，最常用的应该是 unittest。本文的目的就是想系统介绍一下 unittest 的使用。 除了各种单元测试框架，Python 中还有不少可以用来辅助单元测试的模块，我看到的最有用但是却很少人使用的是 wrapt。wrapt 模块提供的功能可以部分替代unittest.mock ，简化单元测试的复杂度。这是我们将介绍的第二部分内容。 本文思路和部分内容借鉴自雨痕老师的Python3 学习笔记，这应该是既流畅的Python 之后我第二推荐的书，很适合有 Python经验的同学作为 Python 学习的补充。 下面我们就正式开始我们的内容。 2. Unittest2.1 Unittest 框架 从上面的 Unittest 框架，我们大体上就能看出使用 Unittest 进行单元测试的整个流程: 构建测试用例(Suit) 通过加载器选择我们想要测试的测试用例(Loader) 执行器执行测试返回结果(Runner) 下面是与框架对应的相关组件: TestLoader: 加载器，查找测试方法 Suit: 测试构建组件 TestCase: 测试用例，实现一个到多个测试方法，测试的基础单元 FunctionTestCase: 继承自 TestCase 专门为函数准备的通用测试类型 TestSuite: 测试套件，组合多个用例或套件 TestRunner: 执行器，执行测试，返回测试结果 TestResult: 测试结果 我们目的是学习各个组件的常见使用，并了解各个组件之间调用的”钩子”，以帮助我们构建更好的单元测试。 2.2 调用钩子TestCase要想明白 unittest 各个组件之间的调用钩子，我们得从 unittest.TestCase 说起: unittest 中所有的测试用例都必须继承自 TestCase 通过实例化 TestCase 创建一个测试过程，其 run 方法是启动测试的钩子函数 run 方法只会执行在实例化 TestCase 通过 methodName 参数传入的方法(默认为runTest) 因此要想测试 TestCase 内多个测试方法，需要为每个测试方法创建一个 TestCase 实例 123class TestCase(object): def __init__(self, methodName='runTest'): pass TestSuite为了批量管理多个测试过程，unittest 提供了 TestSuite 测试套件。其内部的 _tests 属性和 addTests 方法用来收集和添加 TestCase 的实例(也包括TestSuite实例本身)；当其 run 被调用时，会依次调用 _tests 内收集的测试实例的 run 方法，启动每一个测试过程。 1234567891011121314151617class BaseTestSuite(object): \"\"\"A simple test suite that doesn't provide class or module shared fixtures. \"\"\" def __init__(self, tests=()): self._tests = [] self.addTests(tests) def addTests(self, tests): if isinstance(tests, basestring): raise TypeError(\"tests must be an iterable of tests, not a string\") for test in tests: self.addTest(test)class TestSuite(BaseTestSuite): def run(self, result, debug=False): pass TestLoaderTestLoader 是 unittest 用来查找测试方法，批量创建测试用例实例的组件。其提供了多种发现机制，可递归扫描目录，查找符合匹配条件的测试模块；或指定具体的模块，用例类型甚至是某个测试方法。其完整的发现流程是: discover 方法会递归目录，查找文件名与传入模式匹配的所有模块， 用 loadTestsFromModule 在模块内获取所有的测试用例类型 用 loadTestsFromTestCase 为发现的测试用例的全部测试方法创建实例 最终，上面的所有的实例组合成测试套件交给执行器 loadTestsFromTestCase 调用 getTestCaseNames 查找类型中包含特定前缀的测试方法，没有找到时返回 runTest。这个特定前缀由 TestLoader的testMethodPrefix类属性限定。 loadTestsFromModule 按照加载协议约定，先调用 load_tests 函数返回自定义测试套件。仅在模块内没有 load_tests 函数时，返回模块内的所有用例类型。 unittest 内置了默认的加载器实例 defaultTestLoader 可直接使用。 123456789101112131415161718192021222324252627282930313233343536class TestLoader(object): \"\"\" This class is responsible for loading tests according to various criteria and returning them wrapped in a TestSuite \"\"\" testMethodPrefix = 'test' sortTestMethodsUsing = cmp suiteClass = suite.TestSuite _top_level_dir = None def loadTestsFromTestCase(self, testCaseClass): \"\"\"Return a suite of all tests cases contained in testCaseClass\"\"\" pass def loadTestsFromModule(self, module, use_load_tests=True): \"\"\"Return a suite of all tests cases contained in the given module\"\"\" pass def loadTestsFromName(self, name, module=None): pass def loadTestsFromNames(self, names, module=None): \"\"\"Return a suite of all tests cases found using the given sequence of string specifiers. See 'loadTestsFromName()'. \"\"\" pass def getTestCaseNames(self, testCaseClass): \"\"\"Return a sorted sequence of method names found within testCaseClass \"\"\" pass def discover(self, start_dir, pattern='test*.py', top_level_dir=None): pass defaultTestLoader = TestLoader() TestRunnerTestRunner执行器用于启动测试过程并返回测试结果。其实例的run方法接受测试用例或套件作为参数，并调用它们的 run 方法，执行测试并返回测试结果。TestRunner 是默认的执行器类，其默认输出到 sys.stderr，可使用 stream 参数将结果保存到文件中。 1234567891011121314class TextTestRunner(object): \"\"\"A test runner class that displays results in textual form. It prints out the names of tests as they are run, errors as they occur, and a summary of the results at the end of the test run. \"\"\" resultclass = TextTestResult def __init__(self, stream=sys.stderr, descriptions=True, verbosity=1, failfast=False, buffer=False, resultclass=None): pass def run(self, test): pass 下面是 unittest 简单使用示例，现在你应该能明白整个测试的过程了。 12345678910111213141516171819202122from unittest import TestCaseclass DemoTest(TestCase): def test_1(self): self.assertTrue(True) def test_2(self): self.assertFalse(False)# 1. 直接启动测试用例# 加载器loader = unittest.defaultTestLoader# 测试用例suite = DemoTest('test_2')# 实例化执行器，调用 run 方法，传入测试用例，启动测试过程unittest.TextTestRunner(verbosity=2).run(suite) # unittest 默认的执行器# 2. 通过加载器启动测试用例# 加载器返回的测试套件suite = loader.loadTestsFromTestCase(DemoTest)unittest.TextTestRunner(verbosity=2).run(suite) 2.3 命令行unittest 还提供了命令行，通常我们只需要编写测试用例，通过 Python -m unittest 可直接启动测试过程； TestLoader 提供的测试方法发现机制可通过命令参数直接使用。下面是命令行的使用示例: 12345678# 启动方式二: 使用命令执行测试cd /home/tao/project/algo/unittest_demopython -m unittest demaopython -m unittest demao.DemoTestpython -m unittest demao.DemoTest.test_1cd ..python -m unittest discover unittest_demo/ \"de*.py\" 2.4 TestCase 使用为了方便测试，TestCase 还提供了很多钩子函数和断言方法，下面是一个简单的说明: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import unittestclass TestCase(object): def setUp(self): # 每个测试方法执行前后，调用 setUp/tearDown pass def tearDown(self): # pass @classmethod def setUpClass(cls): # 无论多少个实例，setUpClass/tearDownClass 仅执行一次 pass @classmethod def tearDownClass(cls): # pass def assertFalse(self, expr, msg=None): # 如果 expr 不为 False 测试失败 pass def assertTrue(self, expr, msg=None): # 如果 expr 不为 True 测试失败 pass def assertRaises(self, excClass, callableObj=None, *args, **kwargs): pass def assertEqual(self, first, second, msg=None): # 如果 first != second 测试失败 passclass DemoTest(TestCase): @unittest.expectedFailure # 期望测试失败 def test_1(self): self.assertTrue(False) @unittest.skip('reason....') # 忽略该测试，还有几个制定版本 def test_2(self): self.assertFalse(True) def test_3(self): # 测试是否抛出指定异常 with self.assertRaises(Exception): raise Exception() 3. wraptunittest 框架中真正复杂的是 unittest.mock。如果测试目标依赖其他对象或模块，我们可能就需要模拟(mock)替代它。但是 mock 模块使用起来很复杂，不直观，所以这篇文章我不打算讲解 mock。我们来看看如何使用 wrapt 来替代 mock来简化我们的测试。 wrapt 是Python 装饰器的工业级实现。由于 Python 装饰器和单元测试中的模拟行为非常相似，因此 wrapt 在实现一个通用装饰器的同时附加了辅助单元测试的接口。wrapt 使用便利，但是实现却相当复杂。以至于 Wrapt 模块的作者 Graham Dumpleton 先生写了 14 篇博客详细讲述 wrapt 的实现，下面是Graham Dumpleton 先生的博文 和 Wrapt 模块的文档: GrahamDumpleton wrapt blog wrapt 1.10.11 documentation 在此我们只举例说明如何使用，有关 wrapt 的实现后续会翻译Graham Dumpleton先生写的 14 篇博文，其中会详细介绍(译文:使用 wrapt 辅助测试)。 3.1 使用 wrapt 辅助单元测试wrapt 中有两个辅助单元测试的核心接口: transient_function_wrapper: 用于创建一个作用范围受限的模拟行为，像下面这样我们可以轻松实现在 cover_urlopen 内对 urllib2.urlopen 函数的模拟，而 cover_urlopen 外 urllib2.urlopen不受影响 wrapt.ObjectProxy: 代理对象，可以非常直观的实现对类的模拟 transient_function_wrapper1234567891011121314# 模拟函数import urllib2from wrapt import transient_function_wrapper@transient_function_wrapper('urllib2', 'urlopen')def urllib_request_wrap(wrapped, instance, arg, kwarg): return b'覆盖 urllib2.urlopen 函数返回值'# 对 urllib2.urlopen 函数的模拟，只会发生在 urllib_request_wrap 装饰的函数中@urllib_request_wrapdef cover_urlopen(): print urllib2.urlopen(url='http://www.baidu.com/')cover_urlopen() ObjectProxy1234567891011121314151617181920212223242526272829303132333435363738394041# 模拟类from wrapt import ObjectProxyclass Production(object): def __init__(self): self.value = 1 def run(self): print '%s running' % self.__class__.__name__class ProductionProxy(ObjectProxy): def __init__(self, wrapped): super(ProductionProxy, self).__init__(wrapped) self._self_value = 10 # 模拟类属性 @property def value(self): return self._self_value @value.deleter def value(self): del self._self_value # 通过方法覆盖，模拟类方法 def run(self): print 'proxy running'p = Production()print p.valuep_proxy = ProductionProxy(p)print p_proxy.valuep.run()p_proxy.run()print p.valueprint p_proxy.valuedel p_proxy.valueprint p_proxy.value","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"supervisor tornado 部署","slug":"deploy/supervisor_tornado","date":"2017-09-20T16:00:00.000Z","updated":"2020-05-24T15:27:55.257Z","comments":true,"path":"2017/09/21/deploy/supervisor_tornado/","link":"","permalink":"http://yoursite.com/2017/09/21/deploy/supervisor_tornado/","excerpt":"通过 supervisor 创建监听套接字的文件描述符，为多个 tornado 进程共享","text":"通过 supervisor 创建监听套接字的文件描述符，为多个 tornado 进程共享 1. tornado 启动12345678910111213from tornado.netutil import set_close_execdef main(): app = AnalyticApiApplication() http_serve = httpserver.HTTPServer(app) # http_serve.listen(options.port) # supervisor 创建的监听套接字文件描述符，通过 0 号传递给 tornado的所有进程 sock = socket.fromfd(0, family=socket.AF_INET, type=socket.SOCK_STREAM) set_close_exec(sock.fileno()) sock.setblocking(0) # 设置套接字为非阻塞调用 http_serve.add_socket(sock) ioloop.IOLoop.instance().start() 2. supervisor 配置12345678command=/home/tao/.local/bin/pipenv run python app.py --connect=local-dev --debug=1socket=tcp://localhost:8888directory=/home/tao/projects/analytics_apiuser=taonumprocs=4process_name=%(program_name)s_%(process_num)02dstdout_logfile =/var/log/tornado_pyapi_stdout_%(process_num)02d.log stderr_logfile =/var/log/tornado_pyapi_stderr_%(process_num)02d.log 3. tornado.bind_socket12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788def bind_sockets(port, address=None, family=socket.AF_UNSPEC, backlog=_DEFAULT_BACKLOG, flags=None, reuse_port=False): \"\"\"Creates listening sockets bound to the given port and address. Returns a list of socket objects (multiple sockets are returned if the given address maps to multiple IP addresses, which is most common for mixed IPv4 and IPv6 use). Address may be either an IP address or hostname. If it's a hostname, the server will listen on all IP addresses associated with the name. Address may be an empty string or None to listen on all available interfaces. Family may be set to either `socket.AF_INET` or `socket.AF_INET6` to restrict to IPv4 or IPv6 addresses, otherwise both will be used if available. The ``backlog`` argument has the same meaning as for `socket.listen() &lt;socket.socket.listen&gt;`. ``flags`` is a bitmask of AI_* flags to `~socket.getaddrinfo`, like ``socket.AI_PASSIVE | socket.AI_NUMERICHOST``. ``reuse_port`` option sets ``SO_REUSEPORT`` option for every socket in the list. If your platform doesn't support this option ValueError will be raised. \"\"\" if reuse_port and not hasattr(socket, \"SO_REUSEPORT\"): raise ValueError(\"the platform doesn't support SO_REUSEPORT\") sockets = [] if address == \"\": address = None if not socket.has_ipv6 and family == socket.AF_UNSPEC: # Python can be compiled with --disable-ipv6, which causes # operations on AF_INET6 sockets to fail, but does not # automatically exclude those results from getaddrinfo # results. # http://bugs.python.org/issue16208 family = socket.AF_INET if flags is None: flags = socket.AI_PASSIVE bound_port = None for res in set(socket.getaddrinfo(address, port, family, socket.SOCK_STREAM, 0, flags)): af, socktype, proto, canonname, sockaddr = res if (sys.platform == 'darwin' and address == 'localhost' and af == socket.AF_INET6 and sockaddr[3] != 0): # Mac OS X includes a link-local address fe80::1%lo0 in the # getaddrinfo results for 'localhost'. However, the firewall # doesn't understand that this is a local address and will # prompt for access (often repeatedly, due to an apparent # bug in its ability to remember granting access to an # application). Skip these addresses. continue try: sock = socket.socket(af, socktype, proto) except socket.error as e: if errno_from_exception(e) == errno.EAFNOSUPPORT: continue raise set_close_exec(sock.fileno()) if os.name != 'nt': sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) if reuse_port: sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1) if af == socket.AF_INET6: # On linux, ipv6 sockets accept ipv4 too by default, # but this makes it impossible to bind to both # 0.0.0.0 in ipv4 and :: in ipv6. On other systems, # separate sockets *must* be used to listen for both ipv4 # and ipv6. For consistency, always disable ipv4 on our # ipv6 sockets and use a separate ipv4 socket when needed. # # Python 2.x on windows doesn't have IPPROTO_IPV6. if hasattr(socket, \"IPPROTO_IPV6\"): sock.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 1) # automatic port allocation with port=None # should bind on the same port on IPv4 and IPv6 host, requested_port = sockaddr[:2] if requested_port == 0 and bound_port is not None: sockaddr = tuple([host, bound_port] + list(sockaddr[2:])) sock.setblocking(0) sock.bind(sockaddr) bound_port = sock.getsockname()[1] sock.listen(backlog) sockets.append(sock) return sockets","categories":[{"name":"运维","slug":"运维","permalink":"http://yoursite.com/categories/运维/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"virtualenv","slug":"deploy/virtualenv","date":"2017-09-04T05:54:03.000Z","updated":"2020-05-24T15:28:04.514Z","comments":true,"path":"2017/09/04/deploy/virtualenv/","link":"","permalink":"http://yoursite.com/2017/09/04/deploy/virtualenv/","excerpt":"virtualenv 基本使用","text":"virtualenv 基本使用 1. 环境创建virtualenv dirname – 创建虚拟环境source dirname/bin/activate – 启用虚拟环境 virtualenv 可用选项 作用 –distribute dirname 创建新的虚拟环境，并安装 pip –no-site-packages 使系统环境的包对虚拟环境不可见 2.virtualenvwrapper作用：virtualenv 管理工具，方便的创建/激活/管理/销毁虚拟环境 命令 作用 mkvirtualenv virname 新建虚拟环境 workon virname 激活 deactivate 关闭 rmvirtualenv virname 删除","categories":[{"name":"运维","slug":"运维","permalink":"http://yoursite.com/categories/运维/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"hexo github blog","slug":"hexo/hexo-github","date":"2017-09-03T03:58:00.000Z","updated":"2020-05-24T15:38:39.701Z","comments":true,"path":"2017/09/03/hexo/hexo-github/","link":"","permalink":"http://yoursite.com/2017/09/03/hexo/hexo-github/","excerpt":"使用 githup pages 和 hexo搭建 blog，本文不是完整教程，只是整个流程概览和常用命令备忘","text":"使用 githup pages 和 hexo搭建 blog，本文不是完整教程，只是整个流程概览和常用命令备忘 1. github blog 搭建 安装node.js node -v 安装 hexo npm install hexo-cli -g 注册 github 帐号 新建xxx.github.io仓库，xxx 为帐号名称 初始化 hexo blog 123hexo init blogcd blognpm install 配置 hexo github 安装 hexo-deployer-gitnpm install hexo-deployer-git --save 在网站的_config.yml中配置deploy 123deploy:type: git repo: &lt;repository url&gt; branch: [branch] 提交git 12hexo d -ghexo d 2. hexo 常用命令 命令 作用 hexo init dir_name 创建博客目录 hexo clean …. hexo g(generate) 生成静态文件 hexo s(server) 启动本地web服务，用于博客的预览 hexo d(deploy) 部署播客到远端 hexo d -g 生成部署 hexo s -g 生成预览 hexo new “name” 新建文章 hexo new page “name” 新建页面 Quick StartHexo HomedocumentationtroubleshootingHexo GitHub. Create a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"}]}],"categories":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/categories/存储/"},{"name":"运维","slug":"运维","permalink":"http://yoursite.com/categories/运维/"},{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/categories/分布式/"},{"name":"Go","slug":"Go","permalink":"http://yoursite.com/categories/Go/"},{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"极客时间","slug":"极客时间","permalink":"http://yoursite.com/tags/极客时间/"},{"name":"MySQL实战45讲","slug":"MySQL实战45讲","permalink":"http://yoursite.com/tags/MySQL实战45讲/"},{"name":"K8S","slug":"K8S","permalink":"http://yoursite.com/tags/K8S/"},{"name":"数据密集型应用","slug":"数据密集型应用","permalink":"http://yoursite.com/tags/数据密集型应用/"},{"name":"马哥 MySQL 运维","slug":"马哥-MySQL-运维","permalink":"http://yoursite.com/tags/马哥-MySQL-运维/"},{"name":"高性能的MySQL","slug":"高性能的MySQL","permalink":"http://yoursite.com/tags/高性能的MySQL/"},{"name":"go语言入门","slug":"go语言入门","permalink":"http://yoursite.com/tags/go语言入门/"},{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"},{"name":"wrapt","slug":"wrapt","permalink":"http://yoursite.com/tags/wrapt/"},{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"}]}